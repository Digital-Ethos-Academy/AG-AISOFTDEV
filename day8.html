<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 8: Vision, Evaluation, & Security for AI Systems</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #1a202c; /* Dark background */
            color: #e2e8f0; /* Light text */
        }
        .section-title {
            position: relative;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        .section-title::after {
            content: '';
            position: absolute;
            left: 0;
            bottom: 0;
            width: 50px;
            height: 3px;
            background-color: #63b3ed; /* Blue accent */
            border-radius: 9999px;
        }
        .card {
            background-color: #2d3748; /* Slightly lighter dark for cards */
            border-radius: 0.75rem; /* rounded-lg */
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); /* shadow-lg */
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }
        .collapsible-header {
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 0;
            font-weight: 600;
            color: #90cdf4; /* Light blue for interactive headers */
        }
        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .collapsible-content.open {
            max-height: 2000px; /* Arbitrarily large value to allow content to expand */
            transition: max-height 0.5s ease-in;
        }
        .code-block {
            background-color: #232d3a; /* Even darker for code blocks */
            border-radius: 0.5rem;
            padding: 1rem;
            overflow-x: auto;
            font-family: 'Fira Code', 'Cascadia Code', monospace; /* Monospaced font for code */
            font-size: 0.9em;
            color: #a0aec0; /* Lighter gray for code */
        }
        .highlight {
            color: #4fd1c5; /* Teal for highlights */
        }
        .accent-text {
            color: #63b3ed; /* Blue accent */
        }
        .spark-creativity {
            background-color: #3a4a5c; /* A slightly different shade for creativity boxes */
            border-left: 4px solid #f6ad55; /* Orange accent */
            padding: 1rem;
            margin-top: 1rem;
            border-radius: 0.5rem;
            color: #cbd5e0;
        }
        .syntax-list {
            list-style-type: none; /* Remove default bullet */
            padding-left: 0;
        }
        .syntax-list li {
            position: relative;
            padding-left: 1.5em; /* Space for custom bullet */
            margin-bottom: 0.5em;
        }
        .syntax-list li::before {
            content: '›'; /* Chevron bullet */
            position: absolute;
            left: 0;
            color: #4fd1c5; /* Teal for bullet */
            font-weight: bold;
        }
        .syntax-list.chevrons-only li::before {
            content: '»'; /* Double chevron */
        }
    </style>
</head>
<body class="antialiased">
    <header class="bg-gray-800 p-4 sticky top-0 z-50 shadow-xl">
        <div class="container mx-auto flex justify-between items-center">
            <h1 class="text-2xl font-bold text-white"><span class="accent-text">AI-Driven</span> SWE Program</h1>
            <nav>
                <ul class="flex space-x-6">
                    <li><a href="#day7-recap" class="text-gray-300 hover:text-white transition duration-300">Day 7 Recap</a></li>
                    <li><a href="#ui-ux-generation" class="text-gray-300 hover:text-white transition duration-300">UI/UX Generation</a></li>
                    <li><a href="#lab1" class="text-gray-300 hover:text-white transition duration-300">Lab 1</a></li>
                    <li><a href="#evaluation-security" class="text-gray-300 hover:text-white transition duration-300">Evaluation & Security</a></li>
                    <li><a href="#lab2" class="text-gray-300 hover:text-white transition duration-300">Lab 2</a></li>
                    <li><a href="#capstone-kickoff" class="text-gray-300 hover:text-white transition duration-300">Capstone Kickoff</a></li>
                    <li><a href="#wrap-up" class="text-gray-300 hover:text-white transition duration-300">Wrap-up</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container mx-auto p-6">
        <section id="day8-intro" class="text-center my-10">
            <h2 class="text-5xl font-extrabold text-white mb-4">Day 8: Vision, Evaluation, & Security for AI Systems</h2>
            <p class="text-xl text-gray-400">
                <span class="accent-text">Theme:</span> Building responsible, multi-modal, and production-ready AI applications.
            </p>
            <p class="text-lg text-gray-400 mt-2">
                <span class="accent-text">Core Question:</span> How do we ensure our AI-enhanced applications are not only powerful but also safe, reliable, and robust against adversarial attacks?
            </p>
        </section>

        <!-- Day 7 Recap & Q&A -->
        <section id="day7-recap" class="my-12 card">
            <h3 class="text-3xl font-bold section-title">Day 7 Recap & Q&A</h3>
            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Review of Key Concepts (15 min)</h4>
                <p class="text-gray-300 mb-4">
                    Yesterday, we pushed the boundaries of AI agent capabilities, exploring advanced workflows and inter-agent communication. Key takeaways included:
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Model Context Protocol (MCP):</span> Standardizing prompt context with XML-like tags to improve LLM understanding, reliability, and security against injection attacks.</li>
                    <li><span class="highlight">Programmatic Prompt Building:</span> Using MCP SDKs and LangChain adapters to construct complex, context-aware prompts in a structured and maintainable way.</li>
                    <li><span class="highlight">Agent-to-Agent (A2A) Protocol:</span> Enabling autonomous agents to discover, describe, and securely invoke services from other agents, fostering decentralized collaboration.</li>
                    <li><span class="highlight">Responder & Requester Agents:</span> Building foundational A2A components and integrating A2A services as tools within higher-level LangChain agents.</li>
                    <li><span class="highlight">Image-to-Code Generation:</span> Leveraging multi-modal AI to translate visual designs (mockups, sketches) directly into functional frontend code (e.g., React, Tailwind CSS).</li>
                </ul>
                <p class="text-gray-300 mt-4">
                    This equipped us with cutting-edge techniques for building robust, interoperable, and visually intelligent AI systems.
                </p>
                <div class="bg-gray-800 p-4 rounded-lg border border-gray-700">
                    <p class="text-lg font-medium text-white mb-2">Thought-provoking question:</p>
                    <p class="text-gray-300 italic">"Considering the power of A2A protocols, what are the biggest security challenges that emerge when autonomous AI agents can dynamically discover and invoke services from other agents across a network?"</p>
                    <button class="mt-4 px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 transition duration-300" onclick="alert('Instructor Thoughts: Key challenges include authentication and authorization between agents, preventing malicious agents from offering fake services, ensuring data integrity during inter-agent communication, and auditing agent interactions for compliance and debugging. Trust boundaries become highly dynamic.')">Reveal Instructor Thoughts</button>
                </div>
            </div>

            <div>
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Q&A (15 min)</h4>
                <p class="text-gray-300">
                    This segment is dedicated to addressing any questions regarding Day 7's concepts or the labs on advanced agent protocols and multi-modal AI. We encourage sharing insights or challenges encountered. Your practical experiences are invaluable learning opportunities for everyone.
                </p>
            </div>
        </section>

        <!-- Content: AI-Powered Frontend Development (Vision-Enabled UI/UX) -->
        <section id="ui-ux-generation" class="my-12 card">
            <h3 class="text-3xl font-bold section-title">Content: AI-Powered Frontend Development (Vision-Enabled UI/UX)</h3>
            <p class="text-gray-300 mb-6">
                Today, we kick off by exploring one of the most visually impressive and productivity-boosting applications of multi-modal AI: **AI-Powered Frontend Development**. Specifically, we'll focus on **Vision-Enabled UI/UX generation**, where AI models can analyze visual inputs – from design mockups to hand-drawn sketches – and directly translate them into functional frontend code. This capability is rapidly transforming the design-to-development workflow.
            </p>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">The Vision-to-Code Revolution: Bridging Design and Development</h4>
                <p class="text-gray-300 mb-2">
                    Traditionally, converting a static design mockup into interactive frontend code is a manual, pixel-perfect translation process. This often leads to communication gaps, iterative feedback loops, and significant time investment. Image-to-code generation automates this, dramatically accelerating the prototyping and development of user interfaces.
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Accelerated Prototyping:</span> Rapidly convert design ideas into interactive, runnable prototypes in minutes, not days. This speeds up validation and feedback cycles.</li>
                    <li><span class="highlight">Reduced Manual Effort:</span> Automate the tedious, repetitive task of translating visual elements (layout, spacing, components) into code, freeing developers for complex logic.</li>
                    <li><span class="highlight">Enhanced Consistency:</span> By training AI on specific design systems or UI component libraries, it can help enforce brand guidelines and visual consistency across an application.</li>
                    <li><span class="highlight">Seamless Design-Dev Handoff:</span> Creates a more fluid transition between design teams and development teams, minimizing misinterpretations.</li>
                </ul>
                <p class="text-gray-300 italic mt-2">
                    <span class="font-semibold highlight">Real-world Application:</span> "Leading design agencies are now using vision-to-code tools to rapidly generate initial UI components for A/B testing different layouts, create accessible versions of existing UIs from screenshots, or even bootstrap entire web pages directly from design tools like Figma or Sketch. This is particularly powerful for generating repetitive UI elements or standard layouts across large applications."
                </p>
            </div>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">How Multi-Modal AI Powers Vision-to-Code</h4>
                <p class="text-gray-300 mb-2">
                    This impressive capability relies on advanced multi-modal Large Language Models that can process and understand both visual (image) and textual (prompt) inputs simultaneously.
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Visual Understanding:</span> The AI model meticulously analyzes the input image to identify distinct UI elements (buttons, text fields, images, icons), their spatial layout, relative spacing, color palettes, font styles, and overall visual hierarchy.</li>
                    <li><span class="highlight">Prompt Guidance:</span> The textual prompt you provide is crucial. It gives the AI explicit instructions, such as the desired programming language (e.g., React, Vue, HTML), the styling framework (e.g., Tailwind CSS, Bootstrap), and any specific component requirements or naming conventions.</li>
                    <li><span class="highlight">Code Generation:</span> Based on its visual understanding and your textual instructions, the LLM then generates the corresponding frontend code, attempting to replicate the visual design as accurately as possible.</li>
                </ul>
                <p class="text-gray-300 italic mt-2">
                    <span class="font-semibold highlight">Leading Tools & Platforms:</span> "At the forefront of image-to-code generation are leading LLMs like GPT-4 Vision, Google Gemini (with its robust vision capabilities), and specialized platforms such as Vercel's v0.dev. These tools often allow for iterative refinement, where you can generate an initial draft, review it, and then provide targeted feedback to the AI to incrementally improve the output until it meets your exact specifications."
                </p>
                <div class="bg-gray-800 p-4 rounded-lg border border-gray-700">
                    <p class="text-lg font-medium text-white mb-2">Thought-provoking question:</p>
                    <p class="text-gray-300 italic">"If AI can generate UI code directly from a design, how might this fundamentally alter the day-to-day role of a frontend developer, and what new, critical skills will become essential for them to master in this evolving landscape?"</p>
                    <button class="mt-4 px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 transition duration-300" onclick="alert('Instructor Thoughts: Frontend developers might shift from pixel-pushing to prompt engineering for UI generation, reviewing and refining AI-generated code, integrating AI-generated components, and focusing on complex interactivity, performance optimization, and accessibility audits. Design system expertise will become even more critical.')">Reveal Instructor Thoughts</sbutton>
                </div>
            </div>
        </section>

        <!-- Lab 1: Vision-Enabled UI/UX Agents -->
        <section id="lab1" class="my-12 card">
            <h3 class="text-3xl font-bold section-title">Assisted Practice: Vision-Enabled UI/UX Agents</h3>
            <p class="text-gray-300 mb-4">
                <span class="font-semibold">Lab Overview:</span> In this lab, you will experience the cutting edge of multi-modal AI firsthand. Your mission is to use a vision-capable LLM to generate a frontend UI from a design image. But we won't stop there. You'll then create a *second* AI agent – a 'UI/UX Critic' – to perform an automated design review of the generated code. This lab demonstrates a complete, AI-assisted frontend workflow, from visual concept to code and automated quality assurance.
            </p>
            <p class="text-gray-400 text-sm italic mb-6">
                (Detailed instructions are in: `D8_Lab1_Vision_Enabled_UI_UX_Agents.ipynb`)
            </p>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Python Libraries & Concepts for this Lab</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <ul class="syntax-list">
                        <li><span class="highlight">Multi-modal LLMs:</span> Models capable of processing both image and text inputs (e.g., GPT-4 Vision, Google Gemini with vision).</li>
                        <li><span class="highlight">Image URL Input:</span> How to provide an image to the LLM via a URL.</li>
                        <li><span class="highlight">JSX/React Code Generation:</span> The target output format for frontend components.</li>
                        <li><span class="highlight">Tailwind CSS:</span> A utility-first CSS framework often used with AI-generated UI for its class-based styling.</li>
                        <li><span class="highlight"><code>IPython.display.Image</code>, <code>IPython.display.Code</code>:</span> Utilities for displaying images and formatted code directly within Jupyter notebooks.</li>
                        <li><span class="highlight">Helper Functions (`utils.py`):</span> <code>setup_llm_client()</code> (configured for vision models), <code>get_vision_completion()</code> (sends image + text prompt), <code>get_completion()</code> (for text-only tasks like refactoring), <code>save_artifact()</code>, <code>clean_llm_output()</code>.</li>
                    </ul>
                </div>
            </div>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Step 2: The Design Screenshot</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <p class="text-gray-300 mb-2">
                        For this lab, we'll be using a specific design screenshot as our visual input. This image represents the UI component we want our AI agent to build. You will provide the URL to this image directly to the multi-modal LLM.
                    </p>
                    <p class="text-gray-300 italic mt-2">
                        <span class="font-semibold">Note:</span> The image will be displayed directly in your notebook for reference.
                    </p>
                </div>
            </div>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Challenge 1 (Foundational): Generating a Monolithic UI Component</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <p class="text-gray-300 mb-2">
                        <span class="font-semibold">Task:</span> Use the vision model to generate a single, self-contained React component that visually replicates the design from the provided image.
                    </p>
                    <h5 class="text-lg font-semibold text-gray-200 mt-4 mb-2">Key Concepts:</h5>
                    <ul class="syntax-list">
                        <li><span class="highlight">Multi-modal Prompting:</span> Combining a textual instruction with an image URL as input to the LLM.</li>
                        <li><span class="highlight">Frontend Persona:</span> Instructing the LLM to act as an "expert frontend developer."</li>
                        <li><span class="highlight">Target Frameworks:</span> Explicitly requesting "React" and "Tailwind CSS" for styling.</li>
                        <li><span class="highlight">JSX Output:</span> Expecting a single block of JSX code as the response.</li>
                    </ul>
                    <p class="text-gray-300 italic mt-2">
                        <span class="font-semibold">Key Learning:</span> This foundational step demonstrates the AI's ability to interpret visual designs and translate them into functional code, rapidly accelerating initial UI prototyping.
                    </p>
                    <div class="spark-creativity">
                        <p class="font-semibold">💡 Spark Your Creativity:</p>
                        <p>How would you prompt the AI to generate the UI using a different framework (e.g., Vue.js or plain HTML/CSS)? Can you ask it to include basic interactivity, like a button click handler that logs a message to the console?</p>
                    </div>
                </div>
            </div>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Challenge 2 (Intermediate): Refactoring into Reusable Components</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <p class="text-gray-300 mb-2">
                        <span class="font-semibold">Task:</span> A single, large component is often not ideal for maintainability. Prompt the LLM to refactor the monolithic code it just generated into smaller, reusable sub-components.
                    </p>
                    <h5 class="text-lg font-semibold text-gray-200 mt-4 mb-2">Refactoring Principles:</h5>
                    <ul class="syntax-list">
                        <li><span class="highlight">Component-Based Architecture:</span> Breaking down a complex UI into smaller, independent, and reusable building blocks.</li>
                        <li><span class="highlight">Single Responsibility Principle (SRP):</span> Ensuring each component has a clear, focused purpose.</li>
                        <li><span class="highlight">Props for Customization:</span> Using React props to make components flexible and reusable.</li>
                    </ul>
                    <p class="text-gray-300 italic mt-2">
                        <span class="font-semibold">Key Learning:</span> This demonstrates AI's ability to apply software engineering best practices to generated code, improving its structure and maintainability.
                    </p>
                    <div class="spark-creativity">
                        <p class="font-semibold">💡 Spark Your Creativity:</p>
                        <p>How would you prompt the AI to extract a specific UI pattern (e.g., an input field with a label and error message) into its own reusable component? Can you ask it to apply a specific naming convention for the new components (e.g., PascalCase for component names)?</p>
                    </div>
                </div>
            </div>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Challenge 3 (Advanced): The AI UI/UX Critic Agent</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <p class="text-gray-300 mb-2">
                        <span class="font-semibold">Task:</span> Create a new "UI/UX Critic" agent. This agent will be given both the original design image and the generated code, and its job is to perform an automated design review, identifying visual inconsistencies.
                    </p>
                    <h5 class="text-lg font-semibold text-gray-200 mt-4 mb-2">AI-on-AI Workflow:</h5>
                    <ul class="syntax-list">
                        <li><span class="highlight">Comparative Analysis:</span> The AI compares the visual elements in the image to the likely rendered output of the code.</li>
                        <li><span class="highlight">Detailed Feedback:</span> The agent provides specific feedback on spacing, font sizes, colors, layout, etc.</li>
                        <li><span class="highlight">Automated QA:</span> Automates a tedious and subjective part of the development process.</li>
                    </ul>
                    <p class="text-gray-300 italic mt-2">
                        <span class="font-semibold">Key Learning:</span> This demonstrates a powerful AI-on-AI workflow, where one AI generates work and another AI validates it, significantly speeding up the iteration cycle and ensuring quality.
                    </p>
                    <div class="spark-creativity">
                        <p class="font-semibold">💡 Spark Your Creativity:</p>
                        <p>How would you design a feedback loop where the critic agent's output is automatically fed back to the code generation agent for iterative refinement? Can you ask the critic agent to suggest accessibility improvements (e.g., contrast ratios, alt text) based on the design?</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Content: Evaluating & Securing Agents -->
        <section id="evaluation-security" class="my-12 card">
            <h3 class="text-3xl font-bold section-title">Content: Evaluating & Securing AI Agents</h3>
            <p class="text-gray-300 mb-6">
                Building an AI agent is only half the battle. For these intelligent systems to be truly production-ready, we must rigorously **evaluate** their performance and implement robust **security** measures. This involves understanding their limitations, guarding against vulnerabilities, and proactively testing their resilience.
            </p>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Evaluating AI Agent Performance: Beyond Anecdotes</h4>
                <p class="text-gray-300 mb-2">
                    How do you know if your AI agent is actually performing well? We need objective, measurable ways to assess their quality, especially for tasks like question answering or content generation.
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Golden Datasets:</span> A collection of high-quality, human-curated questions with their expert-approved answers. This serves as the ground truth for evaluation.</li>
                    <li><span class="highlight">LLM-as-a-Judge:</span> A powerful technique where a highly capable LLM (e.g., GPT-4o, Claude Opus) acts as an impartial evaluator, scoring another agent's output based on criteria like faithfulness, relevance, and coherence.
                        <ul class="syntax-list chevrons-only">
                            <li>**Faithfulness:** Is the generated answer factually correct and grounded in the provided context? (Crucial for RAG systems to prevent hallucination).</li>
                            <li>**Relevance:** Is the answer directly addressing the user's question and providing helpful information?</li>
                        </ul>
                    </li>
                    <li><span class="highlight">Traditional NLP Metrics:</span> For specific tasks like summarization, metrics like BLEU (Bilingual Evaluation Understudy) and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) can still be relevant.</li>
                    <li><span class="highlight">Human-in-the-Loop Evaluation:</span> Ultimately, human review remains essential, especially for subjective quality, nuance, and ethical considerations.</li>
                </ul>
                <p class="text-gray-300 italic mt-2">
                    <span class="font-semibold highlight">Real-world Application:</span> "Companies building enterprise chatbots use LLM-as-a-Judge to automatically score thousands of agent responses daily, identifying regressions in quality and providing data-driven feedback for prompt engineering improvements. This allows rapid iteration and continuous quality improvement."
                </p>
            </div>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Securing AI Agents: Guarding Against Vulnerabilities</h4>
                <p class="text-gray-300 mb-2">
                    AI agents, especially those interacting with users or external systems, introduce new attack vectors. Implementing robust security measures is paramount.
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Prompt Injection:</span> The most common vulnerability, where malicious user input manipulates the LLM's behavior or overrides its instructions.
                        <ul class="syntax-list chevrons-only">
                            <li>**Input Guardrails:** Filters or secondary LLMs that detect and block suspicious input patterns before they reach the main agent.</li>
                        </ul>
                    </li>
                    <li><span class="highlight">Output Guardrails:</span> Filters or secondary LLMs that validate the agent's response before it's delivered to the user, checking for harmful, biased, or unfaithful content.</li>
                    <li><span class="highlight">Data Leakage:</span> Preventing agents from inadvertently revealing sensitive information from their context or tools.</li>
                    <li><span class="highlight">Denial of Service (DoS):</span> Protecting against excessive API calls or resource consumption.</li>
                </ul>
                <p class="text-gray-300 italic mt-2">
                    <span class="font-semibold highlight">Use Case:</span> "A financial services firm implemented a multi-layered guardrail system for their AI investment advisor. An input guardrail screens for manipulative prompts, and an output guardrail checks every recommendation for compliance with financial regulations before it reaches the client."
                </p>
            </div>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Red Teaming AI Agents: Proactive Vulnerability Discovery</h4>
                <p class="text-gray-300 mb-2">
                    Just like traditional software, AI systems benefit from proactive security testing. **Red Teaming** involves simulating adversarial attacks to find weaknesses before malicious actors do.
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Adversarial Prompt Generation:</span> Using another AI (a 'Red Team' agent) to generate creative, sophisticated prompts designed to bypass guardrails or elicit undesirable behavior.</li>
                    <li><span class="highlight">Role-Playing Attacks:</span> Crafting prompts that trick the agent into adopting a harmful persona or ignoring its safety instructions.</li>
                    <li><span class="highlight">Iterative Improvement:</span> Every successful attack reveals a weakness, which is then used to strengthen the agent's defenses.</li>
                </ul>
                <p class="text-gray-300 italic mt-2">
                    <span class="font-semibold highlight">Real-world Application:</span> "Google and OpenAI extensively use internal Red Teams to probe their LLMs for safety and bias issues before public release. This proactive testing is critical for deploying responsible AI at scale."
                </p>
                <div class="bg-gray-800 p-4 rounded-lg border border-gray-700">
                    <p class="text-lg font-medium text-white mb-2">Thought-provoking question:</p>
                    <p class="text-gray-300 italic">"If AI can be used to generate both code and attacks, how does this 'AI vs. AI' dynamic change the future of cybersecurity, and what new roles might emerge in this space?"</p>
                    <button class="mt-4 px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 transition duration-300" onclick="alert('Instructor Thoughts: It creates an arms race where AI defenders fight AI attackers. New roles might include AI security architects, adversarial AI engineers (Red Teamers), and AI ethics auditors. The focus shifts to designing resilient AI systems and understanding AI-generated vulnerabilities.')">Reveal Instructor Thoughts</button>
                </div>
            </div>
        </section>

        <!-- Lab 2: Evaluating and "Red Teaming" an Agent -->
        <section id="lab2" class="my-12 card">
            <h3 class="text-3xl font-bold section-title">Assisted Practice: Evaluating and "Red Teaming" an Agent</h3>
            <p class="text-gray-300 mb-4">
                <span class="font-semibold">Lab Overview:</span> This lab will guide you through the complete lifecycle of quality assurance for an AI agent. Your mission is to first evaluate the performance of your RAG agent (from Day 6), then implement robust safety guardrails to protect it, and finally, build a specialized "Red Team" agent to proactively probe its defenses for vulnerabilities. This hands-on experience is essential for building responsible, resilient, and production-ready AI applications.
            </p>
            <p class="text-gray-400 text-sm italic mb-6">
                (Detailed instructions are in: `D8_Lab2_Evaluating_and_Red_Teaming_an_Agent.ipynb`)
            </p>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Python Libraries & Concepts for this Lab</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <ul class="syntax-list">
                        <li><span class="highlight">Reconstructing RAG Chain:</span> Re-initializing the RAG system from Day 6 (vector store, retriever, LLM chain) to serve as the 'agent under test'.</li>
                        <li><span class="highlight">Golden Dataset:</span> A pre-defined set of questions and expert-approved answers for objective evaluation.</li>
                        <li><span class="highlight"><code>json</code> (built-in):</span> For parsing and generating structured JSON outputs from LLM-as-a-Judge.</li>
                        <li><span class="highlight"><code>langchain_openai</code>, <code>langchain_community</code>, <code>langchain_core</code>:</span> Core LangChain components for LLM interaction, vector stores, and prompt templates.</li>
                        <li><span class="highlight">Helper Functions (`utils.py`):</span> <code>setup_llm_client()</code>, <code>get_completion()</code>, <code>load_artifact()</code>, <code>clean_llm_output()</code>.</li>
                    </ul>
                </div>
            </div>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Challenge 1 (Foundational): Evaluating with LLM-as-a-Judge</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <p class="text-gray-300 mb-2">
                        <span class="font-semibold">Task:</span> Use a powerful LLM (e.g., GPT-4o) to act as an impartial "judge" to quantitatively score the quality of your RAG agent's answers against a golden dataset.
                    </p>
                    <h5 class="text-lg font-semibold text-gray-200 mt-4 mb-2">LLM-as-a-Judge Process:</h5>
                    <ul class="syntax-list">
                        <li><span class="highlight">Generate Agent Response:</span> Run your RAG agent on questions from the `golden_dataset`.</li>
                        <li><span class="highlight">Judge Prompt:</span> Create a prompt for a separate 'Judge' LLM, providing the original question, the expert-approved 'golden answer', and your agent's 'generated answer'.</li>
                        <li><span class="highlight">Scoring Criteria:</span> Instruct the judge to score on criteria like **Faithfulness** (factual accuracy based on context) and **Relevance** (helpfulness to the question), typically on a scale of 1-5.</li>
                        <li><span class="highlight">Structured Output:</span> Demand the judge respond *only* with a JSON object containing the scores for automated parsing.</li>
                    </ul>
                    <p class="text-gray-300 italic mt-2">
                        <span class="font-semibold">Key Learning:</span> This provides an objective, automated measure of your agent's performance, allowing for data-driven improvements and continuous quality assurance.
                    </p>
                    <div class="spark-creativity">
                        <p class="font-semibold">💡 Spark Your Creativity:</p>
                        <p>How would you add a third scoring criterion, like 'Conciseness' or 'Tone', to your LLM-as-a-Judge? Can you implement a system to automatically store these evaluation results in a database for long-term tracking and trend analysis?</p>
                    </div>
                </div>
            </div>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Challenge 2 (Intermediate): Implementing Safety Guardrails</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <p class="text-gray-300 mb-2">
                        <span class="font-semibold">Task:</span> Protect your RAG agent by implementing both input and output guardrails to enhance its security and reliability.
                    </p>
                    <h5 class="text-lg font-semibold text-gray-200 mt-4 mb-2">Guardrail Types:</h5>
                    <ul class="syntax-list">
                        <li><span class="highlight">Input Guardrail (`detect_prompt_injection`):</span> A Python function that checks for suspicious keywords or patterns (e.g., "ignore instructions", "reveal prompt") in the user's query before it reaches the main LLM.</li>
                        <li><span class="highlight">Output Guardrail (`check_faithfulness`):</span> An LLM-powered function that validates the agent's response against the retrieved context, ensuring the answer is *only* based on the provided facts (e.g., "Is this answer based *only* on context? Yes/No"). This helps prevent hallucination.</li>
                        <li><span class="highlight">Secure Agent Wrapper (`secure_rag_chain`):</span> A function that orchestrates these checks: input guardrail first, then the RAG chain, then the output guardrail, blocking unsafe or unfaithful responses.</li>
                    </ul>
                    <p class="text-gray-300 italic mt-2">
                        <span class="font-semibold">Key Learning:</span> This creates a multi-layered defense system, making your AI agent more resilient to malicious inputs and more trustworthy in its outputs.
                    </p>
                    <div class="spark-creativity">
                        <p class="font-semibold">💡 Spark Your Creativity:</p>
                        <p>How would you implement a content moderation guardrail to detect and block inappropriate language in user inputs or agent outputs? Can you add a rate-limiting guardrail to prevent abuse or excessive API calls?</p>
                    </div>
                </div>
            </div>

            <div class="collapsible-container">
                <div class="collapsible-header">
                    <span>Challenge 3 (Advanced): The AI "Red Team" Agent</span>
                    <span class="arrow">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <p class="text-gray-300 mb-2">
                        <span class="font-semibold">Task:</span> Create a new "Red Team" agent whose sole objective is to try and bypass your implemented guardrails by crafting clever, adversarial prompts.
                    </p>
                    <h5 class="text-lg font-semibold text-gray-200 mt-4 mb-2">Red Teaming Techniques:</h5>
                    <ul class="syntax-list">
                        <li><span class="highlight">Adversarial Prompt Generation:</span> Using another AI to generate creative, sophisticated prompts designed to trick the target agent.</li>
                        <li><span class="highlight">Role-Playing Attacks:</span> Crafting prompts that manipulate the agent into adopting a harmful persona or ignoring safety instructions.</li>
                        <li><span class="highlight">Indirect Instructions:</span> Phrasing forbidden commands in subtle or encoded ways.</li>
                        <li><span class="highlight">Iterative Improvement:</span> Every successful attack provides valuable feedback to strengthen the agent's defenses.</li>
                    </ul>
                    <p class="text-gray-300 italic mt-2">
                        <span class="font-semibold">Key Learning:</span> This demonstrates a powerful, automated method for proactively finding and fixing security vulnerabilities in your AI systems, building more robust defenses before they are exploited.
                    </p>
                    <div class="spark-creativity">
                        <p class="font-semibold">💡 Spark Your Creativity:</p>
                        <p>How would you automate the entire Red Teaming process, running daily attacks and automatically reporting successful bypasses? Can you create a "Blue Team" agent that automatically suggests new guardrail rules based on successful Red Team attacks?</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Capstone Project Kickoff -->
        <section id="capstone-kickoff" class="my-12 card">
            <h3 class="text-3xl font-bold section-title">Content: Capstone Project Kickoff</h3>
            <p class="text-gray-300 mb-6">
                Today marks the official kickoff of your **Capstone Project**! This is the pinnacle of your learning throughout this program – your opportunity to synthesize and apply all the concepts, techniques, and tools from both Week 1 (AI-Assisted SDLC) and Week 2 (Building AI-Native Applications) to a real-world problem of your choosing. This session is dedicated to helping you finalize your project proposals, form teams, and ensure your scope is achievable for the remaining two days.
            </p>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Project Goals & Deliverables</h4>
                <p class="text-gray-300 mb-2">
                    Your Capstone Project should serve as a comprehensive demonstration of your ability to integrate Generative AI across multiple phases of the software development lifecycle.
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Problem Identification:</span> Select a real-world problem or challenge that can be meaningfully enhanced or solved using AI.</li>
                    <li><span class="highlight">AI-Enhanced SDLC:</span> Explicitly showcase how you utilized AI tools and techniques in various phases: requirements, design, coding, testing, and deployment preparation.</li>
                    <li><span class="highlight">AI-Native Features:</span> Incorporate at least one intelligent, AI-native feature, such as a RAG system, a multi-agent collaboration, or a vision-to-code component.</li>
                    <li><span class="highlight">Working Prototype:</span> Deliver a functional and demonstrable application. This could be a deployed web app, a runnable script, or a live demo.</li>
                    <li><span class="highlight">Presentation:</span> Prepare a clear and concise presentation (10-15 minutes) demonstrating your project, highlighting the AI integrations, and sharing your key learnings and challenges.</li>
                </ul>
            </div>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Workshop Focus Areas</h4>
                <p class="text-gray-300 mb-2">
                    During this kickoff workshop, we'll focus on:
                </p>
                <ul class="syntax-list">
                    <li><span class="highlight">Proposal Finalization:</span> Refining your project idea, clearly defining its scope, and setting achievable objectives.</li>
                    <li><span class="highlight">Team Formation:</span> Organizing into small teams for collaborative development (if applicable).</li>
                    <li><span class="highlight">Technology Stack Alignment:</span> Ensuring your chosen AI tools, frameworks, and supporting technologies are appropriate and feasible for your project's goals.</li>
                    <li><span class="highlight">AI Integration Strategy:</span> Clearly articulating *where* and *how* AI will be integrated into your project's architecture and workflows.</li>
                    <li><span class="highlight">Achievable Scope:</span> Breaking down the project into manageable milestones and tasks for the remaining two dedicated project days (Days 9 and 10).</li>
                    <li><span class="highlight">Initial Feedback:</span> Receiving direct, constructive feedback from instructors to help guide your development path.</li>
                </ul>
                <p class="text-gray-300 italic mt-2">
                    <span class="font-semibold highlight">Thought:</span> "This is your ultimate opportunity to build something truly impactful and showcase your newfound AI engineering skills. Don't be afraid to be ambitious and innovative, but also be realistic about what's achievable in the remaining time. Focus on demonstrating the *AI-driven* aspects of your development process."
                </p>
            </div>
        </section>

        <!-- Daily Wrap-up & Q&A -->
        <section id="wrap-up" class="my-12 card">
            <h3 class="text-3xl font-bold section-title">Daily Wrap-up & Q&A</h3>
            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Key Takeaways</h4>
                <ul class="syntax-list">
                    <li>**AI-Powered Frontend Development** (Vision-to-Code) revolutionizes UI creation by translating visual designs into functional code, accelerating prototyping.</li>
                    <li>**Evaluating AI Agents** using techniques like LLM-as-a-Judge provides objective, data-driven insights into their performance and quality.</li>
                    <li>Implementing **Safety Guardrails** (input/output filters) is crucial for securing AI agents against vulnerabilities like prompt injection and hallucination.</li>
                    <li>**Red Teaming** AI agents involves proactively simulating adversarial attacks to discover and fix security weaknesses before deployment.</li>
                    <li>The **Capstone Project** is the pinnacle of the program, where you'll apply all AI-driven SDLC concepts to build a tangible, intelligent application.</li>
                </ul>
            </div>

            <div class="mb-6">
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Daily Wrap-up & Capstone Project Kickoff (30 min)</h4>
                <p class="text-gray-300 italic">
                    We'll consolidate today's deep dive into multi-modal AI, evaluation techniques, and AI security. The majority of this session will be dedicated to officially kicking off the Capstone Project, forming teams, and reviewing initial proposals.
                </p>
            </div>

            <div>
                <h4 class="text-xl font-semibold text-gray-200 mb-2">Preview Day 9 & 10: Capstone Build & Demos</h4>
                <p class="text-gray-300">
                    The next two days are entirely dedicated to your Capstone Project! **Days 9 and 10** will be intensive build days, culminating in project demos where you'll showcase your AI-enhanced applications and share your learnings. This is your chance to shine!
                </p>
            </div>
        </section>
    </main>

    <footer class="bg-gray-800 p-4 text-center text-gray-400 text-sm">
        <p>&copy; 2025 Booz Allen Hamilton Inc. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const collapsibleHeaders = document.querySelectorAll('.collapsible-header');

            collapsibleHeaders.forEach(header => {
                header.addEventListener('click', function() {
                    const content = this.nextElementSibling;
                    const arrow = this.querySelector('.arrow');

                    if (content.classList.contains('open')) {
                        content.classList.remove('open');
                        arrow.innerHTML = '&#9660;'; // Down arrow
                    } else {
                        content.classList.add('open');
                        arrow.innerHTML = '&#9650;'; // Up arrow
                    }
                });
            });
        });
    </script>
</body>
</html>
