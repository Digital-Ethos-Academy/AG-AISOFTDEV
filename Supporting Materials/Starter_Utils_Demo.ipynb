{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5505646",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸš€ Complete Utils.py Demo - Provider Interoperability Showcase\n",
    "\n",
    "This notebook demonstrates **all major functions** in `utils.py` and shows how they work **interchangeably** across different providers (OpenAI, Google, Anthropic, HuggingFace).\n",
    "\n",
    "## Key Features Demonstrated:\n",
    "- âœ… **recommended_models_table** - Filter and display model capabilities\n",
    "- âœ… **get_completion** - Text generation across providers\n",
    "- âœ… **get_image_generation_completion** - Image creation\n",
    "- âœ… **get_vision_completion** - Multimodal vision tasks\n",
    "- âœ… **get_image_edit_completion** - Image editing\n",
    "- âœ… **transcribe_audio** - Audio to text\n",
    "\n",
    "**Important**: The same function signatures work with ANY compatible model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete! All functions imported.\n",
      "\n",
      "ğŸ”‘ Available providers: OpenAI, Google, Anthropic, HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import all utils functions\n",
    "from utils import (\n",
    "    load_environment,\n",
    "    setup_llm_client,\n",
    "    recommended_models_table,\n",
    "    get_completion,\n",
    "    get_image_generation_completion,\n",
    "    get_vision_completion,\n",
    "    get_image_edit_completion,\n",
    "    transcribe_audio,\n",
    "    save_artifact,\n",
    "    RECOMMENDED_MODELS\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_environment()\n",
    "\n",
    "print(\"âœ… Setup complete! All functions imported.\")\n",
    "\n",
    "# Check which API keys are available\n",
    "available_providers = []\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    available_providers.append('OpenAI')\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    available_providers.append('Google')\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    available_providers.append('Anthropic')\n",
    "if os.getenv('HUGGINGFACE_API_KEY'):\n",
    "    available_providers.append('HuggingFace')\n",
    "\n",
    "print(f\"\\nğŸ”‘ Available providers: {', '.join(available_providers) if available_providers else 'None (using free models only)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-header",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Model Discovery with `recommended_models_table`\n",
    "\n",
    "This function helps you find the right model for your task. Filter by:\n",
    "- **provider**: 'openai', 'google', 'anthropic', 'huggingface'\n",
    "- **task**: 'text', 'vision', 'image', 'audio'\n",
    "- **capabilities**: text_generation, vision, image_generation, audio_transcription, image_modification\n",
    "- **requirements**: min_context, min_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "models-table-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š All available models:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| MiniMaxAI/MiniMax-M2 | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 128,000 | 8,192 |\n",
       "| Qwen/Qwen-Image | huggingface | âŒ | âŒ | âœ… | âŒ | âŒ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | âŒ | âŒ | âŒ | âœ… | âŒ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | âŒ | âŒ | âŒ | âœ… | âŒ | - | - |\n",
       "| claude-haiku-4-5-20251001 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 64,000 |\n",
       "| claude-opus-4-1-20250805 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 32,000 |\n",
       "| claude-sonnet-4-5-20250929 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 64,000 |\n",
       "| dall-e-3 | openai | âŒ | âŒ | âœ… | âŒ | âŒ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | âœ… | âœ… | âŒ | âŒ | âŒ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | âŒ | âŒ | âœ… | âœ… | âŒ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | âŒ | âŒ | âœ… | âœ… | âŒ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | âŒ | âŒ | âŒ | âŒ | âŒ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini-transcribe | openai | âŒ | âŒ | âŒ | âŒ | âœ… | - | - |\n",
       "| gpt-4o-transcribe | openai | âŒ | âŒ | âŒ | âŒ | âœ… | - | - |\n",
       "| gpt-5-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 32,768 | 8,192 |\n",
       "| o3 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | âŒ | âŒ | âœ… | âŒ | âŒ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 4,096 | 1,024 |\n",
       "| veo-3.1-fast-generate-preview | google | âŒ | âŒ | âŒ | âŒ | âŒ | 1,024 | - |\n",
       "| veo-3.1-generate-preview | google | âŒ | âŒ | âŒ | âŒ | âŒ | 1,024 | - |\n",
       "| whisper-1 | openai | âŒ | âŒ | âŒ | âŒ | âœ… | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"ğŸ“Š All available models:\")\n",
    "print(\"=\" * 60)\n",
    "_ = recommended_models_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "models-filtered",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Text models with large context (>100k tokens):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| MiniMaxAI/MiniMax-M2 | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 128,000 | 8,192 |\n",
       "| claude-haiku-4-5-20251001 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 64,000 |\n",
       "| claude-opus-4-1-20250805 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 32,000 |\n",
       "| claude-sonnet-4-5-20250929 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 64,000 |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | âœ… | âœ… | âŒ | âŒ | âŒ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-lite | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gpt-4.1 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | âœ… | âŒ | âŒ | âŒ | âŒ | 10,000,000 | 100,000 |\n",
       "| o3 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 100,000 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ–¼ï¸ Image generation models:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | âŒ | âŒ | âœ… | âŒ | âŒ | - | - |\n",
       "| dall-e-3 | openai | âŒ | âŒ | âœ… | âŒ | âŒ | - | - |\n",
       "| gemini-2.0-flash-preview-image-generation | google | âŒ | âŒ | âœ… | âœ… | âŒ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash-image-preview | google | âŒ | âŒ | âœ… | âœ… | âŒ | 32,768 | 32,768 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | âŒ | âŒ | âœ… | âŒ | âŒ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘ï¸ Vision-capable models:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| claude-haiku-4-5-20251001 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 64,000 |\n",
       "| claude-opus-4-1-20250805 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 32,000 |\n",
       "| claude-sonnet-4-5-20250929 | anthropic | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 64,000 |\n",
       "| gemini-1.5-flash | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | âœ… | âœ… | âŒ | âŒ | âŒ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-lite | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | âœ… | âœ… | âŒ | âŒ | âŒ | 1,048,576 | 65,536 |\n",
       "| gpt-4.1 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 400,000 | 128,000 |\n",
       "| o3 | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | âœ… | âœ… | âŒ | âŒ | âŒ | 200,000 | 100,000 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸµ Audio transcription models:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| gpt-4o-mini-transcribe | openai | âŒ | âŒ | âŒ | âŒ | âœ… | - | - |\n",
       "| gpt-4o-transcribe | openai | âŒ | âŒ | âŒ | âŒ | âœ… | - | - |\n",
       "| whisper-1 | openai | âŒ | âŒ | âŒ | âŒ | âœ… | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœï¸ Image editing models:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image-Edit | huggingface | âŒ | âŒ | âŒ | âœ… | âŒ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | âŒ | âŒ | âŒ | âœ… | âŒ | - | - |\n",
       "| gemini-2.0-flash-preview-image-generation | google | âŒ | âŒ | âœ… | âœ… | âŒ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash-image-preview | google | âŒ | âŒ | âœ… | âœ… | âŒ | 32,768 | 32,768 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter examples\n",
    "print(\"ğŸ¯ Text models with large context (>100k tokens):\")\n",
    "_ = recommended_models_table(text_generation=True, min_context=100_000)\n",
    "\n",
    "print(\"\\nğŸ–¼ï¸ Image generation models:\")\n",
    "_ = recommended_models_table(image_generation=True)\n",
    "\n",
    "print(\"\\nğŸ‘ï¸ Vision-capable models:\")\n",
    "_ = recommended_models_table(vision=True)\n",
    "\n",
    "print(\"\\nğŸµ Audio transcription models:\")\n",
    "_ = recommended_models_table(audio_transcription=True)\n",
    "\n",
    "print(\"\\nâœï¸ Image editing models:\")\n",
    "_ = recommended_models_table(image_modification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-gen-header",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Text Generation with `get_completion` - Provider Interoperability\n",
    "\n",
    "The **same function** works with models from different providers. Watch how we use identical code with different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-gen-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a common prompt\n",
    "prompt = \"Write a haiku about artificial intelligence.\"\n",
    "\n",
    "# Test with different providers (only those with API keys)\n",
    "test_models = []\n",
    "\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    test_models.append(('gemini-2.5-flash', 'Google'))\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    test_models.append(('gpt-4o-mini', 'OpenAI'))\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    test_models.append(('claude-sonnet-4-20250514', 'Anthropic'))\n",
    "\n",
    "# Always available (free tier)\n",
    "test_models.append(('mistralai/Mistral-7B-Instruct-v0.3', 'HuggingFace'))\n",
    "\n",
    "print(f\"ğŸ¤– Testing text generation with {len(test_models)} models...\\n\")\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, provider_name in test_models:\n",
    "    try:\n",
    "        # Setup client - SAME function for all providers\n",
    "        client, model, provider = setup_llm_client(model_name)\n",
    "        \n",
    "        if client is None:\n",
    "            print(f\"\\nâŒ {provider_name} - Could not setup {model_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Generate text - SAME function for all providers\n",
    "        result = get_completion(\n",
    "            prompt=prompt,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            api_provider=provider,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… {provider_name} ({model_name}):\")\n",
    "        print(f\"{result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ {provider_name} - Error: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ¨ Notice: Same function, different providers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "image-gen-header",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Image Generation with `get_image_generation_completion`\n",
    "\n",
    "Generate images using models from different providers with the **same interface**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-gen-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generation prompt\n",
    "image_prompt = \"A serene Japanese garden with cherry blossoms and a wooden bridge over a koi pond\"\n",
    "\n",
    "# Models that support image generation\n",
    "image_models = []\n",
    "\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    image_models.append(('gemini-2.5-flash-image-preview', 'Google'))\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    image_models.append(('dall-e-3', 'OpenAI'))\n",
    "\n",
    "# Free HuggingFace models\n",
    "image_models.append(('stabilityai/stable-diffusion-3.5-large', 'HuggingFace'))\n",
    "\n",
    "print(f\"ğŸ¨ Testing image generation with {len(image_models)} models...\\n\")\n",
    "print(f\"Prompt: '{image_prompt}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "for model_name, provider_name in image_models:\n",
    "    try:\n",
    "        print(f\"\\nğŸ–¼ï¸ Generating with {provider_name} ({model_name})...\")\n",
    "        \n",
    "        # Setup client - SAME function\n",
    "        client, model, provider = setup_llm_client(model_name)\n",
    "        \n",
    "        if client is None:\n",
    "            print(f\"âŒ Could not setup {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate image - SAME function for all providers\n",
    "        file_path, data_url = get_image_generation_completion(\n",
    "            prompt=image_prompt,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            api_provider=provider\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Success! Image saved to: {file_path}\")\n",
    "        generated_images.append((file_path, provider_name))\n",
    "        \n",
    "        # Display the image\n",
    "        if os.path.exists(file_path):\n",
    "            display(Markdown(f\"### {provider_name} Result:\"))\n",
    "            display(Image(filename=file_path, width=400))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ“ Generated {len(generated_images)} images successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vision-header",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Vision Analysis with `get_vision_completion`\n",
    "\n",
    "Analyze images using vision models from different providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vision-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a test image using PIL\n",
    "from PIL import Image as PILImage, ImageDraw, ImageFont\n",
    "import io\n",
    "\n",
    "# Create a simple test image with shapes and text\n",
    "img = PILImage.new('RGB', (400, 300), color='lightblue')\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Draw some shapes\n",
    "draw.rectangle([50, 50, 150, 150], fill='red', outline='darkred', width=3)\n",
    "draw.ellipse([200, 50, 350, 150], fill='yellow', outline='orange', width=3)\n",
    "draw.polygon([(100, 200), (150, 250), (50, 250)], fill='green', outline='darkgreen', width=3)\n",
    "\n",
    "# Add text\n",
    "draw.text((150, 200), \"Test Image\", fill='black')\n",
    "draw.text((120, 230), \"Shapes & Colors\", fill='navy')\n",
    "\n",
    "# Save the test image\n",
    "test_image_path = 'artifacts/test_vision.png'\n",
    "os.makedirs(os.path.dirname(test_image_path), exist_ok=True)\n",
    "img.save(test_image_path)\n",
    "\n",
    "print(\"Test image created:\")\n",
    "display(Image(filename=test_image_path, width=400))\n",
    "\n",
    "# Vision models to test\n",
    "vision_models = []\n",
    "\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    vision_models.append(('gemini-2.5-flash', 'Google'))\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    vision_models.append(('gpt-4o', 'OpenAI'))\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    vision_models.append(('claude-opus-4-20250514', 'Anthropic'))\n",
    "\n",
    "vision_prompt = \"Describe the shapes, colors, and text in this image. Be specific about positions and colors.\"\n",
    "\n",
    "print(f\"\\nğŸ‘ï¸ Testing vision with {len(vision_models)} models...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, provider_name in vision_models:\n",
    "    try:\n",
    "        print(f\"\\nğŸ” Analyzing with {provider_name} ({model_name})...\")\n",
    "        \n",
    "        # Setup client - SAME function\n",
    "        client, model, provider = setup_llm_client(model_name)\n",
    "        \n",
    "        if client is None:\n",
    "            print(f\"âŒ Could not setup {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze image - SAME function for all providers\n",
    "        result = get_vision_completion(\n",
    "            prompt=vision_prompt,\n",
    "            image_path_or_url=test_image_path,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            api_provider=provider\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… {provider_name} Analysis:\")\n",
    "        print(f\"{result[:500]}...\" if len(result) > 500 else result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ¨ Same vision function works across providers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "image-edit-header",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Image Editing with `get_image_edit_completion`\n",
    "\n",
    "Edit existing images using compatible models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-edit-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models that support image editing\n",
    "edit_models = [\n",
    "    ('black-forest-labs/FLUX.1-Kontext-dev', 'HuggingFace'),\n",
    "    ('Qwen/Qwen-Image-Edit', 'HuggingFace'),\n",
    "    ('gemini-2.5-flash-image-preview', 'Google'),\n",
    "    ('gemini-2.0-flash-preview-image-generation', 'Google'), \n",
    "]\n",
    "\n",
    "# Use the test image we created earlier\n",
    "edit_prompt = \"Add a rainbow in the background and make the shapes glow\"\n",
    "\n",
    "print(f\"âœï¸ Testing image editing with {len(edit_models)} models...\\n\")\n",
    "print(f\"Edit instruction: '{edit_prompt}'\\n\")\n",
    "print(\"Original image:\")\n",
    "display(Image(filename=test_image_path, width=300))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, provider_name in edit_models:\n",
    "    try:\n",
    "        print(f\"\\nğŸ¨ Editing with {provider_name} ({model_name})...\")\n",
    "        \n",
    "        # Setup client - SAME function\n",
    "        client, model, provider = setup_llm_client(model_name)\n",
    "        \n",
    "        if client is None:\n",
    "            print(f\"âŒ Could not setup {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Edit image - SAME function interface\n",
    "        edited_path, data_url = get_image_edit_completion(\n",
    "            prompt=edit_prompt,\n",
    "            image_path=test_image_path,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            api_provider=provider\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Success! Edited image saved to: {edited_path}\")\n",
    "        \n",
    "        # Display the edited image\n",
    "        if edited_path and os.path.exists(edited_path):\n",
    "            display(Markdown(f\"### {provider_name} Edited Result:\"))\n",
    "            display(Image(filename=edited_path, width=400))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audio-header",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Audio Transcription with `transcribe_audio`\n",
    "\n",
    "Transcribe audio files using different providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "audio-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a simple test audio file\n",
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "# Generate a simple sine wave audio file\n",
    "sample_rate = 44100\n",
    "duration = 2  # seconds\n",
    "frequency = 440  # A4 note\n",
    "\n",
    "t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "audio_data = np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "# Convert to 16-bit PCM\n",
    "audio_data = (audio_data * 32767).astype(np.int16)\n",
    "\n",
    "# Save as WAV file\n",
    "audio_path = 'artifacts/test_audio.wav'\n",
    "os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n",
    "\n",
    "with wave.open(audio_path, 'w') as wav_file:\n",
    "    wav_file.setnchannels(1)  # mono\n",
    "    wav_file.setsampwidth(2)  # 16-bit\n",
    "    wav_file.setframerate(sample_rate)\n",
    "    wav_file.writeframes(audio_data.tobytes())\n",
    "\n",
    "print(f\"ğŸ“ Test audio file created: {audio_path}\")\n",
    "print(f\"Duration: {duration} seconds, Frequency: {frequency} Hz\\n\")\n",
    "\n",
    "# Models that support audio transcription\n",
    "audio_models = []\n",
    "\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    audio_models.append(('whisper-1', 'OpenAI'))\n",
    "\n",
    "# Note: Google Speech-to-Text requires additional setup\n",
    "# You would need: google-cloud-speech library and service account credentials\n",
    "\n",
    "if not audio_models:\n",
    "    print(\"âš ï¸ No audio transcription models available.\")\n",
    "    print(\"To use audio transcription, you need:\")\n",
    "    print(\"  - OpenAI API key for Whisper\")\n",
    "    print(\"  - Or Google Cloud Speech-to-Text credentials\")\n",
    "else:\n",
    "    print(f\"ğŸµ Testing audio transcription with {len(audio_models)} models...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, provider_name in audio_models:\n",
    "        try:\n",
    "            print(f\"\\nğŸ¤ Transcribing with {provider_name} ({model_name})...\")\n",
    "            \n",
    "            # Setup client - SAME function\n",
    "            client, model, provider = setup_llm_client(model_name)\n",
    "            \n",
    "            if client is None:\n",
    "                print(f\"âŒ Could not setup {model_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Transcribe audio - SAME function interface\n",
    "            result = transcribe_audio(\n",
    "                audio_path=audio_path,\n",
    "                client=client,\n",
    "                model_name=model,\n",
    "                api_provider=provider,\n",
    "                language_code=\"en-US\"\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… {provider_name} Transcription:\")\n",
    "            print(f\"{result}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if \"sine wave\" in error_msg.lower() or \"speech\" in error_msg.lower():\n",
    "                print(f\"â„¹ï¸ Note: The test audio is a sine wave, not speech.\")\n",
    "                print(f\"   For real transcription, use an actual speech recording.\")\n",
    "            else:\n",
    "                print(f\"âŒ Error: {error_msg[:150]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-header",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Advanced: Multi-Provider Comparison\n",
    "\n",
    "Let's run the same creative task across multiple providers to see their different styles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative writing task for comparison\n",
    "creative_prompt = \"\"\"\n",
    "Write a 3-sentence story about a robot learning to paint. \n",
    "Include emotion and a surprising twist.\n",
    "\"\"\"\n",
    "\n",
    "# Collect available text models from different providers\n",
    "comparison_models = []\n",
    "\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    comparison_models.append(('gemini-2.5-flash', 'Google'))\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    comparison_models.append(('gpt-4o-mini', 'OpenAI'))  \n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    comparison_models.append(('claude-sonnet-4-20250514', 'Anthropic'))\n",
    "\n",
    "# Free models\n",
    "comparison_models.append(('mistralai/Mistral-7B-Instruct-v0.3', 'HuggingFace (Mistral)'))\n",
    "comparison_models.append(('meta-llama/Llama-3.3-70B-Instruct', 'HuggingFace (Llama)'))\n",
    "\n",
    "print(\"ğŸ­ Creative Writing Comparison Across Providers\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {creative_prompt.strip()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "responses = []\n",
    "\n",
    "for model_name, provider_name in comparison_models:\n",
    "    try:\n",
    "        client, model, provider = setup_llm_client(model_name)\n",
    "        \n",
    "        if client is None:\n",
    "            continue\n",
    "        \n",
    "        # Use same temperature for fair comparison\n",
    "        result = get_completion(\n",
    "            prompt=creative_prompt,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            api_provider=provider,\n",
    "            temperature=0.8  # Higher temperature for creativity\n",
    "        )\n",
    "        \n",
    "        responses.append((provider_name, model_name, result))\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Display results in a nice format\n",
    "for provider_name, model_name, response in responses:\n",
    "    print(f\"\\nğŸ“ **{provider_name}** (`{model_name}`)\")\n",
    "    print(\"-\" * 40)\n",
    "    print(response.strip())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ¯ Compared {len(responses)} different models with the SAME function!\")\n",
    "print(\"   Notice how each provider has its own style and approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## ğŸ“ Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **ğŸ”„ Provider Interoperability**: All functions work with ANY compatible model\n",
    "2. **ğŸ¯ Consistent Interface**: Same function signatures across providers\n",
    "3. **ğŸ” Easy Discovery**: Use `recommended_models_table()` to find the right model\n",
    "4. **âš¡ Simple Setup**: One `setup_llm_client()` call configures everything\n",
    "\n",
    "### Function Reference:\n",
    "\n",
    "```python\n",
    "# Setup (works for ANY model)\n",
    "client, model, provider = setup_llm_client(\"model-name\")\n",
    "\n",
    "# Text Generation (all providers)\n",
    "text = get_completion(prompt, client, model, provider)\n",
    "\n",
    "# Image Generation (dall-e-3, gemini-image, stable-diffusion, etc.)\n",
    "path, url = get_image_generation_completion(prompt, client, model, provider)\n",
    "\n",
    "# Vision Analysis (gpt-4o, gemini, claude, etc.)\n",
    "analysis = get_vision_completion(prompt, image_path, client, model, provider)\n",
    "\n",
    "# Image Editing (FLUX, Qwen-Edit, etc.)\n",
    "edited_path, url = get_image_edit_completion(prompt, image_path, client, model, provider)\n",
    "\n",
    "# Audio Transcription (whisper-1, google-stt, etc.)\n",
    "text = transcribe_audio(audio_path, client, model, provider)\n",
    "```\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Check Capabilities First**: Use `recommended_models_table()` to verify model supports your task\n",
    "2. **Handle Errors Gracefully**: Different providers may have different rate limits or availability\n",
    "3. **Use Appropriate Models**: Vision models for images, audio models for speech, etc.\n",
    "4. **Save Artifacts**: Use `save_artifact()` to organize outputs\n",
    "5. **Test Multiple Providers**: Compare results across providers for best quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final example: Building a multi-modal pipeline\n",
    "print(\"ğŸš€ Multi-Modal Pipeline Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Generate an image\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    print(\"\\n1ï¸âƒ£ Generating an image...\")\n",
    "    client, model, provider = setup_llm_client('gemini-2.5-flash-image-preview')\n",
    "    if client:\n",
    "        try:\n",
    "            img_path, _ = get_image_generation_completion(\n",
    "                \"A futuristic city with flying cars at sunset\",\n",
    "                client, model, provider\n",
    "            )\n",
    "            print(f\"   âœ… Image generated: {img_path}\")\n",
    "            \n",
    "            # Step 2: Analyze the generated image\n",
    "            print(\"\\n2ï¸âƒ£ Analyzing the generated image...\")\n",
    "            client2, model2, provider2 = setup_llm_client('gemini-2.5-flash')\n",
    "            if client2:\n",
    "                analysis = get_vision_completion(\n",
    "                    \"Describe this image in detail. What mood does it convey?\",\n",
    "                    img_path, client2, model2, provider2\n",
    "                )\n",
    "                print(f\"   âœ… Analysis: {analysis[:200]}...\")\n",
    "                \n",
    "                # Step 3: Generate a story based on the analysis\n",
    "                print(\"\\n3ï¸âƒ£ Creating a story based on the image...\")\n",
    "                story_prompt = f\"Based on this image description, write a short sci-fi story (3 paragraphs):\\n{analysis[:300]}\"\n",
    "                story = get_completion(story_prompt, client2, model2, provider2, temperature=0.9)\n",
    "                print(f\"   âœ… Story generated!\")\n",
    "                print(f\"\\nğŸ“– The Story:\\n{story[:500]}...\")\n",
    "                \n",
    "                # Save everything\n",
    "                save_artifact(f\"# Multi-Modal Pipeline Results\\n\\n## Image Analysis\\n{analysis}\\n\\n## Generated Story\\n{story}\", \n",
    "                             \"artifacts/pipeline_results.md\")\n",
    "                print(\"\\n   ğŸ“ Results saved to artifacts/pipeline_results.md\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Pipeline error: {str(e)[:100]}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ This example requires Google API key for image generation.\")\n",
    "    print(\"   Set GOOGLE_API_KEY in your .env file to run this example.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ¨ That's the power of provider interoperability!\")\n",
    "print(\"   Same functions, different providers, endless possibilities! ğŸ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
