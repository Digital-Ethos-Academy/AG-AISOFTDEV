{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5505646",
   "metadata": {},
   "source": [
    "\n",
    "# 🧪 Starter Notebook — `utils.py` Quick Demo\n",
    "\n",
    "This notebook helps you **verify your setup** and learn the **core patterns** for using `utils.py` in this course.\n",
    "\n",
    "- Safe to run **as-is** (no external API calls by default).\n",
    "- Clear, **uncomment-to-try** cells for text, vision, image gen/edit, and audio transcription once you have keys.\n",
    "- Uses the **artifact helpers** so everything saves in predictable places under `artifacts/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059160f7",
   "metadata": {},
   "source": [
    "## 1) Import `utils.py` and quick environment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfb7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ utils.py imported OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    # Assumes the notebook is in 'Supporting Materials/'\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "except IndexError:\n",
    "    # Fallback for different execution environments\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "try:\n",
    "    import utils\n",
    "    importlib.reload(utils)\n",
    "    print(\"✅ utils.py imported OK\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Could not import utils.py. Ensure it's in your project folder. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142994c6",
   "metadata": {},
   "source": [
    "## 2) Load `.env` and explore recommended models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5b052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GOOGLE_API_KEY is loaded\n",
      "API Key preview: AIzaSy...7xqk\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered examples:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import load_environment, recommended_models_table\n",
    "import os\n",
    "\n",
    "load_environment()  # prints a warning if no .env is found\n",
    "\n",
    "# Check if GOOGLE_API_KEY is loaded\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if google_api_key:\n",
    "    print(\"✅ GOOGLE_API_KEY is loaded\")\n",
    "    # Print first and last few characters for verification (without revealing the full key)\n",
    "    print(f\"API Key preview: {google_api_key[:6]}...{google_api_key[-4:]}\")\n",
    "else:\n",
    "    print(\"❌ GOOGLE_API_KEY is not loaded\")\n",
    "\n",
    "_ = recommended_models_table()  # full list\n",
    "print(\"\\nFiltered examples:\")\n",
    "_ = recommended_models_table(task=\"text\", min_context=100_000)\n",
    "_ = recommended_models_table(task=\"image\")\n",
    "_ = recommended_models_table(task=\"audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f34dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function recommended_models_table in module utils.models:\n",
      "\n",
      "recommended_models_table(task: 'str | None' = None, provider: 'str | None' = None, text_generation: 'bool | None' = None, vision: 'bool | None' = None, image_generation: 'bool | None' = None, audio_transcription: 'bool | None' = None, min_context: 'int | None' = None, min_output_tokens: 'int | None' = None, image_modification: 'bool | None' = None) -> 'str'\n",
      "    Return a markdown table of recommended models filtered by capabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(recommended_models_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cd939",
   "metadata": {},
   "source": [
    "## 3) Configure a client (no API call yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a98d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-22 14:14:42,995 ag_aisoftdev.utils INFO LLM Client configured provider=huggingface model=deepseek-ai/DeepSeek-V3.1 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: huggingface | Model: deepseek-ai/DeepSeek-V3.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils import setup_llm_client\n",
    "MODEL = \"deepseek-ai/DeepSeek-V3.1\"  # change when running locally (e.g., \"gemini-2.5-pro\", \"claude-opus-4-1-20250805\")\n",
    "client, model_name, provider = setup_llm_client(MODEL)\n",
    "print(\"Provider:\", provider, \"| Model:\", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78a26a",
   "metadata": {},
   "source": [
    "## 4) Artifact helpers (always offline-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec120501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview: # Demo artifact\n",
      "This was written by the starter notebook.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils import save_artifact, load_artifact\n",
    "save_artifact(\"# Demo artifact\\nThis was written by the starter notebook.\", \"artifacts/notes/starter_demo.md\", overwrite=True)\n",
    "print(\"Preview:\", (load_artifact(\"artifacts/notes/starter_demo.md\") or \"\")[:120])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91b874",
   "metadata": {},
   "source": [
    "## 5) (Optional) PlantUML — requires internet; leave commented if offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ac4d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 14:14:47,126 ag_aisoftdev.utils INFO PlantUML diagram rendered. provider=None model=None latency_ms=None artifacts_path=/Users/agaleana/repos/AG-AISOFTDEV/artifacts/diagrams/quickcheck.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/agaleana/repos/AG-AISOFTDEV/artifacts/diagrams/quickcheck.png')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import render_plantuml_diagram\n",
    "puml = \"\"\"\n",
    "@startuml\n",
    "actor User\n",
    "User -> API: POST /employees\n",
    "API -> DB: insert record\n",
    "@enduml\n",
    "\"\"\"\n",
    "render_plantuml_diagram(puml, \"artifacts/diagrams/quickcheck.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6b5b2",
   "metadata": {},
   "source": [
    "## 6) Text completion — uncomment when keys are set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25ae53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course. Here are 3 essential bullet tips for writing clean, maintainable FastAPI code:\n",
      "\n",
      "*   **Structure Your Project with Routers and Separate Modules**\n",
      "    Don't put all your endpoints in a single `main.py` file. Break your application into logical components using **APIRouters**.\n",
      "    *   **Example Structure:**\n",
      "        ```\n",
      "        app/\n",
      "        ├── main.py          # Import and include all routers\n",
      "        ├── api/\n",
      "        │   ├── __init__.py\n",
      "        │   ├── routers/\n",
      "        │   │   ├── __init__.py\n",
      "        │   │   ├── items.py  # Contains router for /items/*\n",
      "        │   │   └── users.py  # Contains router for /users/*\n",
      "        │   └── dependencies.py  # Reusable dependencies (e.g., get_db)\n",
      "        ├── models/          # SQLAlchemy or Pydantic models\n",
      "        ├── schemas/         # Pydantic schemas (request/response models)\n",
      "        └── core/            # Config, security, etc.\n",
      "        ```\n",
      "    *   **Why it's clean:** This promotes separation of concerns, makes code easier to navigate, and allows teams to work on different parts simultaneously.\n",
      "\n",
      "*   **Leverage Pydantic Models for All Data Validation**\n",
      "    Use Pydantic's `BaseModel` not just for response output, but rigorously for **all input and output** data structures.\n",
      "    *   **Do this:** Define explicit schemas for `Request` bodies, `Response` models, and even query parameters. Use model inheritance (e.g., a `UserBase` model) to avoid duplication between `UserCreate`, `UserUpdate`, and `UserResponse` schemas.\n",
      "    *   **Why it's clean:** It centralizes validation logic, automatically generates precise API documentation, ensures data integrity, and makes your code self-documenting and much safer.\n",
      "\n",
      "*   **Use FastAPI's Dependency Injection System Extensively**\n",
      "    Extract common logic (like database session management, authentication, permission checks, and logging) into **dependencies**. Use them as function parameters in your path operations.\n",
      "    *   **Example:** Instead of manually getting a DB session in every endpoint, create a dependency `get_db()` that yields a session and closes it automatically.\n",
      "    *   **Why it's clean:** It drastically reduces code duplication, makes your endpoints focused purely on business logic, simplifies testing (you can easily mock dependencies), and improves overall architecture by decoupling components.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils import get_completion\n",
    "print(get_completion(\"Give me 3 bullet tips for clean FastAPI code.\", \n",
    "                     client, model_name, provider, temperature=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7652b",
   "metadata": {},
   "source": [
    "## 7) Image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use a Google Image model\n",
    "# If you get a timeout, it might be because the model is taking too long to respond.\n",
    "# You can increase the timeout by setting the UTILS_TIMEOUT_READ environment variable\n",
    "# in your .env file. For example:\n",
    "# UTILS_TIMEOUT_READ=300\n",
    "\n",
    "from utils import setup_llm_client, get_image_generation_completion\n",
    "from IPython.display import Image, display\n",
    "\n",
    "IMAGE_MODEL = \"gemini-2.0-flash-preview-image-generation\"\n",
    "\n",
    "# Get the provider and client for the selected model\n",
    "image_client, model_name, image_provider = setup_llm_client(IMAGE_MODEL)\n",
    "\n",
    "# Define the prompt for the image generation\n",
    "prompt = \"A photorealistic image of a cat wearing a superhero cape\"\n",
    "\n",
    "# Generate the image\n",
    "try:\n",
    "    print(f\"Attempting to generate an image with {image_provider}:{model_name}...\")\n",
    "    img_bytes, mime = get_image_generation_completion(\n",
    "        image_client, prompt, model_name, image_provider\n",
    "    )\n",
    "\n",
    "    # Display the generated image\n",
    "    if img_bytes:\n",
    "        print(f\"Successfully generated a {mime} image.\")\n",
    "        display(Image(data=img_bytes))\n",
    "    else:\n",
    "        print(\"Image generation failed to return image data.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during image generation: {e}\")\n",
    "    # If you get a 404 error, it might be because the model is not available\n",
    "    # in your region or for your API key. Try a different model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409bd32f",
   "metadata": {},
   "source": [
    "## 8) Vision (image + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b92150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 14:16:30,259 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision not available for provider/model 'openai/gpt-4o' — [openai:] vision error: Not implemented in this environment\n",
      "Tip: Choose a vision-capable model in cell 3, e.g. 'gpt-4o' (OpenAI) or 'gemini-2.5-pro' (Google).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils import get_vision_completion_compat\n",
    "# Pick an existing image you generated above, or we make a simple fallback\n",
    "url = \"artifacts/diagrams/quickcheck.png\"\n",
    "VISION_MODEL = \"gpt-4o\"\n",
    "vision_client, vision_model_name, vision_provider = setup_llm_client(VISION_MODEL)\n",
    "\n",
    "\n",
    "\n",
    "result, err = get_vision_completion_compat(\n",
    "    \"explain this image\", url, vision_client, vision_model_name, vision_provider\n",
    ")\n",
    "if err:\n",
    "    print(\n",
    "        f\"Vision not available for provider/model '{vision_provider}/{vision_model_name}' — {err}\\n\"\n",
    "        \"Tip: Choose a vision-capable model in cell 3, e.g. 'gpt-4o' (OpenAI) or 'gemini-2.5-pro' (Google).\"\n",
    "    )\n",
    "else:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad031f6",
   "metadata": {},
   "source": [
    "## 9) Image editing — uncomment for supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import get_image_edit_completion\n",
    "from IPython.display import Image, display\n",
    "\n",
    "IMAGE_EDIT_MODEL = \"Qwen/Qwen-Image-Edit\" \n",
    "image_edit_client, image_edit_model_name, image_edit_provider = setup_llm_client(IMAGE_EDIT_MODEL)\n",
    "print(\"Provider:\", image_edit_provider, \"| Model:\", image_edit_model_name)\n",
    "\n",
    "\n",
    "edited_path, data_url_or_msg = get_image_edit_completion(\n",
    "    \"Add a monkey riding the rocket.\",\n",
    "    \"artifacts/screens/image_1758557108.png\", # <-- change the path to an existing image like the one generated above\n",
    "    image_edit_client, image_edit_model_name, image_edit_provider\n",
    ")\n",
    "\n",
    "if edited_path and data_url_or_msg.startswith(\"data:image\"):\n",
    "    # The image is automatically saved by the helper function, so we just display it.\n",
    "    display(Image(url=data_url_or_msg))\n",
    "else:\n",
    "    # If there was an error, print it\n",
    "    print(f\"An error occurred: {data_url_or_msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790cd1f",
   "metadata": {},
   "source": [
    "## 10) Audio transcription — uncomment for Whisper/Google STT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import transcribe_audio\n",
    "TRANSCRIPTION_MODEL = \"whisper-1\" \n",
    "transcription_client, transcription_model_name, transcription_provider = setup_llm_client(TRANSCRIPTION_MODEL)\n",
    "print(\"Provider:\", transcription_provider, \"| Model:\", transcription_model_name)\n",
    "\n",
    "\n",
    "transcribe_audio(\"artifacts/audio/sample.wav\", transcription_client, transcription_model_name, transcription_provider, language_code=\"en-US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445327c",
   "metadata": {},
   "source": [
    "## 11) Clean model output (strip code fences) — always safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import clean_llm_output\n",
    "raw = \"\"\"```python\n",
    "print(\"hello\")\n",
    "```\"\"\"\n",
    "print(\"Cleaned:\", clean_llm_output(raw, language=\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fec01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client(api_key=\"GOOGLE_API_KEY\")\n",
    "response = client.models.generate_content(\n",
    "   model=\"gemini-2.5-flash-image-preview\",\n",
    "   contents=(\n",
    "       \"Show me how to bake a macaron with images.\"\n",
    "   ),\n",
    "   config=types.GenerateContentConfig(\n",
    "        response_modalities=[\"TEXT\", \"IMAGE\"]\n",
    "   ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display inline images from the `response` object (GenerateContentResponse)\n",
    "parts = response.candidates[0].content.parts\n",
    "images_shown = 0\n",
    "\n",
    "for idx, part in enumerate(parts):\n",
    "    blob = getattr(part, \"inline_data\", None)\n",
    "    if not blob or not getattr(blob, \"data\", None):\n",
    "        continue\n",
    "\n",
    "    img_bytes = blob.data  # raw bytes\n",
    "    mime = getattr(blob, \"mime_type\", None) or \"image/png\"\n",
    "    fmt = mime.split(\"/\")[-1] if \"/\" in mime else mime\n",
    "\n",
    "    try:\n",
    "        # Prefer displaying directly from bytes\n",
    "        display(Image(data=img_bytes, format=fmt))\n",
    "        images_shown += 1\n",
    "    except Exception:\n",
    "        # Fallback: save to disk and display from file\n",
    "        out_path = f\"artifacts/screens/response_image_{idx}.{fmt}\"\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(img_bytes)\n",
    "        display(Image(filename=out_path))\n",
    "        images_shown += 1\n",
    "\n",
    "if images_shown == 0:\n",
    "    print(\"No inline images found in the response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Test direct Google API call with multiple attempts and longer timeout\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"API key not found\")\n",
    "else:\n",
    "    print(\"API key found, making direct call...\")\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Try multiple models\n",
    "    models_to_try = [\n",
    "        \"gemini-2.5-flash-image-preview\",\n",
    "        \"gemini-2.0-flash-preview-image-generation\",\n",
    "        \"imagen-3.0-preview\",\n",
    "        \"imagegeneration\"\n",
    "    ]\n",
    "    \n",
    "    prompt = \"A cute baby sea otter wearing a beret and glasses, reading a book by the seashore, digital art\"\n",
    "    \n",
    "    for model_name in models_to_try:\n",
    "        try:\n",
    "            print(f\"Trying model: {model_name}\")\n",
    "            response = client.models.generate_content(\n",
    "               model=model_name,\n",
    "               contents=prompt,\n",
    "               config=types.GenerateContentConfig(\n",
    "                    response_modalities=[\"TEXT\", \"IMAGE\"],\n",
    "                    http_options=types.HttpOptions(timeout=300)\n",
    "               ),\n",
    "            )\n",
    "            print(f\"Success with model {model_name}!\")\n",
    "            \n",
    "            # Display inline images from the `response` object (GenerateContentResponse)\n",
    "            parts = response.candidates[0].content.parts\n",
    "            images_shown = 0\n",
    "\n",
    "            for idx, part in enumerate(parts):\n",
    "                blob = getattr(part, \"inline_data\", None)\n",
    "                if not blob or not getattr(blob, \"data\", None):\n",
    "                    continue\n",
    "\n",
    "                img_bytes = blob.data  # raw bytes\n",
    "                mime = getattr(blob, \"mime_type\", None) or \"image/png\"\n",
    "                fmt = mime.split(\"/\")[-1] if \"/\" in mime else mime\n",
    "\n",
    "                try:\n",
    "                    # Prefer displaying directly from bytes\n",
    "                    display(Image(data=img_bytes, format=fmt))\n",
    "                    images_shown += 1\n",
    "                except Exception:\n",
    "                    # Fallback: save to disk and display from file\n",
    "                    out_path = f\"artifacts/screens/response_image_{idx}.{fmt}\"\n",
    "                    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "                    with open(out_path, \"wb\") as f:\n",
    "                        f.write(img_bytes)\n",
    "                    display(Image(filename=out_path))\n",
    "                    images_shown += 1\n",
    "\n",
    "            if images_shown == 0:\n",
    "                print(\"No inline images found in the response.\")\n",
    "            else:\n",
    "                print(f\"Successfully displayed {images_shown} images.\")\n",
    "            break  # If successful, break out of the loop\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed with model {model_name}: {e}\")\n",
    "            if \"404\" in str(e) or \"not found\" in str(e).lower():\n",
    "                continue  # Try the next model\n",
    "            else:\n",
    "                # For other errors, wait a bit and try again\n",
    "                print(\"Waiting 5 seconds before retry...\")\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    print(f\"Retrying model: {model_name}\")\n",
    "                    response = client.models.generate_content(\n",
    "                       model=model_name,\n",
    "                       contents=prompt,\n",
    "                       config=types.GenerateContentConfig(\n",
    "                            response_modalities=[\"TEXT\", \"IMAGE\"],\n",
    "                            http_options=types.HttpOptions(timeout=300)\n",
    "                       ),\n",
    "                    )\n",
    "                    print(f\"Retry success with model {model_name}!\")\n",
    "                    # Display code here (same as above)\n",
    "                    parts = response.candidates[0].content.parts\n",
    "                    images_shown = 0\n",
    "\n",
    "                    for idx, part in enumerate(parts):\n",
    "                        blob = getattr(part, \"inline_data\", None)\n",
    "                        if not blob or not getattr(blob, \"data\", None):\n",
    "                            continue\n",
    "\n",
    "                        img_bytes = blob.data  # raw bytes\n",
    "                        mime = getattr(blob, \"mime_type\", None) or \"image/png\"\n",
    "                        fmt = mime.split(\"/\")[-1] if \"/\" in mime else mime\n",
    "\n",
    "                        try:\n",
    "                            # Prefer displaying directly from bytes\n",
    "                            display(Image(data=img_bytes, format=fmt))\n",
    "                            images_shown += 1\n",
    "                        except Exception:\n",
    "                            # Fallback: save to disk and display from file\n",
    "                            out_path = f\"artifacts/screens/response_image_{idx}.{fmt}\"\n",
    "                            os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "                            with open(out_path, \"wb\") as f:\n",
    "                                f.write(img_bytes)\n",
    "                            display(Image(filename=out_path))\n",
    "                            images_shown += 1\n",
    "\n",
    "                    if images_shown == 0:\n",
    "                        print(\"No inline images found in the response.\")\n",
    "                    else:\n",
    "                        print(f\"Successfully displayed {images_shown} images.\")\n",
    "                    break  # If successful, break out of the loop\n",
    "                except Exception as retry_error:\n",
    "                    print(f\"Retry also failed: {retry_error}\")\n",
    "                    continue  # Try the next model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
