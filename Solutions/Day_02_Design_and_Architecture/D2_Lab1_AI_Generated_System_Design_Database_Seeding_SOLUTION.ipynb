{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding (Solution)\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and Python code for Day 2's first lab. It demonstrates the workflow of generating design artifacts (SQL schema, seed data) and then using code to create a physical database from them.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the `day1_prd.md` artifact from Day 1. This document is the single source of truth for our project's requirements and provides the essential context for the LLM to generate a relevant and accurate database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n",
      "✅ LLM Client configured: Using 'google' with model 'gemini-2.5-pro'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different artifacts to use the latest models from different providers.\n",
    "# - Schema generation uses a strong instruction-following model\n",
    "# - Seed data generation uses a model tuned for data generation\n",
    "schema_client, schema_model_name, schema_api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "seed_client, seed_model_name, seed_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"artifacts/day1_prd.md\")\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load day1_prd.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Explanation:**\n",
    "This prompt instructs the LLM to act as a Database Administrator (DBA). By providing the full PRD as context, we enable the LLM to understand the entities and relationships required by the application. The prompt specifically asks for `CREATE TABLE` statements, guiding the LLM to produce a ready-to-use SQL script. We then clean up the response to remove markdown fences and save the pure SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "Schema Enhanced prompt\n",
      " <prompt>\n",
      "    <persona>\n",
      "        You are an expert Database Administrator (DBA) with 15+ years of experience designing highly-normalized, production-grade SQL schemas for SQLite and other relational databases.\n",
      "    </persona>\n",
      "\n",
      "    <context>\n",
      "        You are designing the initial database for the “ConnectFlow Onboarding Platform,” whose requirements are summarized below.  \n",
      "        Key points to satisfy:  \n",
      "        •  The schema must be normalized (3rd Normal Form).  \n",
      "        •  It must, at minimum, contain a users table and an onboarding_tasks table linked by a foreign key.  \n",
      "        •  Required columns:  \n",
      "            – users: id (PK), name, email, role (values such as ‘New Hire’, ‘Manager’, ‘People Ops Admin’).  \n",
      "            – onboarding_tasks: id (PK), title, description, due_date, status (e.g., ‘Pending’, ‘Completed’), user_id (FK → users.id).  \n",
      "        •  Use SQLite conventions (INTEGER PRIMARY KEY, TEXT, DATE, etc.).  \n",
      "        •  Include any necessary indexes or CHECK constraints to enforce data integrity (e.g., status domain, unique email).  \n",
      "        •  Do NOT add any commentary or headings—return only valid raw SQL CREATE TABLE statements (plus optional CREATE INDEX lines).\n",
      "    </context>\n",
      "\n",
      "    <examples>\n",
      "        Example (guide only; do not copy table/column names):  \n",
      "        Input requirement: “A books table with unique ISBN and a foreign-keyed authors table.”  \n",
      "        Desired output:  \n",
      "        ```\n",
      "        CREATE TABLE authors (\n",
      "            id          INTEGER PRIMARY KEY,\n",
      "            name        TEXT NOT NULL\n",
      "        );\n",
      "\n",
      "        CREATE TABLE books (\n",
      "            id          INTEGER PRIMARY KEY,\n",
      "            isbn        TEXT NOT NULL UNIQUE,\n",
      "            title       TEXT NOT NULL,\n",
      "            author_id   INTEGER NOT NULL,\n",
      "            FOREIGN KEY (author_id) REFERENCES authors(id)\n",
      "        );\n",
      "\n",
      "        CREATE INDEX idx_books_author_id ON books(author_id);\n",
      "        ```\n",
      "    </examples>\n",
      "\n",
      "    <instructions>\n",
      "        1. Analyze the context and mandatory columns.  \n",
      "        2. Add any auxiliary lookup or junction tables only if absolutely necessary to preserve normalization.  \n",
      "        3. Define suitable data types, NOT NULL constraints, UNIQUE constraints, CHECK constraints, and foreign keys.  \n",
      "        4. Return the final schema as raw SQL CREATE TABLE (and optional CREATE INDEX) statements—nothing else.\n",
      "    </instructions>\n",
      "\n",
      "    <output_format>\n",
      "        Raw SQL only. No explanations, comments, or code fences.\n",
      "    </output_format>\n",
      "</prompt>\n",
      "CREATE TABLE users (\n",
      "    id          INTEGER PRIMARY KEY,\n",
      "    name        TEXT NOT NULL,\n",
      "    email       TEXT NOT NULL UNIQUE,\n",
      "    role        TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager', 'People Ops Admin'))\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    id          INTEGER PRIMARY KEY,\n",
      "    title       TEXT NOT NULL,\n",
      "    description TEXT NOT NULL,\n",
      "    due_date    DATE NOT NULL,\n",
      "    status      TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "    user_id     INTEGER NOT NULL,\n",
      "    FOREIGN KEY (user_id) REFERENCES users(id)\n",
      ");\n",
      "\n",
      "CREATE INDEX idx_onboarding_tasks_user_id ON onboarding_tasks(user_id);\n",
      "✅ Successfully saved artifact to: artifacts/schema.sql\n"
     ]
    }
   ],
   "source": [
    "schema_prompt = f\"\"\"\n",
    "You are an expert Database Administrator (DBA).\n",
    "\n",
    "Based on the following Product Requirements Document (PRD), design a normalized SQL schema for a SQLite database. The schema should include tables for users and their assigned onboarding tasks.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "The schema should have at least a `users` table and an `onboarding_tasks` table with a foreign key relationship.\n",
    "- The `users` table should include an id, name, email, and role (e.g., 'New Hire', 'Manager').\n",
    "- The `onboarding_tasks` table should include an id, a title, a description, a due_date, a status (e.g., 'Pending', 'Completed'), and a user_id foreign key.\n",
    "\n",
    "Output only the raw SQL `CREATE TABLE` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    # Enhance the raw schema prompt using the project's prompt enhancer\n",
    "    enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "    print(\"Schema Enhanced prompt\\n\", enhanced_schema_prompt)\n",
    "\n",
    "    # Send the enhanced prompt to the schema-specific LLM client\n",
    "    generated_schema = get_completion(enhanced_schema_prompt, schema_client, schema_model_name, schema_api_provider)\n",
    "\n",
    "    # Clean up the generated schema\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "\n",
    "    # Save the cleaned schema to a file\n",
    "    save_artifact(cleaned_schema, \"artifacts/schema.sql\")\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Explanation:**\n",
    "An empty database isn't very useful for development. This prompt asks the LLM to generate realistic seed data. By providing both the PRD (for thematic context) and the SQL schema (for structural correctness), we guide the LLM to create `INSERT` statements that are both thematically appropriate (e.g., onboarding-related task titles) and syntactically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "Seed Data Enhanced prompt\n",
      " <prompt>\n",
      "  <persona>\n",
      "    You are a Senior SQL Data Architect who specializes in creating realistic sample datasets for product demos and QA environments.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "    Below are two critical knowledge blocks you must use as the sole factual basis for your work.\n",
      "\n",
      "    ---BEGIN PRD---\n",
      "    # Product Requirements Document: ConnectFlow Onboarding Platform\n",
      "    (abbreviated summary)\n",
      "    • Platform supports New Hires, Managers, and People Ops Admin roles.  \n",
      "    • Core functions include personalized onboarding experiences, manager checklists, and compliance tracking.  \n",
      "    • Key user personas: Chloe (New Hire Engineer), David (Manager), Priya (People Ops Partner).\n",
      "    ---END PRD---\n",
      "\n",
      "    ---BEGIN SQL SCHEMA---\n",
      "    CREATE TABLE users (\n",
      "        id          INTEGER PRIMARY KEY,\n",
      "        name        TEXT NOT NULL,\n",
      "        email       TEXT NOT NULL UNIQUE,\n",
      "        role        TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager', 'People Ops Admin'))\n",
      "    );\n",
      "\n",
      "    CREATE TABLE onboarding_tasks (\n",
      "        id          INTEGER PRIMARY KEY,\n",
      "        title       TEXT NOT NULL,\n",
      "        description TEXT NOT NULL,\n",
      "        due_date    DATE NOT NULL,\n",
      "        status      TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "        user_id     INTEGER NOT NULL,\n",
      "        FOREIGN KEY (user_id) REFERENCES users(id)\n",
      "    );\n",
      "\n",
      "    CREATE INDEX idx_onboarding_tasks_user_id ON onboarding_tasks(user_id);\n",
      "    ---END SQL SCHEMA---\n",
      "  </context>\n",
      "\n",
      "  <examples>\n",
      "    Example (illustrative only — IDs and dates must differ in your final answer):\n",
      "\n",
      "    INSERT INTO users (id, name, email, role)\n",
      "    VALUES\n",
      "      (1, 'Sample Name', 'sample@example.com', 'New Hire');\n",
      "\n",
      "    INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id)\n",
      "    VALUES\n",
      "      (1, 'Complete HR Paperwork', 'Fill out mandatory HR documents', '2024-08-15', 'Pending', 1);\n",
      "  </examples>\n",
      "\n",
      "  <instructions>\n",
      "    1. Think step by step to assign unique primary keys, realistic names/emails, and role-appropriate tasks that align with the PRD narratives of Chloe, David, and Priya.  \n",
      "    2. Generate at least 3 users covering every role type (“New Hire”, “Manager”, “People Ops Admin”).  \n",
      "    3. Generate 5–10 onboarding_tasks records whose user_id values correctly reference the users you create. Ensure task titles/descriptions are thematically consistent with an onboarding scenario (e.g., “Setup Dev Environment”, “Review Manager Playbook”).  \n",
      "    4. Use ISO date format YYYY-MM-DD for due_date, with plausible future dates within the next 60 days.  \n",
      "    5. Randomize task status between ‘Pending’ and ‘Completed’ where appropriate.  \n",
      "    6. Guarantee email uniqueness and relational integrity (every onboarding_tasks.user_id exists in users.id).  \n",
      "    7. Output ONLY the raw SQL INSERT statements; no commentary, no code fences.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "INSERT INTO users (id, name, email, role) VALUES\n",
      "  (...), (...), ...;\n",
      "\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES\n",
      "  (...), (...), ...;\n",
      "  </output_format>\n",
      "</prompt>\n",
      "INSERT INTO users (id, name, email, role)\n",
      "VALUES\n",
      "  (101, 'Chloe Chen', 'chloe.chen@connectflow.io', 'New Hire'),\n",
      "  (201, 'David Rodriguez', 'david.rodriguez@connectflow.io', 'Manager'),\n",
      "  (301, 'Priya Sharma', 'priya.sharma@connectflow.io', 'People Ops Admin');\n",
      "\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id)\n",
      "VALUES\n",
      "  (1001, 'Complete Benefits Enrollment', 'Select and confirm your health, dental, and retirement plan options in the benefits portal.', '2024-07-15', 'Completed', 101),\n",
      "  (1002, 'Setup Development Environment', 'Follow the engineering wiki to install required software, clone the main repository, and run local tests.', '2024-07-18', 'Pending', 101),\n",
      "  (1003, 'Attend New Hire Orientation', 'Join the company-wide orientation session to learn about our mission, values, and culture.', '2024-07-12', 'Completed', 101),\n",
      "  (1004, 'Review Manager Playbook for New Hires', 'Familiarize yourself with the 30-day onboarding checklist and performance expectations for your new report.', '2024-07-10', 'Completed', 201),\n",
      "  (1005, 'Schedule 1:1 Introduction with Chloe', 'Set up an initial meeting to discuss goals, communication styles, and the 30-day plan.', '2024-07-14', 'Pending', 201),\n",
      "  (1006, 'Verify I-9 Documentation for Chloe Chen', 'Complete Section 2 of the I-9 form and upload required documents to the compliance system.', '2024-07-11', 'Completed', 301),\n",
      "  (1007, 'Provision IT Equipment', 'Order and configure a laptop and monitor for the new hire, Chloe Chen.', '2024-07-09', 'Completed', 301),\n",
      "  (1008, 'Assign Onboarding Buddy', 'Pair Chloe with an experienced team member to help with her integration into the team.', '2024-07-21', 'Pending', 201);\n",
      "✅ Successfully saved artifact to: artifacts/seed_data.sql\n"
     ]
    }
   ],
   "source": [
    "seed_data_prompt = f\"\"\"\n",
    "You are a data specialist. Based on the provided PRD and SQL schema, generate 5-10 realistic SQL `INSERT` statements to populate the tables with sample data for an onboarding tool.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**SQL Schema:**\n",
    "<schema>\n",
    "{cleaned_schema}\n",
    "</schema>\n",
    "\n",
    "Generate at least 3 users and 5 tasks assigned to those users.\n",
    "Output only the raw SQL `INSERT` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    # Enhance the seed data prompt for better structure and fidelity\n",
    "    enhanced_seed_prompt = prompt_enhancer(seed_data_prompt)\n",
    "    print(\"Seed Data Enhanced prompt\\n\", enhanced_seed_prompt)\n",
    "\n",
    "    # Use the seed-data specific client\n",
    "    generated_seed_data = get_completion(enhanced_seed_prompt, seed_client, seed_model_name, seed_api_provider)\n",
    "\n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "\n",
    "    # Save the cleaned seed data to a file\n",
    "    save_artifact(cleaned_seed_data, \"artifacts/seed_data.sql\")\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Explanation:**\n",
    "This Python function demonstrates a crucial engineering task: turning text-based artifacts into a live system component. The `create_database` function uses Python's built-in `sqlite3` library.\n",
    "1.  It establishes a connection to a database file, which creates the file if it doesn't exist.\n",
    "2.  It reads the `schema.sql` artifact and executes it. It's important to use `cursor.executescript()` here. While `cursor.execute()` is designed for a single SQL statement, `executescript()` is necessary for running a string that contains multiple SQL statements, which is exactly what our `schema.sql` and `seed_data.sql` files contain.\n",
    "3.  It then reads and executes the `seed_data.sql` artifact to populate the newly created tables.\n",
    "4.  `conn.commit()` saves all the changes to the database file.\n",
    "5.  The `finally` block ensures that `conn.close()` is always called, which is a critical best practice to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to database at /Users/armando/Documents/Github/AG-AISOFTDEV/artifacts/onboarding.db\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "\n",
    "    # Delete the old database file if it exists to start fresh\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "        print(f\"Removed existing database file at {db_path}\")\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # Read and execute the schema file\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        if schema_sql:\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Read and execute the seed data file if it exists\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "# Define file paths\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"onboarding.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
