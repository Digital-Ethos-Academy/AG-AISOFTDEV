{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding (Solution)\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and Python code for Day 2's first lab. It demonstrates the workflow of generating design artifacts (SQL schema, seed data) and then using code to create a physical database from them.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the `day1_prd.md` artifact from Day 1. This document is the single source of truth for our project's requirements and provides the essential context for the LLM to generate a relevant and accurate database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n",
      "✅ LLM Client configured: Using 'google' with model 'gemini-2.5-pro'\n",
      "✅ LLM Client configured: Using 'google' with model 'gemini-2.5-pro'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different artifacts to use the latest models from different providers.\n",
    "# - Schema generation uses a strong instruction-following model\n",
    "# - Seed data generation uses a model tuned for data generation\n",
    "schema_client, schema_model_name, schema_api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "seed_client, seed_model_name, seed_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"artifacts/day1_prd.md\")\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load day1_prd.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Explanation:**\n",
    "This prompt instructs the LLM to act as a Database Administrator (DBA). By providing the full PRD as context, we enable the LLM to understand the entities and relationships required by the application. The prompt specifically asks for `CREATE TABLE` statements, guiding the LLM to produce a ready-to-use SQL script. We then clean up the response to remove markdown fences and save the pure SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "Schema Enhanced prompt\n",
      " <prompt>\n",
      "    <persona>\n",
      "        You are a senior Database Administrator (DBA) with extensive experience designing highly-normalized relational schemas for mission-critical SaaS platforms.\n",
      "    </persona>\n",
      "\n",
      "    <context>\n",
      "        You are building the backend database for the “Nexus Onboarding Platform,” whose high-level Product Requirements Document (PRD) appears below.  \n",
      "        Key functional need for this task: store users (new hires, managers, HR, etc.) and the onboarding tasks assigned to each user.  \n",
      "        Non-functional constraints: SQLite 3, normalized tables, clear foreign-key relationships, future-proof against expansion (additional roles, task templates, progress tracking, etc.).\n",
      "        The PRD (verbatim) is provided here for reference:  \n",
      "        ```markdown\n",
      "        # Product Requirements Document: Nexus Onboarding Platform\n",
      "        …(full PRD text as given above)…\n",
      "        ```\n",
      "    </context>\n",
      "\n",
      "    <instructions>\n",
      "        1. Think step-by-step (internally) to derive the minimal yet fully normalized schema that satisfies the PRD and allows future extensibility.  \n",
      "           • At minimum include two tables: `users` and `onboarding_tasks`, with a proper foreign-key relationship.  \n",
      "           • Feel free to add junction or lookup tables (e.g., `roles`, `task_statuses`) if required for 3rd-normal-form compliance.  \n",
      "        2. Use SQLite data types (`INTEGER`, `TEXT`, `DATE`, etc.).  \n",
      "        3. Enforce data integrity with `PRIMARY KEY`, `UNIQUE`, `NOT NULL`, `CHECK`, and `FOREIGN KEY` constraints where appropriate.  \n",
      "        4. For every foreign key, specify `ON DELETE CASCADE`.  \n",
      "        5. After reasoning, output ONLY the final raw SQL `CREATE TABLE` statements—no comments, no explanations, no surrounding text.\n",
      "    </instructions>\n",
      "\n",
      "    <output_format>\n",
      "        Raw SQL `CREATE TABLE` statements, separated by semicolons. No commentary before or after.\n",
      "    </output_format>\n",
      "</prompt>\n",
      "Schema Enhanced prompt\n",
      " <prompt>\n",
      "    <persona>\n",
      "        You are a senior Database Administrator (DBA) with extensive experience designing highly-normalized relational schemas for mission-critical SaaS platforms.\n",
      "    </persona>\n",
      "\n",
      "    <context>\n",
      "        You are building the backend database for the “Nexus Onboarding Platform,” whose high-level Product Requirements Document (PRD) appears below.  \n",
      "        Key functional need for this task: store users (new hires, managers, HR, etc.) and the onboarding tasks assigned to each user.  \n",
      "        Non-functional constraints: SQLite 3, normalized tables, clear foreign-key relationships, future-proof against expansion (additional roles, task templates, progress tracking, etc.).\n",
      "        The PRD (verbatim) is provided here for reference:  \n",
      "        ```markdown\n",
      "        # Product Requirements Document: Nexus Onboarding Platform\n",
      "        …(full PRD text as given above)…\n",
      "        ```\n",
      "    </context>\n",
      "\n",
      "    <instructions>\n",
      "        1. Think step-by-step (internally) to derive the minimal yet fully normalized schema that satisfies the PRD and allows future extensibility.  \n",
      "           • At minimum include two tables: `users` and `onboarding_tasks`, with a proper foreign-key relationship.  \n",
      "           • Feel free to add junction or lookup tables (e.g., `roles`, `task_statuses`) if required for 3rd-normal-form compliance.  \n",
      "        2. Use SQLite data types (`INTEGER`, `TEXT`, `DATE`, etc.).  \n",
      "        3. Enforce data integrity with `PRIMARY KEY`, `UNIQUE`, `NOT NULL`, `CHECK`, and `FOREIGN KEY` constraints where appropriate.  \n",
      "        4. For every foreign key, specify `ON DELETE CASCADE`.  \n",
      "        5. After reasoning, output ONLY the final raw SQL `CREATE TABLE` statements—no comments, no explanations, no surrounding text.\n",
      "    </instructions>\n",
      "\n",
      "    <output_format>\n",
      "        Raw SQL `CREATE TABLE` statements, separated by semicolons. No commentary before or after.\n",
      "    </output_format>\n",
      "</prompt>\n",
      "CREATE TABLE roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    user_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES roles(role_id) ON DELETE CASCADE\n",
      ");\n",
      "\n",
      "CREATE TABLE task_templates (\n",
      "    task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    task_name TEXT NOT NULL UNIQUE,\n",
      "    description TEXT\n",
      ");\n",
      "\n",
      "CREATE TABLE task_statuses (\n",
      "    status_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    status_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    onboarding_task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    task_id INTEGER NOT NULL,\n",
      "    assigned_to INTEGER NOT NULL,\n",
      "    assigned_by INTEGER NOT NULL,\n",
      "    due_date DATE,\n",
      "    status_id INTEGER NOT NULL,\n",
      "    FOREIGN KEY (task_id) REFERENCES task_templates(task_id) ON DELETE CASCADE,\n",
      "    FOREIGN KEY (assigned_to) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "    FOREIGN KEY (assigned_by) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "    FOREIGN KEY (status_id) REFERENCES task_statuses(status_id) ON DELETE CASCADE\n",
      ");\n",
      "✅ Successfully saved artifact to: artifacts/schema.sql\n",
      "CREATE TABLE roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    user_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES roles(role_id) ON DELETE CASCADE\n",
      ");\n",
      "\n",
      "CREATE TABLE task_templates (\n",
      "    task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    task_name TEXT NOT NULL UNIQUE,\n",
      "    description TEXT\n",
      ");\n",
      "\n",
      "CREATE TABLE task_statuses (\n",
      "    status_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    status_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    onboarding_task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    task_id INTEGER NOT NULL,\n",
      "    assigned_to INTEGER NOT NULL,\n",
      "    assigned_by INTEGER NOT NULL,\n",
      "    due_date DATE,\n",
      "    status_id INTEGER NOT NULL,\n",
      "    FOREIGN KEY (task_id) REFERENCES task_templates(task_id) ON DELETE CASCADE,\n",
      "    FOREIGN KEY (assigned_to) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "    FOREIGN KEY (assigned_by) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "    FOREIGN KEY (status_id) REFERENCES task_statuses(status_id) ON DELETE CASCADE\n",
      ");\n",
      "✅ Successfully saved artifact to: artifacts/schema.sql\n"
     ]
    }
   ],
   "source": [
    "schema_prompt = f\"\"\"\n",
    "You are an expert Database Administrator (DBA).\n",
    "\n",
    "Based on the following Product Requirements Document (PRD), design a normalized SQL schema for a SQLite database. The schema should include tables for users and their assigned onboarding tasks.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "The schema should have at least a `users` table and an `onboarding_tasks` table with a foreign key relationship.\n",
    "- The `users` table should include an id, name, email, and role (e.g., 'New Hire', 'Manager').\n",
    "- The `onboarding_tasks` table should include an id, a title, a description, a due_date, a status (e.g., 'Pending', 'Completed'), and a user_id foreign key.\n",
    "\n",
    "Output only the raw SQL `CREATE TABLE` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    # Enhance the raw schema prompt using the project's prompt enhancer\n",
    "    enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "    print(\"Schema Enhanced prompt\\n\", enhanced_schema_prompt)\n",
    "\n",
    "    # Send the enhanced prompt to the schema-specific LLM client\n",
    "    generated_schema = get_completion(enhanced_schema_prompt, schema_client, schema_model_name, schema_api_provider)\n",
    "\n",
    "    # Clean up the generated schema\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "\n",
    "    # Save the cleaned schema to a file\n",
    "    save_artifact(cleaned_schema, \"artifacts/schema.sql\")\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Explanation:**\n",
    "An empty database isn't very useful for development. This prompt asks the LLM to generate realistic seed data. By providing both the PRD (for thematic context) and the SQL schema (for structural correctness), we guide the LLM to create `INSERT` statements that are both thematically appropriate (e.g., onboarding-related task titles) and syntactically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "Seed Data Enhanced prompt\n",
      " <prompt>\n",
      "\n",
      "  <persona>\n",
      "    You are a Senior Data Engineer with deep expertise in relational-data modeling and SQL data-seeding for SaaS HR platforms.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "    Below are two authoritative reference blocks you MUST use for grounding:\n",
      "\n",
      "    ---BEGIN PRD---\n",
      "    # Product Requirements Document: Nexus Onboarding Platform\n",
      "    (full text exactly as provided in the user input)\n",
      "    ---END PRD---\n",
      "\n",
      "    ---BEGIN SQL_SCHEMA---\n",
      "    CREATE TABLE roles (\n",
      "        role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        role_name TEXT NOT NULL UNIQUE\n",
      "    );\n",
      "\n",
      "    CREATE TABLE users (\n",
      "        user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        user_name TEXT NOT NULL,\n",
      "        email TEXT NOT NULL UNIQUE,\n",
      "        role_id INTEGER NOT NULL,\n",
      "        FOREIGN KEY (role_id) REFERENCES roles(role_id) ON DELETE CASCADE\n",
      "    );\n",
      "\n",
      "    CREATE TABLE task_templates (\n",
      "        task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        task_name TEXT NOT NULL UNIQUE,\n",
      "        description TEXT\n",
      "    );\n",
      "\n",
      "    CREATE TABLE task_statuses (\n",
      "        status_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        status_name TEXT NOT NULL UNIQUE\n",
      "    );\n",
      "\n",
      "    CREATE TABLE onboarding_tasks (\n",
      "        onboarding_task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        task_id INTEGER NOT NULL,\n",
      "        assigned_to INTEGER NOT NULL,\n",
      "        assigned_by INTEGER NOT NULL,\n",
      "        due_date DATE,\n",
      "        status_id INTEGER NOT NULL,\n",
      "        FOREIGN KEY (task_id) REFERENCES task_templates(task_id) ON DELETE CASCADE,\n",
      "        FOREIGN KEY (assigned_to) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "        FOREIGN KEY (assigned_by) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "        FOREIGN KEY (status_id) REFERENCES task_statuses(status_id) ON DELETE CASCADE\n",
      "    );\n",
      "    ---END SQL_SCHEMA---\n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "    1. Think step-by-step (internally) to design coherent, realistic sample data that reflects the PRD personas (e.g., Maria the Software Engineer, David the Marketing Manager, Sarah the HR Coordinator) and typical onboarding tasks (e.g., “Complete SSO Setup,” “Read Culture Deck”).\n",
      "    2. Internally map foreign-key dependencies so that INSERT order is valid (roles ➔ users ➔ task_templates & task_statuses ➔ onboarding_tasks).\n",
      "    3. Generate 5-10 SQL INSERT statements that:\n",
      "       • Insert ≥ 3 distinct roles.  \n",
      "       • Insert ≥ 3 users linked to those roles.  \n",
      "       • Insert ≥ 3 task templates.  \n",
      "       • Insert ≥ 3 task statuses (e.g., Not Started, In Progress, Completed).  \n",
      "       • Insert ≥ 5 onboarding_tasks that assign the templates to the users, each with meaningful due_date values (ISO YYYY-MM-DD) within a plausible 90-day window.\n",
      "    4. Use realistic names, emails, and due dates, but no personally identifiable real data.\n",
      "    5. Maintain referential integrity (all foreign keys must resolve).\n",
      "    6. Do NOT output any DDL or comments—only the raw SQL INSERT statements in executable order.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "    Raw SQL only. One statement per line, terminated with a semicolon, nothing else.\n",
      "  </output_format>\n",
      "\n",
      "</prompt>\n",
      "Seed Data Enhanced prompt\n",
      " <prompt>\n",
      "\n",
      "  <persona>\n",
      "    You are a Senior Data Engineer with deep expertise in relational-data modeling and SQL data-seeding for SaaS HR platforms.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "    Below are two authoritative reference blocks you MUST use for grounding:\n",
      "\n",
      "    ---BEGIN PRD---\n",
      "    # Product Requirements Document: Nexus Onboarding Platform\n",
      "    (full text exactly as provided in the user input)\n",
      "    ---END PRD---\n",
      "\n",
      "    ---BEGIN SQL_SCHEMA---\n",
      "    CREATE TABLE roles (\n",
      "        role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        role_name TEXT NOT NULL UNIQUE\n",
      "    );\n",
      "\n",
      "    CREATE TABLE users (\n",
      "        user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        user_name TEXT NOT NULL,\n",
      "        email TEXT NOT NULL UNIQUE,\n",
      "        role_id INTEGER NOT NULL,\n",
      "        FOREIGN KEY (role_id) REFERENCES roles(role_id) ON DELETE CASCADE\n",
      "    );\n",
      "\n",
      "    CREATE TABLE task_templates (\n",
      "        task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        task_name TEXT NOT NULL UNIQUE,\n",
      "        description TEXT\n",
      "    );\n",
      "\n",
      "    CREATE TABLE task_statuses (\n",
      "        status_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        status_name TEXT NOT NULL UNIQUE\n",
      "    );\n",
      "\n",
      "    CREATE TABLE onboarding_tasks (\n",
      "        onboarding_task_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        task_id INTEGER NOT NULL,\n",
      "        assigned_to INTEGER NOT NULL,\n",
      "        assigned_by INTEGER NOT NULL,\n",
      "        due_date DATE,\n",
      "        status_id INTEGER NOT NULL,\n",
      "        FOREIGN KEY (task_id) REFERENCES task_templates(task_id) ON DELETE CASCADE,\n",
      "        FOREIGN KEY (assigned_to) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "        FOREIGN KEY (assigned_by) REFERENCES users(user_id) ON DELETE CASCADE,\n",
      "        FOREIGN KEY (status_id) REFERENCES task_statuses(status_id) ON DELETE CASCADE\n",
      "    );\n",
      "    ---END SQL_SCHEMA---\n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "    1. Think step-by-step (internally) to design coherent, realistic sample data that reflects the PRD personas (e.g., Maria the Software Engineer, David the Marketing Manager, Sarah the HR Coordinator) and typical onboarding tasks (e.g., “Complete SSO Setup,” “Read Culture Deck”).\n",
      "    2. Internally map foreign-key dependencies so that INSERT order is valid (roles ➔ users ➔ task_templates & task_statuses ➔ onboarding_tasks).\n",
      "    3. Generate 5-10 SQL INSERT statements that:\n",
      "       • Insert ≥ 3 distinct roles.  \n",
      "       • Insert ≥ 3 users linked to those roles.  \n",
      "       • Insert ≥ 3 task templates.  \n",
      "       • Insert ≥ 3 task statuses (e.g., Not Started, In Progress, Completed).  \n",
      "       • Insert ≥ 5 onboarding_tasks that assign the templates to the users, each with meaningful due_date values (ISO YYYY-MM-DD) within a plausible 90-day window.\n",
      "    4. Use realistic names, emails, and due dates, but no personally identifiable real data.\n",
      "    5. Maintain referential integrity (all foreign keys must resolve).\n",
      "    6. Do NOT output any DDL or comments—only the raw SQL INSERT statements in executable order.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "    Raw SQL only. One statement per line, terminated with a semicolon, nothing else.\n",
      "  </output_format>\n",
      "\n",
      "</prompt>\n",
      "INSERT INTO roles (role_name) VALUES ('Software Engineer');\n",
      "INSERT INTO roles (role_name) VALUES ('Marketing Manager');\n",
      "INSERT INTO roles (role_name) VALUES ('HR Coordinator');\n",
      "INSERT INTO users (user_name, email, role_id) VALUES ('Maria Rodriguez', 'maria.r@nexustech.io', 1);\n",
      "INSERT INTO users (user_name, email, role_id) VALUES ('David Chen', 'david.c@nexustech.io', 2);\n",
      "INSERT INTO users (user_name, email, role_id) VALUES ('Sarah Jenkins', 'sarah.j@nexustech.io', 3);\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Complete SSO and 2FA Setup', 'Configure your Single Sign-On and Two-Factor Authentication for all company systems.');\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Read Company Culture Deck', 'Review the NexusTech Culture Deck to understand our values and mission.');\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Sign Employment Agreement', 'Digitally sign and submit your official employment agreement via the HR portal.');\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Setup Development Environment', 'Follow the engineering guide to set up your local development environment.');\n",
      "INSERT INTO task_statuses (status_name) VALUES ('Not Started');\n",
      "INSERT INTO task_statuses (status_name) VALUES ('In Progress');\n",
      "INSERT INTO task_statuses (status_name) VALUES ('Completed');\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (3, 1, 3, '2024-08-05', 3);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (1, 1, 3, '2024-08-07', 2);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (4, 1, 3, '2024-08-15', 1);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (3, 2, 3, '2024-08-05', 3);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (2, 2, 3, '2024-08-10', 1);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (1, 2, 3, '2024-08-07', 3);\n",
      "✅ Successfully saved artifact to: artifacts/seed_data.sql\n",
      "INSERT INTO roles (role_name) VALUES ('Software Engineer');\n",
      "INSERT INTO roles (role_name) VALUES ('Marketing Manager');\n",
      "INSERT INTO roles (role_name) VALUES ('HR Coordinator');\n",
      "INSERT INTO users (user_name, email, role_id) VALUES ('Maria Rodriguez', 'maria.r@nexustech.io', 1);\n",
      "INSERT INTO users (user_name, email, role_id) VALUES ('David Chen', 'david.c@nexustech.io', 2);\n",
      "INSERT INTO users (user_name, email, role_id) VALUES ('Sarah Jenkins', 'sarah.j@nexustech.io', 3);\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Complete SSO and 2FA Setup', 'Configure your Single Sign-On and Two-Factor Authentication for all company systems.');\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Read Company Culture Deck', 'Review the NexusTech Culture Deck to understand our values and mission.');\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Sign Employment Agreement', 'Digitally sign and submit your official employment agreement via the HR portal.');\n",
      "INSERT INTO task_templates (task_name, description) VALUES ('Setup Development Environment', 'Follow the engineering guide to set up your local development environment.');\n",
      "INSERT INTO task_statuses (status_name) VALUES ('Not Started');\n",
      "INSERT INTO task_statuses (status_name) VALUES ('In Progress');\n",
      "INSERT INTO task_statuses (status_name) VALUES ('Completed');\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (3, 1, 3, '2024-08-05', 3);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (1, 1, 3, '2024-08-07', 2);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (4, 1, 3, '2024-08-15', 1);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (3, 2, 3, '2024-08-05', 3);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (2, 2, 3, '2024-08-10', 1);\n",
      "INSERT INTO onboarding_tasks (task_id, assigned_to, assigned_by, due_date, status_id) VALUES (1, 2, 3, '2024-08-07', 3);\n",
      "✅ Successfully saved artifact to: artifacts/seed_data.sql\n"
     ]
    }
   ],
   "source": [
    "seed_data_prompt = f\"\"\"\n",
    "You are a data specialist. Based on the provided PRD and SQL schema, generate 5-10 realistic SQL `INSERT` statements to populate the tables with sample data for an onboarding tool.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**SQL Schema:**\n",
    "<schema>\n",
    "{cleaned_schema}\n",
    "</schema>\n",
    "\n",
    "Generate at least 3 users and 5 tasks assigned to those users.\n",
    "Output only the raw SQL `INSERT` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    # Enhance the seed data prompt for better structure and fidelity\n",
    "    enhanced_seed_prompt = prompt_enhancer(seed_data_prompt)\n",
    "    print(\"Seed Data Enhanced prompt\\n\", enhanced_seed_prompt)\n",
    "\n",
    "    # Use the seed-data specific client\n",
    "    generated_seed_data = get_completion(enhanced_seed_prompt, seed_client, seed_model_name, seed_api_provider)\n",
    "\n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "\n",
    "    # Save the cleaned seed data to a file\n",
    "    save_artifact(cleaned_seed_data, \"artifacts/seed_data.sql\")\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Explanation:**\n",
    "This Python function demonstrates a crucial engineering task: turning text-based artifacts into a live system component. The `create_database` function uses Python's built-in `sqlite3` library.\n",
    "1.  It establishes a connection to a database file, which creates the file if it doesn't exist.\n",
    "2.  It reads the `schema.sql` artifact and executes it. It's important to use `cursor.executescript()` here. While `cursor.execute()` is designed for a single SQL statement, `executescript()` is necessary for running a string that contains multiple SQL statements, which is exactly what our `schema.sql` and `seed_data.sql` files contain.\n",
    "3.  It then reads and executes the `seed_data.sql` artifact to populate the newly created tables.\n",
    "4.  `conn.commit()` saves all the changes to the database file.\n",
    "5.  The `finally` block ensures that `conn.close()` is always called, which is a critical best practice to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing database file at /Users/armando/Documents/Github/AG-AISOFTDEV/artifacts/onboarding.db\n",
      "Successfully connected to database at /Users/armando/Documents/Github/AG-AISOFTDEV/artifacts/onboarding.db\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "\n",
    "    # Delete the old database file if it exists to start fresh\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "        print(f\"Removed existing database file at {db_path}\")\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # Read and execute the schema file\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        if schema_sql:\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Read and execute the seed data file if it exists\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "# Define file paths\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"onboarding.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
