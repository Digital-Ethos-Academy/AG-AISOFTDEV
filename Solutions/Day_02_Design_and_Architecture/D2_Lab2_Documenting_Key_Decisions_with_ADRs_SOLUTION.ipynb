{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs (Solution)\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for the ADR generation lab. It demonstrates how to use an LLM for comparative research and then synthesize that research into a structured, formal document.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'huggingface' with model 'deepseek-ai/DeepSeek-V3.1'\n",
      "✅ LLM Client configured: Using 'anthropic' with model 'claude-opus-4-1-20250805'\n",
      "✅ LLM Client configured: Using 'openai' with model 'gpt-5-2025-08-07'\n",
      "✅ LLM Client configured: Using 'anthropic' with model 'claude-opus-4-1-20250805'\n",
      "✅ LLM Client configured: Using 'openai' with model 'gpt-5-2025-08-07'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, recommended_models_table, render_plantuml_diagram, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different tasks using models from different providers.\n",
    "# - Template generation: use a HuggingFace instruction-following model\n",
    "# - Research/comparison: use an Anthropic model\n",
    "# - Synthesis (final ADR): use another HuggingFace model to demonstrate multi-provider usage\n",
    "template_client, template_model_name, template_api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "research_client, research_model_name, research_api_provider = setup_llm_client(model_name=\"claude-opus-4-1-20250805\")\n",
    "synthesis_client, synthesis_model_name, synthesis_api_provider = setup_llm_client(model_name=\"gpt-5-2025-08-07\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks the LLM to generate a standard markdown template for an ADR. The key is to be specific about the sections required (`Title`, `Status`, `Context`, `Decision`, `Consequences`), which guides the LLM to produce a well-structured and reusable template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "```markdown\n",
      "# Title: [A short, descriptive title for the decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
      "\n",
      "## Context\n",
      "- [Describe the problem, the driving forces, and the constraints.]\n",
      "\n",
      "## Decision\n",
      "- [State the chosen solution clearly and concisely.]\n",
      "\n",
      "## Consequences\n",
      "- [List the positive outcomes, negative trade-offs, and any future work required.]\n",
      "```\n",
      "✅ Successfully saved artifact to: artifacts/templates/adr_template.md\n",
      "```markdown\n",
      "# Title: [A short, descriptive title for the decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
      "\n",
      "## Context\n",
      "- [Describe the problem, the driving forces, and the constraints.]\n",
      "\n",
      "## Decision\n",
      "- [State the chosen solution clearly and concisely.]\n",
      "\n",
      "## Consequences\n",
      "- [List the positive outcomes, negative trade-offs, and any future work required.]\n",
      "```\n",
      "✅ Successfully saved artifact to: artifacts/templates/adr_template.md\n"
     ]
    }
   ],
   "source": [
    "adr_template_prompt = \"\"\"You are a principal engineer who champions clear documentation. Generate a concise, reusable markdown template for an Architectural Decision Record (ADR).\n",
    "\n",
    "The template must include the following sections:\n",
    "- # Title: [A short, descriptive title for the decision]\n",
    "- **Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
    "- ## Context\n",
    "  - [Describe the problem, the driving forces, and the constraints.]\n",
    "- ## Decision\n",
    "  - [State the chosen solution clearly and concisely.]\n",
    "- ## Consequences\n",
    "  - [List the positive outcomes, negative trade-offs, and any future work required.]\"\"\"\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "enhanced_adr_template_prompt = prompt_enhancer(adr_template_prompt)\n",
    "adr_template_content = get_completion(enhanced_adr_template_prompt, template_client, template_model_name, template_api_provider)\n",
    "print(adr_template_content)\n",
    "\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"templates/adr_template.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Explanation:**\n",
    "This prompt leverages the LLM's vast training data to perform a comparative analysis. By instructing it to be an \"unbiased research assistant\" and asking for \"pros and cons for each,\" we guide the model to provide a balanced view rather than a simple recommendation. This produces a more valuable and objective input for our own decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "### 1. Executive Comparison Table\n",
      "\n",
      "| Criterion | PostgreSQL + pgvector – Pros | PostgreSQL + pgvector – Cons | Dedicated Vector DB – Pros | Dedicated Vector DB – Cons |\n",
      "|-----------|------------------------------|------------------------------|---------------------------|----------------------------|\n",
      "| *Operational Complexity* | Single database system to manage; existing PostgreSQL expertise applies; unified backup/monitoring infrastructure | Requires careful index tuning; manual optimization of HNSW parameters; potential vacuum/bloat management issues | Purpose-built for vector operations; automatic optimization; simpler configuration for vector-specific workloads | Additional system to deploy and maintain; separate backup strategies; new monitoring requirements; potential data synchronization complexity |\n",
      "| *Cost* | No additional licensing fees; leverages existing PostgreSQL infrastructure; reduced operational overhead from single system | May require larger instance sizes for mixed workloads; potential performance degradation affecting other operations | Often open-source options available; optimized resource utilization for vector operations | Additional infrastructure costs; separate operational team training; potential commercial licensing for enterprise features |\n",
      "| *Query Flexibility* | Full SQL capabilities; seamless joins with relational data; complex filtering with standard WHERE clauses; transaction support | Limited to distance metrics supported by pgvector; no built-in hybrid search without additional extensions | Native hybrid search (vector + keyword); specialized pre/post-filtering optimizations; built-in reranking capabilities | Limited or no SQL support; complex integrations for joining with business data; typically eventual consistency model |\n",
      "| *Scalability* | Vertical scaling well-understood; read replicas for query distribution; partition strategies available | Vector index rebuilding costs; memory limitations for large HNSW indexes; no native sharding for vectors | Designed for horizontal scaling; efficient memory management for billions of vectors; optimized data structures | May require significant architectural changes at scale; vendor lock-in risks; migration complexity between solutions |\n",
      "| *Other Considerations* | Mature ecosystem; ACID compliance; extensive documentation and community support | Limited to 2000 dimensions currently; fewer vector-specific features; slower feature velocity for vector capabilities | Rapid feature development; specialized vector features (quantization, GPU acceleration); purpose-built query optimizers | Younger ecosystems; varying maturity levels; potential stability concerns; limited enterprise support options |\n",
      "\n",
      "### 2. Detailed Analysis\n",
      "\n",
      "**Operational Complexity**: PostgreSQL with pgvector benefits from unified operational workflows, allowing teams to leverage existing database administration skills and tooling for monitoring, backup, and disaster recovery. However, dedicated vector databases reduce complexity for vector-specific operations through purpose-built abstractions and automatic optimization, though they introduce the overhead of managing a separate system with its own operational requirements and potential data consistency challenges.\n",
      "\n",
      "**Cost**: The PostgreSQL approach minimizes licensing and infrastructure costs by consolidating workloads, though this may necessitate over-provisioning to handle mixed workload patterns effectively. Dedicated vector databases, while often available as open-source, introduce costs through additional infrastructure, operational overhead, and potential commercial licensing for enterprise-grade features like advanced security or support contracts.\n",
      "\n",
      "**Query Flexibility**: PostgreSQL excels in scenarios requiring complex business logic, offering full SQL support for joining embeddings with structured data, implementing row-level security, and maintaining transactional consistency. Dedicated vector databases provide superior vector-specific query capabilities including hybrid search and specialized similarity algorithms, but typically require application-level orchestration for complex data relationships and lack the query expressiveness of SQL.\n",
      "\n",
      "**Scalability**: PostgreSQL's vertical scaling approach works well for moderate vector workloads but faces limitations with index memory requirements and rebuild costs as collections grow beyond hundreds of millions of vectors. Dedicated vector databases architect specifically for horizontal scaling of vector workloads, employing techniques like quantization and distributed indexes, though achieving this scale requires careful capacity planning and potential architectural refactoring.\n",
      "\n",
      "**Other Considerations**: PostgreSQL's maturity provides stability and predictability crucial for production systems, with extensive documentation and community support, though vector-specific features lag behind specialized solutions. Dedicated vector databases iterate rapidly on vector-specific capabilities but vary significantly in production readiness, with some lacking enterprise features like authentication, audit logging, or compliance certifications.\n",
      "\n",
      "### 3. Final Recommendation\n",
      "\n",
      "• **Choose PostgreSQL + pgvector when**: \n",
      "  - Vector dataset is under 10 million items with dimensions ≤ 2000\n",
      "  - Strong SQL requirements exist for joining embeddings with business data\n",
      "  - Team has deep PostgreSQL expertise but limited bandwidth for new technologies\n",
      "  - ACID compliance and transactional consistency are mandatory\n",
      "  - The use case involves frequent updates to both vectors and metadata\n",
      "\n",
      "• **Choose a dedicated vector database when**:\n",
      "  - Scaling beyond 50 million vectors or anticipating rapid growth\n",
      "  - Hybrid search (semantic + keyword) is a core requirement\n",
      "  - Vector search is the primary workload with minimal relational data needs\n",
      "  - Sub-100ms latency requirements at high QPS (>1000)\n",
      "  - Advanced features like GPU acceleration or vector quantization provide meaningful benefits\n",
      "\n",
      "• **Migration strategy**: Start with PostgreSQL + pgvector for MVP validation, establishing clear metrics for migration triggers (query latency, index build time, memory usage). Implement an abstraction layer early to facilitate future migration if needed.\n",
      "### 1. Executive Comparison Table\n",
      "\n",
      "| Criterion | PostgreSQL + pgvector – Pros | PostgreSQL + pgvector – Cons | Dedicated Vector DB – Pros | Dedicated Vector DB – Cons |\n",
      "|-----------|------------------------------|------------------------------|---------------------------|----------------------------|\n",
      "| *Operational Complexity* | Single database system to manage; existing PostgreSQL expertise applies; unified backup/monitoring infrastructure | Requires careful index tuning; manual optimization of HNSW parameters; potential vacuum/bloat management issues | Purpose-built for vector operations; automatic optimization; simpler configuration for vector-specific workloads | Additional system to deploy and maintain; separate backup strategies; new monitoring requirements; potential data synchronization complexity |\n",
      "| *Cost* | No additional licensing fees; leverages existing PostgreSQL infrastructure; reduced operational overhead from single system | May require larger instance sizes for mixed workloads; potential performance degradation affecting other operations | Often open-source options available; optimized resource utilization for vector operations | Additional infrastructure costs; separate operational team training; potential commercial licensing for enterprise features |\n",
      "| *Query Flexibility* | Full SQL capabilities; seamless joins with relational data; complex filtering with standard WHERE clauses; transaction support | Limited to distance metrics supported by pgvector; no built-in hybrid search without additional extensions | Native hybrid search (vector + keyword); specialized pre/post-filtering optimizations; built-in reranking capabilities | Limited or no SQL support; complex integrations for joining with business data; typically eventual consistency model |\n",
      "| *Scalability* | Vertical scaling well-understood; read replicas for query distribution; partition strategies available | Vector index rebuilding costs; memory limitations for large HNSW indexes; no native sharding for vectors | Designed for horizontal scaling; efficient memory management for billions of vectors; optimized data structures | May require significant architectural changes at scale; vendor lock-in risks; migration complexity between solutions |\n",
      "| *Other Considerations* | Mature ecosystem; ACID compliance; extensive documentation and community support | Limited to 2000 dimensions currently; fewer vector-specific features; slower feature velocity for vector capabilities | Rapid feature development; specialized vector features (quantization, GPU acceleration); purpose-built query optimizers | Younger ecosystems; varying maturity levels; potential stability concerns; limited enterprise support options |\n",
      "\n",
      "### 2. Detailed Analysis\n",
      "\n",
      "**Operational Complexity**: PostgreSQL with pgvector benefits from unified operational workflows, allowing teams to leverage existing database administration skills and tooling for monitoring, backup, and disaster recovery. However, dedicated vector databases reduce complexity for vector-specific operations through purpose-built abstractions and automatic optimization, though they introduce the overhead of managing a separate system with its own operational requirements and potential data consistency challenges.\n",
      "\n",
      "**Cost**: The PostgreSQL approach minimizes licensing and infrastructure costs by consolidating workloads, though this may necessitate over-provisioning to handle mixed workload patterns effectively. Dedicated vector databases, while often available as open-source, introduce costs through additional infrastructure, operational overhead, and potential commercial licensing for enterprise-grade features like advanced security or support contracts.\n",
      "\n",
      "**Query Flexibility**: PostgreSQL excels in scenarios requiring complex business logic, offering full SQL support for joining embeddings with structured data, implementing row-level security, and maintaining transactional consistency. Dedicated vector databases provide superior vector-specific query capabilities including hybrid search and specialized similarity algorithms, but typically require application-level orchestration for complex data relationships and lack the query expressiveness of SQL.\n",
      "\n",
      "**Scalability**: PostgreSQL's vertical scaling approach works well for moderate vector workloads but faces limitations with index memory requirements and rebuild costs as collections grow beyond hundreds of millions of vectors. Dedicated vector databases architect specifically for horizontal scaling of vector workloads, employing techniques like quantization and distributed indexes, though achieving this scale requires careful capacity planning and potential architectural refactoring.\n",
      "\n",
      "**Other Considerations**: PostgreSQL's maturity provides stability and predictability crucial for production systems, with extensive documentation and community support, though vector-specific features lag behind specialized solutions. Dedicated vector databases iterate rapidly on vector-specific capabilities but vary significantly in production readiness, with some lacking enterprise features like authentication, audit logging, or compliance certifications.\n",
      "\n",
      "### 3. Final Recommendation\n",
      "\n",
      "• **Choose PostgreSQL + pgvector when**: \n",
      "  - Vector dataset is under 10 million items with dimensions ≤ 2000\n",
      "  - Strong SQL requirements exist for joining embeddings with business data\n",
      "  - Team has deep PostgreSQL expertise but limited bandwidth for new technologies\n",
      "  - ACID compliance and transactional consistency are mandatory\n",
      "  - The use case involves frequent updates to both vectors and metadata\n",
      "\n",
      "• **Choose a dedicated vector database when**:\n",
      "  - Scaling beyond 50 million vectors or anticipating rapid growth\n",
      "  - Hybrid search (semantic + keyword) is a core requirement\n",
      "  - Vector search is the primary workload with minimal relational data needs\n",
      "  - Sub-100ms latency requirements at high QPS (>1000)\n",
      "  - Advanced features like GPU acceleration or vector quantization provide meaningful benefits\n",
      "\n",
      "• **Migration strategy**: Start with PostgreSQL + pgvector for MVP validation, establishing clear metrics for migration triggers (query latency, index build time, memory usage). Implement an abstraction layer early to facilitate future migration if needed.\n"
     ]
    }
   ],
   "source": [
    "db_research_prompt = \"\"\"You are an unbiased research assistant. Your task is to provide a balanced technical comparison for a software development team.\n",
    "\n",
    "For the use case of a new hire onboarding tool that needs a semantic search feature, compare and contrast the following two approaches:\n",
    "\n",
    "1.  **Approach 1:** Using PostgreSQL with the `pgvector` extension.\n",
    "2.  **Approach 2:** Using a specialized, dedicated vector database (e.g., ChromaDB, FAISS, Weaviate).\n",
    "\n",
    "Please provide a summary of the pros and cons for each approach. Consider factors like operational complexity, cost, query flexibility, and scalability for a small-to-medium sized enterprise application.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "enhanced_db_research_prompt = prompt_enhancer(db_research_prompt)\n",
    "db_research_output = get_completion(enhanced_db_research_prompt, research_client, research_model_name, research_api_provider)\n",
    "print(db_research_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Explanation:**\n",
    "This prompt demonstrates a powerful synthesis task. We provide the LLM with two key inputs: unstructured information (the research) and a desired structure (the template). The agent's job is to merge them, creating a polished, formal document. This is a repeatable pattern for turning raw analysis into professional documentation. By assigning the persona of a Staff Engineer, we guide the LLM to adopt a formal and authoritative tone suitable for an official project artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "```markdown\n",
      "# Title: Use PostgreSQL with pgvector for Vector Embedding Storage\n",
      "\n",
      "**Status:** Proposed\n",
      "\n",
      "## Context\n",
      "- We need to choose a store and query layer for vector embeddings; options evaluated:\n",
      "  - PostgreSQL with pgvector\n",
      "  - Dedicated vector database\n",
      "- Executive comparison highlights:\n",
      "  - Operational complexity:\n",
      "    - Postgres: single system, existing Postgres expertise, unified backups/monitoring\n",
      "    - Dedicated: purpose-built for vectors, auto-optimization; but adds a separate system, separate backups/monitoring, and data sync complexity\n",
      "  - Cost:\n",
      "    - Postgres: no extra license, reuse current infra, lower ops overhead; possible need for a larger instance and performance contention\n",
      "    - Dedicated: often OSS and resource-efficient for vectors; but added infra/ops costs, training, and possible commercial license\n",
      "  - Query flexibility:\n",
      "    - Postgres: full SQL joins, rich filtering, transactions; limited distance metrics, no native hybrid search\n",
      "    - Dedicated: native hybrid search and specialized optimizations; weaker/absent SQL, complex joins, eventual consistency\n",
      "  - Scalability:\n",
      "    - Postgres: vertical scaling, read replicas, partitions; costly index rebuilds, memory limits, no native vector sharding\n",
      "    - Dedicated: built for horizontal scaling to billions of vectors; risk of vendor lock-in and migration pain\n",
      "  - Other:\n",
      "    - Postgres: mature, ACID, large community; ≤ 2000 dimensions, slower vector feature velocity\n",
      "    - Dedicated: rapid feature development, GPU and quantization; younger ecosystem with stability/enterprise gaps\n",
      "- Detailed insights:\n",
      "  - Postgres + pgvector provides unified ops and ACID, but needs manual tuning and hits memory limits beyond ~10M vectors\n",
      "  - Dedicated vector DBs excel at >50M vectors, sub-100 ms latency at high QPS, hybrid search, and GPU/quantization, but add a new tech stack and potential lock-in\n",
      "- Research recommendation:\n",
      "  - Start with Postgres + pgvector for MVP (<10M vectors, ≤2000 dims, strong SQL needs, ACID)\n",
      "  - Define and track metrics: latency, index build time, memory\n",
      "  - Build an abstraction to enable migration if scale or feature needs exceed Postgres limits\n",
      "\n",
      "## Decision\n",
      "- We will use PostgreSQL with the pgvector extension for storing and querying vector embeddings.\n",
      "\n",
      "## Consequences\n",
      "- Positive outcomes:\n",
      "  - Simplified operations via a single system with existing Postgres expertise\n",
      "  - Lower operational cost by reusing current infrastructure and avoiding new licenses\n",
      "  - Full SQL joins, rich filtering, transactions, and ACID guarantees\n",
      "  - Clear initial boundaries aligned with research: ≤2000 dimensions and datasets under ~10M vectors\n",
      "- Trade-offs and risks:\n",
      "  - Requires manual tuning (index configuration, HNSW parameters) and ongoing vacuum/bloat management\n",
      "  - Limited distance metrics and slower vector feature velocity compared to dedicated systems\n",
      "  - Potential performance contention within the shared Postgres instance\n",
      "  - Scaling constraints: primarily vertical scaling, read replicas, partitions; costly index rebuilds; memory limits; no native vector sharding\n",
      "  - May fall short of sub-100 ms latency at high QPS and advanced hybrid/GPU/quantization needs at larger scales\n",
      "  - Dedicated vector DBs offer better fit beyond ~50M vectors but introduce a separate system, ops overhead, and potential vendor lock-in\n",
      "- Mitigations and follow-ups:\n",
      "  - Define and monitor metrics: query latency, index build time, memory usage\n",
      "  - Implement an abstraction layer now to reduce migration pain if Postgres limits are reached\n",
      "```\n",
      "✅ Successfully saved artifact to: artifacts/adr_001_database_choice.md\n",
      "```markdown\n",
      "# Title: Use PostgreSQL with pgvector for Vector Embedding Storage\n",
      "\n",
      "**Status:** Proposed\n",
      "\n",
      "## Context\n",
      "- We need to choose a store and query layer for vector embeddings; options evaluated:\n",
      "  - PostgreSQL with pgvector\n",
      "  - Dedicated vector database\n",
      "- Executive comparison highlights:\n",
      "  - Operational complexity:\n",
      "    - Postgres: single system, existing Postgres expertise, unified backups/monitoring\n",
      "    - Dedicated: purpose-built for vectors, auto-optimization; but adds a separate system, separate backups/monitoring, and data sync complexity\n",
      "  - Cost:\n",
      "    - Postgres: no extra license, reuse current infra, lower ops overhead; possible need for a larger instance and performance contention\n",
      "    - Dedicated: often OSS and resource-efficient for vectors; but added infra/ops costs, training, and possible commercial license\n",
      "  - Query flexibility:\n",
      "    - Postgres: full SQL joins, rich filtering, transactions; limited distance metrics, no native hybrid search\n",
      "    - Dedicated: native hybrid search and specialized optimizations; weaker/absent SQL, complex joins, eventual consistency\n",
      "  - Scalability:\n",
      "    - Postgres: vertical scaling, read replicas, partitions; costly index rebuilds, memory limits, no native vector sharding\n",
      "    - Dedicated: built for horizontal scaling to billions of vectors; risk of vendor lock-in and migration pain\n",
      "  - Other:\n",
      "    - Postgres: mature, ACID, large community; ≤ 2000 dimensions, slower vector feature velocity\n",
      "    - Dedicated: rapid feature development, GPU and quantization; younger ecosystem with stability/enterprise gaps\n",
      "- Detailed insights:\n",
      "  - Postgres + pgvector provides unified ops and ACID, but needs manual tuning and hits memory limits beyond ~10M vectors\n",
      "  - Dedicated vector DBs excel at >50M vectors, sub-100 ms latency at high QPS, hybrid search, and GPU/quantization, but add a new tech stack and potential lock-in\n",
      "- Research recommendation:\n",
      "  - Start with Postgres + pgvector for MVP (<10M vectors, ≤2000 dims, strong SQL needs, ACID)\n",
      "  - Define and track metrics: latency, index build time, memory\n",
      "  - Build an abstraction to enable migration if scale or feature needs exceed Postgres limits\n",
      "\n",
      "## Decision\n",
      "- We will use PostgreSQL with the pgvector extension for storing and querying vector embeddings.\n",
      "\n",
      "## Consequences\n",
      "- Positive outcomes:\n",
      "  - Simplified operations via a single system with existing Postgres expertise\n",
      "  - Lower operational cost by reusing current infrastructure and avoiding new licenses\n",
      "  - Full SQL joins, rich filtering, transactions, and ACID guarantees\n",
      "  - Clear initial boundaries aligned with research: ≤2000 dimensions and datasets under ~10M vectors\n",
      "- Trade-offs and risks:\n",
      "  - Requires manual tuning (index configuration, HNSW parameters) and ongoing vacuum/bloat management\n",
      "  - Limited distance metrics and slower vector feature velocity compared to dedicated systems\n",
      "  - Potential performance contention within the shared Postgres instance\n",
      "  - Scaling constraints: primarily vertical scaling, read replicas, partitions; costly index rebuilds; memory limits; no native vector sharding\n",
      "  - May fall short of sub-100 ms latency at high QPS and advanced hybrid/GPU/quantization needs at larger scales\n",
      "  - Dedicated vector DBs offer better fit beyond ~50M vectors but introduce a separate system, ops overhead, and potential vendor lock-in\n",
      "- Mitigations and follow-ups:\n",
      "  - Define and monitor metrics: query latency, index build time, memory usage\n",
      "  - Implement an abstraction layer now to reduce migration pain if Postgres limits are reached\n",
      "```\n",
      "✅ Successfully saved artifact to: artifacts/adr_001_database_choice.md\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"templates/adr_template.md\")\n",
    "\n",
    "synthesis_prompt = f\"\"\"You are a Staff Engineer responsible for documenting key architectural decisions.\n",
    "\n",
    "Your task is to populate the provided ADR template to formally document the decision to **use PostgreSQL with the pgvector extension** for our project.\n",
    "\n",
    "Use the research provided below to fill in the 'Context' and 'Consequences' sections of the template. Be thorough and objective, summarizing the key points from the research.\n",
    "\n",
    "--- ADR TEMPLATE ---\n",
    "{adr_template}\n",
    "--- END TEMPLATE ---\n",
    "\n",
    "--- RESEARCH CONTEXT ---\n",
    "{db_research_output}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "The final ADR should be complete and ready for review.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    enhanced_synthesis_prompt = prompt_enhancer(synthesis_prompt)\n",
    "    final_adr = get_completion(enhanced_synthesis_prompt, synthesis_client, synthesis_model_name, synthesis_api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"artifacts/adr_001_database_choice.md\")\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
