{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs (Solution)\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for the ADR generation lab. It demonstrates how to use an LLM for comparative research and then synthesize that research into a structured, formal document.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-5-2025-08-07'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, recommended_models_table\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-5-2025-08-07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| codex-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ✅ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3 | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-V3-Small | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-VL2 | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Small | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/Janus-Pro-7B | huggingface | ✅ | ❌ | ❌ | 8,192 | 2,048 |\n",
       "| gemini-2.0-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-live-001 | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ✅ | ✅ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-deep-think | google | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| gemini-live-2.5-flash-preview | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-veo-3 | google | ✅ | ❌ | ❌ | - | - |\n",
       "| google-cloud/speech-to-text/latest_long | google | ❌ | ❌ | ✅ | - | - |\n",
       "| google-cloud/speech-to-text/latest_short | google | ❌ | ❌ | ✅ | - | - |\n",
       "| gpt-4.1 | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-mini | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.5 | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-image-1 | openai | ✅ | ✅ | ❌ | - | - |\n",
       "| imagen-3.0-generate-002 | google | ❌ | ✅ | ❌ | - | - |\n",
       "| imagen-4.0-generate-001 | google | ❌ | ✅ | ❌ | 480 | - |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| whisper-1 | openai | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| codex-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ✅ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3 | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-V3-Small | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-VL2 | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Small | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/Janus-Pro-7B | huggingface | ✅ | ❌ | ❌ | 8,192 | 2,048 |\\n| gemini-2.0-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-live-001 | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ✅ | ✅ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-deep-think | google | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| gemini-live-2.5-flash-preview | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-veo-3 | google | ✅ | ❌ | ❌ | - | - |\\n| google-cloud/speech-to-text/latest_long | google | ❌ | ❌ | ✅ | - | - |\\n| google-cloud/speech-to-text/latest_short | google | ❌ | ❌ | ✅ | - | - |\\n| gpt-4.1 | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-mini | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.5 | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-image-1 | openai | ✅ | ✅ | ❌ | - | - |\\n| imagen-3.0-generate-002 | google | ❌ | ✅ | ❌ | - | - |\\n| imagen-4.0-generate-001 | google | ❌ | ✅ | ❌ | 480 | - |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| whisper-1 | openai | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks the LLM to generate a standard markdown template for an ADR. The key is to be specific about the sections required (`Title`, `Status`, `Context`, `Decision`, `Consequences`), which guides the LLM to produce a well-structured and reusable template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "# Title: [A short, descriptive title for the decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
      "\n",
      "## Context\n",
      "[Describe the problem, the driving forces, and the constraints.]\n",
      "\n",
      "## Decision\n",
      "[State the chosen solution clearly and concisely.]\n",
      "\n",
      "## Consequences\n",
      "- Positive\n",
      "  - [List the positive outcomes.]\n",
      "- Negative\n",
      "  - [List the negative trade-offs.]\n",
      "- Follow-up\n",
      "  - [List any future work required, mitigations, or monitoring.]\n",
      "✅ Successfully saved artifact to: templates/adr_template.md\n"
     ]
    }
   ],
   "source": [
    "adr_template_prompt = \"\"\"You are a principal engineer who champions clear documentation. Generate a concise, reusable markdown template for an Architectural Decision Record (ADR).\n",
    "\n",
    "The template must include the following sections:\n",
    "- # Title: [A short, descriptive title for the decision]\n",
    "- **Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
    "- ## Context\n",
    "  - [Describe the problem, the driving forces, and the constraints.]\n",
    "- ## Decision\n",
    "  - [State the chosen solution clearly and concisely.]\n",
    "- ## Consequences\n",
    "  - [List the positive outcomes, negative trade-offs, and any future work required.]\"\"\"\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "adr_template_content = get_completion(adr_template_prompt, client, model_name, api_provider)\n",
    "print(adr_template_content)\n",
    "\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"templates/adr_template.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Explanation:**\n",
    "This prompt leverages the LLM's vast training data to perform a comparative analysis. By instructing it to be an \"unbiased research assistant\" and asking for \"pros and cons for each,\" we guide the model to provide a balanced view rather than a simple recommendation. This produces a more valuable and objective input for our own decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "Below is a balanced comparison for a new-hire onboarding tool that needs semantic search, focusing on small-to-medium enterprise needs.\n",
      "\n",
      "Approach 1: PostgreSQL with pgvector\n",
      "Pros\n",
      "- Lower operational complexity\n",
      "  - One database to run, backup, secure, and monitor; fits existing DBA/DevOps workflows.\n",
      "  - Managed Postgres offerings now support pgvector (AWS RDS/Aurora, GCP Cloud SQL, Azure), reducing setup friction.\n",
      "- Cost-effective\n",
      "  - Avoids paying for and running a separate vector store; fewer moving parts and network hops.\n",
      "- Rich query flexibility\n",
      "  - Full SQL, joins, JSONB, role-based access control, row-level security.\n",
      "  - Easy hybrid search: combine Postgres full-text search (tsvector/BM25) with vector similarity in one query.\n",
      "  - ACID transactions let you keep embeddings, metadata, and permissions consistent.\n",
      "- Adequate performance at SME scale\n",
      "  - HNSW index in pgvector gives good recall/latency for up to low-millions of 512–1536-d vectors on modest hardware, typical for internal onboarding corpora.\n",
      "\n",
      "Cons\n",
      "- Scaling limits and tuning\n",
      "  - HNSW insert/update cost is higher; bulk loads and frequent updates can be slow. Index builds are CPU/memory intensive.\n",
      "  - Filtered nearest-neighbor queries are less optimized: you can filter in SQL, but you won’t get a single “compound” index that narrows by metadata then vectors. Workarounds (partial indexes per tenant) don’t scale to many tenants.\n",
      "  - Horizontal scale is manual (partitioning/sharding; Citus helps but introduces complexity and cross-shard merge logic).\n",
      "- Latency/recall ceiling\n",
      "  - For very large corpora, high QPS, or strict p99 latencies, specialized stores often outperform.\n",
      "- ORM/friction points\n",
      "  - Not all ORMs natively support the vector type; you may use raw SQL for vector ops.\n",
      "- Ops details to mind\n",
      "  - Autovacuum/analyze and memory settings affect performance; index bloat and reindexing can show up in heavy-write workloads.\n",
      "\n",
      "Best fit\n",
      "- You want to keep infrastructure simple and costs low.\n",
      "- Data size up to hundreds of thousands to a few million chunks, moderate QPS, predictable metadata filters.\n",
      "- Strong need for SQL joins, transactional updates, and integrated access control/compliance in a single system.\n",
      "\n",
      "Approach 2: Dedicated vector database (e.g., Weaviate; FAISS- or Chroma-based stacks)\n",
      "Pros\n",
      "- Performance and scale headroom\n",
      "  - Purpose-built ANN indexes (often HNSW with product quantization, sharding, replicas) deliver lower latencies and better recall at larger scales and higher QPS.\n",
      "  - Distributed-by-default designs simplify multi-node scale-out and cross-shard top-k merging.\n",
      "- Vector-native capabilities\n",
      "  - Efficient filtered vector search (pre-filtering via inverted/bitmap indexes, per-tenant isolation) and hybrid search often come built-in.\n",
      "  - Some offer automatic ingestion pipelines, multimodal support, metadata schema, and observability tuned for vector workloads.\n",
      "- Operational options\n",
      "  - Managed services (for Weaviate and others) can offload ops; libraries like FAISS give maximum control/lowest latency for in-process use cases.\n",
      "\n",
      "Cons\n",
      "- More moving parts and cost\n",
      "  - Another datastore to provision, secure, back up, monitor, and pay for; network hops add latency and failure modes.\n",
      "- Weaker general query flexibility\n",
      "  - Limited join semantics; you’ll denormalize metadata or keep Postgres alongside and coordinate changes across systems.\n",
      "- Consistency and compliance trade-offs\n",
      "  - Some are eventually consistent; RBAC/row-level security and audit/compliance features may lag mature RDBMSs.\n",
      "- Maturity varies by product\n",
      "  - FAISS is a library (not a DB): you must build persistence, replication, and concurrency control.\n",
      "  - Chroma is convenient for prototyping/smaller deployments but not designed for large-scale, multi-tenant production with high availability.\n",
      "\n",
      "Best fit\n",
      "- You expect millions-to-tens-of-millions of vectors, high concurrency, strict p95/p99 latency, or heavy filtered searches across many tenants.\n",
      "- You want vector-native features (advanced hybrid search, per-collection tuning, autoscaling) and can accept another datastore.\n",
      "- You prefer a managed vector DB to reduce the ops burden at larger scale.\n",
      "\n",
      "Cost, complexity, and scalability summary\n",
      "- Operational complexity\n",
      "  - Postgres + pgvector: lowest complexity; reuse existing ops tooling and skills.\n",
      "  - Dedicated vector DB: adds another system. Managed options ease ops but increase vendor dependencies.\n",
      "- Cost\n",
      "  - Postgres + pgvector: cheapest for SME scale (one managed database).\n",
      "  - Dedicated vector DB: extra infra/service cost; worth it when latency/scale demands justify it.\n",
      "- Query flexibility\n",
      "  - Postgres + pgvector: strongest (SQL, joins, full-text, JSONB, transactions, RLS).\n",
      "  - Dedicated vector DB: good vector/filter/hybrid features but weaker for relational logic; often paired with a relational DB anyway.\n",
      "- Scalability/performance\n",
      "  - Postgres + pgvector: solid up to low-millions vectors and moderate QPS; more tuning required for heavy filters and multi-tenant isolation.\n",
      "  - Dedicated vector DB: better for very large corpora, high QPS, complex filtering, and strict low-latency SLAs.\n",
      "\n",
      "Practical recommendation for a new-hire onboarding tool\n",
      "- Default choice: Postgres + pgvector. Internal knowledge bases typically have tens to hundreds of thousands of chunks, modest QPS, and benefit from ACID, joins, and integrated access control. It keeps cost and ops low and is “good enough” with HNSW and a hybrid full-text + vector approach.\n",
      "- Consider a dedicated vector DB if you anticipate rapid growth to millions of chunks per tenant, many tenants with strict per-tenant filters, or demanding latency/throughput requirements. In that case, choose a mature, production-oriented option (e.g., Weaviate managed) rather than rolling FAISS/Chroma unless you’re ready to build persistence, HA, and ops around them.\n",
      "\n",
      "Tips if you choose Postgres + pgvector\n",
      "- Use HNSW indexes (good recall/latency); batch inserts and then build the index to speed up ingestion.\n",
      "- Combine tsvector (GIN) with vector distance and rerank top candidates for quality.\n",
      "- Partition by tenant or logical domain to improve filtered search; consider partial indexes when tenants are few and large.\n",
      "- Monitor index size, autovacuum, and memory settings; plan for periodic reindexing if write-heavy.\n"
     ]
    }
   ],
   "source": [
    "db_research_prompt = \"\"\"You are an unbiased research assistant. Your task is to provide a balanced technical comparison for a software development team.\n",
    "\n",
    "For the use case of a new hire onboarding tool that needs a semantic search feature, compare and contrast the following two approaches:\n",
    "\n",
    "1.  **Approach 1:** Using PostgreSQL with the `pgvector` extension.\n",
    "2.  **Approach 2:** Using a specialized, dedicated vector database (e.g., ChromaDB, FAISS, Weaviate).\n",
    "\n",
    "Please provide a summary of the pros and cons for each approach. Consider factors like operational complexity, cost, query flexibility, and scalability for a small-to-medium sized enterprise application.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "db_research_output = get_completion(db_research_prompt, client, model_name, api_provider)\n",
    "print(db_research_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Explanation:**\n",
    "This prompt demonstrates a powerful synthesis task. We provide the LLM with two key inputs: unstructured information (the research) and a desired structure (the template). The agent's job is to merge them, creating a polished, formal document. This is a repeatable pattern for turning raw analysis into professional documentation. By assigning the persona of a Staff Engineer, we guide the LLM to adopt a formal and authoritative tone suitable for an official project artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "# Title: Adopt PostgreSQL with pgvector for semantic search and storage\n",
      "\n",
      "Status: Accepted\n",
      "\n",
      "## Context\n",
      "We are building a new-hire onboarding tool that requires semantic search over internal documents and knowledge chunks. The expected usage profile and constraints are consistent with a small-to-medium enterprise (SME) environment:\n",
      "- Data characteristics and scale\n",
      "  - Corpus size in the tens to hundreds of thousands of chunks initially, with a plausible path to low-millions of 512–1536-dimension embeddings.\n",
      "  - Moderate query per second (QPS) requirements; predictable metadata filters (e.g., department, role, location).\n",
      "- Functional requirements\n",
      "  - Hybrid search: combine semantic similarity with keyword/full-text search.\n",
      "  - Rich relational queries and joins across content, users, permissions, and metadata.\n",
      "  - ACID guarantees to keep embeddings, metadata, and permissions consistent.\n",
      "  - Integrated access control (RBAC/row-level security) and audit/compliance aligned with existing RDBMS practices.\n",
      "- Operational constraints\n",
      "  - Minimize operational complexity and cost; prefer leveraging existing DBA/DevOps workflows and tooling.\n",
      "  - Prefer a single managed database service for availability, backups, security, monitoring, and compliance.\n",
      "  - Avoid introducing and operating an additional specialized datastore unless scale or latency needs justify it.\n",
      "\n",
      "Two primary approaches were evaluated:\n",
      "1) PostgreSQL with the pgvector extension\n",
      "- Pros: lowest operational complexity (one DB), cost-effective (no separate vector store), rich SQL/JSONB/joins, RLS/RBAC, easy hybrid search (Postgres full-text + vector similarity), and ACID transactions. Managed offerings (AWS RDS/Aurora, GCP Cloud SQL, Azure) support pgvector, reducing setup friction. HNSW indexes provide adequate recall/latency for low-millions vectors on modest hardware, which matches our expected scale.\n",
      "- Cons: scaling and tuning overheads for large writes and frequent updates (HNSW insert/update cost; index builds are CPU/memory intensive). Filtered ANN queries lack a single compound metadata+vector index, so complex filtering at large scale can underperform; workarounds like partial indexes per tenant don’t scale well with many tenants. Horizontal scale requires partitioning/sharding (e.g., Citus) and adds complexity. Some ORMs lack native vector-type support. Operational tuning (autovacuum/analyze, memory, index bloat/reindexing) matters more under heavy writes.\n",
      "\n",
      "2) Dedicated vector databases (e.g., Weaviate; FAISS-/Chroma-based stacks)\n",
      "- Pros: better performance at very large scales/high QPS with low latency SLAs; vector-native filtered search and hybrid search; distributed-by-default scale-out; managed options available; libraries like FAISS offer maximum control for in-process use.\n",
      "- Cons: introduces another datastore to provision, secure, back up, and monitor; additional cost and network hops; weaker relational query capabilities result in denormalization or dual-write/coordinated consistency with Postgres; RBAC/RLS/compliance may lag mature RDBMSs; product maturity varies (FAISS is a library, Chroma not intended for large-scale HA production).\n",
      "\n",
      "Given our expected data size, moderate performance needs, strong requirement for relational logic and permissions, and desire to keep costs and operations simple, PostgreSQL with pgvector aligns best with the project’s goals. We will keep a migration path in mind should scale, multi-tenancy breadth, or latency SLOs change materially.\n",
      "\n",
      "## Decision\n",
      "Use PostgreSQL with the pgvector extension as the primary datastore for embeddings and metadata. Implement HNSW indexes for vector similarity, combine Postgres full-text search (tsvector/BM25) with vector distance for hybrid search in a single SQL query, and leverage Postgres RBAC/row-level security and ACID transactions to keep data and permissions consistent. Prefer a managed Postgres offering that supports pgvector.\n",
      "\n",
      "## Consequences\n",
      "- Positive\n",
      "  - Lower operational complexity: single database to run, back up, secure, and monitor; reuses existing DBA/DevOps skills and tooling.\n",
      "  - Cost-effective at SME scale: avoids an additional vector store and associated infra/service costs; fewer network hops and failure modes.\n",
      "  - Rich query flexibility: full SQL, joins, JSONB, transactional updates, and integrated RBAC/row-level security in one system.\n",
      "  - Hybrid search in one place: combine Postgres full-text search with vector similarity and reranking without cross-system coordination.\n",
      "  - Adequate performance for our scale: HNSW in pgvector provides good recall/latency for low-millions vectors and moderate QPS typical of internal onboarding tools.\n",
      "  - Managed support available: pgvector is supported on major managed Postgres platforms (AWS RDS/Aurora, GCP Cloud SQL, Azure), reducing setup friction.\n",
      "\n",
      "- Negative\n",
      "  - Write and indexing costs: HNSW index builds are CPU/memory intensive; inserts/updates are slower than simple B-tree workloads; bulk loads and frequent updates may require careful batching.\n",
      "  - Filtered ANN limitations: no single compound metadata+vector index; complex or highly selective filters (esp. many-tenant scenarios) can degrade performance; partial indexes per tenant don’t scale to large tenant counts.\n",
      "  - Horizontal scale complexity: sharding/partitioning (or Citus) adds operational and query-planning complexity and cross-shard merge logic.\n",
      "  - ORM support gaps: some ORMs lack native vector type/operators, necessitating raw SQL or extensions.\n",
      "  - Operational tuning: performance sensitive to autovacuum/analyze settings, memory/work_mem, and index bloat; heavy-write workloads may require periodic reindexing and careful monitoring.\n",
      "  - Latency/recall ceiling: for very large corpora, high concurrency, or strict p95/p99 latency goals, specialized vector databases may outperform pgvector.\n",
      "\n",
      "- Follow-up\n",
      "  - Implementation\n",
      "    - Use a managed Postgres that supports pgvector; enable pgvector extension.\n",
      "    - Model schema to store embeddings, metadata, and permissions together; enable RLS where appropriate.\n",
      "    - Build HNSW indexes for vector columns; use tsvector + GIN for full-text; implement hybrid queries with reranking.\n",
      "    - Prefer batch ingestion; consider building HNSW after bulk load to reduce ingestion time.\n",
      "  - Performance and scale\n",
      "    - Establish SLOs (target/peak QPS, p95/p99 latency, acceptable recall); create load tests and benchmarks against representative corpora.\n",
      "    - Partition by tenant or logical domain if filtered searches become a bottleneck; evaluate partial indexes only when tenants are few and large.\n",
      "    - Tune autovacuum/analyze and memory parameters; monitor index size and bloat; plan for periodic reindexing under heavy writes.\n",
      "  - Tooling and developer experience\n",
      "    - Validate ORM capabilities; standardize on raw SQL for vector ops if needed and provide repository utilities.\n",
      "  - Risk management and migration\n",
      "    - Define scale thresholds that trigger reevaluation (e.g., >5–10M vectors per tenant, sustained high concurrency, stringent p99 latency, or heavy multi-tenant filtered queries).\n",
      "    - If thresholds are exceeded, assess a managed dedicated vector DB (e.g., Weaviate) and design a migration/dual-write plan, including consistency, HA, and cost implications.\n",
      "✅ Successfully saved artifact to: artifacts/adr_001_database_choice.md\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"templates/adr_template.md\")\n",
    "\n",
    "synthesis_prompt = f\"\"\"You are a Staff Engineer responsible for documenting key architectural decisions.\n",
    "\n",
    "Your task is to populate the provided ADR template to formally document the decision to **use PostgreSQL with the pgvector extension** for our project.\n",
    "\n",
    "Use the research provided below to fill in the 'Context' and 'Consequences' sections of the template. Be thorough and objective, summarizing the key points from the research.\n",
    "\n",
    "--- ADR TEMPLATE ---\n",
    "{adr_template}\n",
    "--- END TEMPLATE ---\n",
    "\n",
    "--- RESEARCH CONTEXT ---\n",
    "{db_research_output}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "The final ADR should be complete and ready for review.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    final_adr = get_completion(synthesis_prompt, client, model_name, api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"artifacts/adr_001_database_choice.md\")\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
