{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs (Solution)\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for the ADR generation lab. It demonstrates how to use an LLM for comparative research and then synthesize that research into a structured, formal document.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-5-2025-08-07'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, recommended_models_table\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-5-2025-08-07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| codex-mini-latest | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ✅ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3 | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-V3-Small | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-VL2 | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Small | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/Janus-Pro-7B | huggingface | ✅ | ❌ | ❌ | 0 | 0 |\n",
       "| gemini-2.0-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-live-001 | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ✅ | ✅ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-veo-3 | google | ✅ | ❌ | ❌ | - | - |\n",
       "| google-cloud/speech-to-text/latest_long | google | ❌ | ❌ | ✅ | - | - |\n",
       "| google-cloud/speech-to-text/latest_short | google | ❌ | ❌ | ✅ | - | - |\n",
       "| gpt-4.1 | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-mini | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.5 | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-image-1 | openai | ✅ | ✅ | ❌ | - | - |\n",
       "| imagen-3.0-generate-002 | google | ❌ | ✅ | ❌ | - | - |\n",
       "| imagen-4.0-generate-001 | google | ❌ | ✅ | ❌ | 480 | - |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| whisper-1 | openai | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| codex-mini-latest | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ✅ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3 | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-V3-Small | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-VL2 | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Small | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/Janus-Pro-7B | huggingface | ✅ | ❌ | ❌ | 0 | 0 |\\n| gemini-2.0-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-live-001 | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ✅ | ✅ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-veo-3 | google | ✅ | ❌ | ❌ | - | - |\\n| google-cloud/speech-to-text/latest_long | google | ❌ | ❌ | ✅ | - | - |\\n| google-cloud/speech-to-text/latest_short | google | ❌ | ❌ | ✅ | - | - |\\n| gpt-4.1 | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-mini | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.5 | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-image-1 | openai | ✅ | ✅ | ❌ | - | - |\\n| imagen-3.0-generate-002 | google | ❌ | ✅ | ❌ | - | - |\\n| imagen-4.0-generate-001 | google | ❌ | ✅ | ❌ | 480 | - |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| whisper-1 | openai | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks the LLM to generate a standard markdown template for an ADR. The key is to be specific about the sections required (`Title`, `Status`, `Context`, `Decision`, `Consequences`), which guides the LLM to produce a well-structured and reusable template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "```markdown\n",
      "# Title: [A short, descriptive title for the decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
      "\n",
      "## Context\n",
      "- Problem: [Describe the problem.]\n",
      "- Driving forces: [Key goals, quality attributes, stakeholders, deadlines.]\n",
      "- Constraints: [Technical, organizational, legal, budget, legacy, etc.]\n",
      "\n",
      "## Decision\n",
      "- [State the chosen solution clearly and concisely.]\n",
      "- Rationale: [Why this option over others; key trade-offs considered.]\n",
      "\n",
      "## Consequences\n",
      "- Positive outcomes: [Benefits and desired results.]\n",
      "- Negative trade-offs: [Costs, risks, compromises, new limitations.]\n",
      "- Future work: [Follow-ups, migration/cleanup tasks, monitoring, review date.]\n",
      "```\n",
      "✅ Successfully saved artifact to: templates/adr_template.md\n"
     ]
    }
   ],
   "source": [
    "adr_template_prompt = \"\"\"You are a principal engineer who champions clear documentation. Generate a concise, reusable markdown template for an Architectural Decision Record (ADR).\n",
    "\n",
    "The template must include the following sections:\n",
    "- # Title: [A short, descriptive title for the decision]\n",
    "- **Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
    "- ## Context\n",
    "  - [Describe the problem, the driving forces, and the constraints.]\n",
    "- ## Decision\n",
    "  - [State the chosen solution clearly and concisely.]\n",
    "- ## Consequences\n",
    "  - [List the positive outcomes, negative trade-offs, and any future work required.]\"\"\"\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "adr_template_content = get_completion(adr_template_prompt, client, model_name, api_provider)\n",
    "print(adr_template_content)\n",
    "\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"templates/adr_template.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Explanation:**\n",
    "This prompt leverages the LLM's vast training data to perform a comparative analysis. By instructing it to be an \"unbiased research assistant\" and asking for \"pros and cons for each,\" we guide the model to provide a balanced view rather than a simple recommendation. This produces a more valuable and objective input for our own decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "Below is a balanced comparison for a small-to-medium enterprise onboarding tool that needs semantic search.\n",
      "\n",
      "Approach 1: PostgreSQL with pgvector\n",
      "\n",
      "Pros\n",
      "- Low operational overhead and cost\n",
      "  - One datastore to run, back up, monitor, and secure (you likely already run Postgres).\n",
      "  - No extra service or network hop; easier local/dev parity.\n",
      "  - Managed Postgres offerings (AWS RDS, GCP Cloud SQL, Azure) commonly support pgvector.\n",
      "- Strong query flexibility\n",
      "  - Full SQL with joins, filters, aggregations, window functions, and transactions.\n",
      "  - Easy to combine semantic search with relational filters (department, role, region) and permissions (row-level security).\n",
      "  - Hybrid search is straightforward by combining pgvector with Postgres full-text (tsvector/tsrank) or trigram.\n",
      "- Data integrity and governance\n",
      "  - ACID, migrations, backups (PITR), auditing, RLS; mature tooling and compliance posture.\n",
      "  - Store document, metadata, and embedding in one place; updates are transactional.\n",
      "- Adequate performance for SME scale\n",
      "  - HNSW and IVFFlat indexes are available; good latency with up to low millions of vectors and modest QPS.\n",
      "\n",
      "Cons\n",
      "- Scaling ceilings and tuning complexity\n",
      "  - HNSW is memory-hungry; index builds/maintenance can be slow for very large corpora.\n",
      "  - IVFFlat requires training and careful tuning; recall/latency trade-offs can be non-trivial.\n",
      "  - Horizontal sharding for vectors is DIY; Postgres isn’t optimized for very large, high-QPS vector workloads.\n",
      "- Fewer vector-native features\n",
      "  - No built-in GPU acceleration or product quantization; fewer compression/index variants than specialized systems.\n",
      "  - Advanced vector ops (e.g., multi-vector per doc with weighting, cross-modal embeddings) require more app logic.\n",
      "- Operational nuances\n",
      "  - Large index rebuilds during restore/reindex can be time-consuming.\n",
      "  - You must manage vacuum/analyze and planner hints for predictable performance.\n",
      "\n",
      "Approach 2: Specialized vector database (e.g., Weaviate) or library (e.g., FAISS), or dev-first store (e.g., Chroma)\n",
      "\n",
      "Pros\n",
      "- High performance at larger scale\n",
      "  - Optimized ANN implementations (HNSW, IVF-PQ, DiskANN-like approaches depending on product).\n",
      "  - Better latency/throughput and higher recall at 10M+ vectors or higher QPS; some support GPU acceleration.\n",
      "- Vector-native features\n",
      "  - Built-in hybrid search (dense + BM25), re-ranking, multi-vector fields, cross-modal search, and configurable recall/latency controls.\n",
      "  - Rich vector ops and observability around recall/precision trade-offs.\n",
      "- Elastic scaling and operational features (varies by product)\n",
      "  - Weaviate and managed services provide sharding, replication, and auto-scaling; good for growing corpora and teams.\n",
      "  - Some offer ingestion pipelines, background indexing, and schema tooling suited to RAG/semantic apps.\n",
      "\n",
      "Cons\n",
      "- Additional infrastructure and potential vendor lock-in\n",
      "  - Another datastore to run, secure, back up, monitor, and pay for (or a managed service bill).\n",
      "  - Consistency and data modeling complexity if your source of truth stays in Postgres (dual writes, sync jobs, eventual consistency).\n",
      "- Query flexibility gaps\n",
      "  - Metadata filters are supported, but full relational joins/transactions are limited or non-existent; often you fetch IDs and re-join in your app or keep a separate cache.\n",
      "  - Security models are typically simpler than Postgres RLS; multi-tenant isolation often relies on collections/namespaces.\n",
      "- Product maturity varies\n",
      "  - FAISS is a library (you must build the service, persistence, and filtering around it).\n",
      "  - Chroma is great for prototyping and small workloads, less suited to large-scale production.\n",
      "  - Self-hosting distributed vector DBs adds ops complexity comparable to running a search cluster.\n",
      "- Cost can rise with scale\n",
      "  - Memory-heavy indexes, possible GPU needs, and/or managed-service pricing per vector/throughput.\n",
      "\n",
      "Head-to-head guidance for an SME onboarding tool\n",
      "\n",
      "- Operational complexity\n",
      "  - Lowest: Postgres + pgvector (single system, mature ops, transactional updates).\n",
      "  - Higher: Dedicated vector DB (extra service, backups, security, monitoring). Managed options reduce ops but add vendor dependence.\n",
      "- Cost\n",
      "  - Typically lower with pgvector, especially if you already have Postgres capacity.\n",
      "  - Dedicated vector DB adds infra or managed-service costs; may pay off only at higher scale/perf needs.\n",
      "- Query flexibility\n",
      "  - Strongest with Postgres (SQL, joins, hybrid with FTS, RLS).\n",
      "  - Vector DBs provide good metadata filters and vector features, but complex joins and strict transactional semantics are limited.\n",
      "- Scalability/performance\n",
      "  - Pgvector is solid up to low millions of embeddings and modest QPS; beyond that, tuning and memory requirements grow.\n",
      "  - Vector DBs shine for larger corpora, higher QPS, lower latency targets, advanced vector features, or GPU use.\n",
      "\n",
      "Pragmatic recommendation\n",
      "\n",
      "- Choose Postgres + pgvector if:\n",
      "  - Your corpus is small-to-medium (for example, up to a few million chunks), QPS is modest, and you want fast delivery with minimal ops.\n",
      "  - You need strong relational filtering, RBAC/RLS, and transactional consistency between documents and embeddings.\n",
      "  - You prefer to keep infra simple and costs predictable, and can accept slightly higher latencies than a specialized engine at very large scale.\n",
      "\n",
      "- Choose a specialized vector DB if:\n",
      "  - You expect rapid growth beyond a few million vectors, high QPS, or strict low-latency SLAs.\n",
      "  - You need advanced vector features (e.g., hybrid dense+sparse with tuned weighting, cross-modal search, multi-vector fields) and are comfortable with an additional datastore or a managed service.\n",
      "  - You’re building a search-centric system where vector performance is the primary bottleneck, and you can handle app-level joins and consistency.\n",
      "\n",
      "If uncertain, a common path is to start with Postgres + pgvector (fewer moving parts, fast to ship) and define clear thresholds to revisit a dedicated vector DB (e.g., median latency > X ms at Y QPS, corpus > Z million vectors, recall below target despite tuning). This reduces upfront complexity while leaving room to migrate if scale and requirements demand it.\n"
     ]
    }
   ],
   "source": [
    "db_research_prompt = \"\"\"You are an unbiased research assistant. Your task is to provide a balanced technical comparison for a software development team.\n",
    "\n",
    "For the use case of a new hire onboarding tool that needs a semantic search feature, compare and contrast the following two approaches:\n",
    "\n",
    "1.  **Approach 1:** Using PostgreSQL with the `pgvector` extension.\n",
    "2.  **Approach 2:** Using a specialized, dedicated vector database (e.g., ChromaDB, FAISS, Weaviate).\n",
    "\n",
    "Please provide a summary of the pros and cons for each approach. Consider factors like operational complexity, cost, query flexibility, and scalability for a small-to-medium sized enterprise application.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "db_research_output = get_completion(db_research_prompt, client, model_name, api_provider)\n",
    "print(db_research_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Explanation:**\n",
    "This prompt demonstrates a powerful synthesis task. We provide the LLM with two key inputs: unstructured information (the research) and a desired structure (the template). The agent's job is to merge them, creating a polished, formal document. This is a repeatable pattern for turning raw analysis into professional documentation. By assigning the persona of a Staff Engineer, we guide the LLM to adopt a formal and authoritative tone suitable for an official project artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "# Title: Adopt PostgreSQL with pgvector for semantic search and vector storage\n",
      "\n",
      "Status: Accepted\n",
      "\n",
      "## Context\n",
      "- Problem:\n",
      "  - The onboarding tool needs semantic search over documents/chunks while enforcing relational filters (e.g., department, role, region) and access controls.\n",
      "  - We must keep operational overhead low and maintain transactional consistency between source content, metadata, and embeddings.\n",
      "\n",
      "- Driving forces:\n",
      "  - Minimize operational complexity and cost (ideally a single datastore to run, back up, secure, and monitor).\n",
      "  - Strong query flexibility: combine vector similarity with SQL joins, filters, aggregations, and transactions.\n",
      "  - Governance and data integrity: ACID guarantees, migrations, backups (including PITR), auditing, and row-level security for multi-tenant or role-based access.\n",
      "  - Hybrid search capability (dense vectors + full-text/trigram) within one system.\n",
      "  - Adequate performance for a small-to-medium enterprise workload (up to low millions of vectors, modest QPS).\n",
      "  - Good local/dev parity and straightforward CI/CD.\n",
      "  - Availability of managed offerings (AWS RDS, GCP Cloud SQL, Azure) that support pgvector.\n",
      "\n",
      "- Constraints:\n",
      "  - Team capacity and budget favor fewer moving parts and predictable costs.\n",
      "  - Avoid introducing additional datastores unless clear scalability/performance needs justify it.\n",
      "  - The application requires strict transactional consistency between documents and embeddings.\n",
      "  - Security expectations include robust, database-enforced policies (e.g., RLS).\n",
      "  - Anticipated workload does not require GPU acceleration or highly specialized vector features at launch.\n",
      "  - Scaling beyond several million vectors or high-QPS, low-latency SLAs may require re-evaluation.\n",
      "\n",
      "## Decision\n",
      "- Use PostgreSQL with the pgvector extension to store embeddings and perform approximate nearest neighbor (ANN) search. Co-locate documents, metadata, and embeddings in the same database schema, and combine vector similarity with relational filters and (optional) full-text search in SQL.\n",
      "\n",
      "- Rationale:\n",
      "  - Compared to specialized vector databases (e.g., Weaviate) or libraries (e.g., FAISS, dev-first stores like Chroma), Postgres + pgvector offers:\n",
      "    - Lower operational overhead and cost (single system, mature tooling, simpler backups and security).\n",
      "    - Stronger query flexibility and transactional semantics (joins, filters, aggregations, RLS).\n",
      "    - Straightforward hybrid search by combining pgvector with Postgres full-text search or trigram indexes.\n",
      "    - Adequate performance at our expected scale; HNSW and IVFFlat indexes provide reasonable latency/recall for low millions of vectors and modest QPS.\n",
      "  - Trade-offs (accepted):\n",
      "    - Specialized vector systems can outperform at 10M+ vectors, very high QPS, or strict low-latency SLAs, and often include GPU options and richer vector-native features.\n",
      "    - Postgres requires careful index selection/tuning and has higher memory footprints for HNSW; IVFFlat needs training and balancing recall vs. latency.\n",
      "\n",
      "## Consequences\n",
      "- Positive outcomes:\n",
      "  - Operational simplicity: one datastore to provision, secure, monitor, back up, and restore; improved local/dev parity.\n",
      "  - Cost efficiency: leverage existing Postgres capacity or managed Postgres; avoid an additional service and its bills.\n",
      "  - Query power: use SQL to combine semantic search with complex relational filters, permissions (RLS), and aggregations in a single round trip.\n",
      "  - Governance and integrity: ACID transactions keep documents, metadata, and embeddings consistent; mature auditing and compliance posture.\n",
      "  - Hybrid search: straightforward combination of embeddings with Postgres full-text/trigram search in the same query.\n",
      "  - Managed ecosystem support: pgvector is supported on major cloud Postgres offerings.\n",
      "\n",
      "- Negative trade-offs:\n",
      "  - Scaling ceilings and tuning complexity:\n",
      "    - HNSW indexes are memory-intensive; index build/maintenance can be slow on very large corpora.\n",
      "    - IVFFlat requires training and careful parameter tuning; recall/latency trade-offs can be non-trivial.\n",
      "    - Horizontal sharding for embeddings is DIY; Postgres is not optimized for very large, high-QPS vector workloads.\n",
      "  - Fewer vector-native features vs. specialized systems (e.g., no GPU acceleration or product quantization out of the box; limited built-in re-ranking and multi-vector weighting).\n",
      "  - Operational nuances:\n",
      "    - Large index rebuilds during restore/reindex can be time-consuming.\n",
      "    - Requires disciplined vacuum/analyze and query planning to maintain predictable performance.\n",
      "\n",
      "- Future work:\n",
      "  - Index strategy and tuning:\n",
      "    - Start with HNSW for simplicity/recall; evaluate IVFFlat for larger corpora or tighter latency budgets.\n",
      "    - Establish embedding dimensions, distance metric, and index parameters; define reindexing and training procedures.\n",
      "  - Performance and capacity:\n",
      "    - Baseline and monitor recall, p50/p95 latency, QPS, memory usage, and index build times; set SLOs.\n",
      "    - Define clear revisit thresholds for a dedicated vector database (e.g., corpus size, QPS, latency, recall targets).\n",
      "  - Data model and governance:\n",
      "    - Finalize schema to store documents, metadata, and vectors together; implement RLS and auditing as required.\n",
      "    - Implement transactional pipelines for embedding generation/updates and background index maintenance.\n",
      "  - Operations:\n",
      "    - Confirm managed Postgres support for pgvector in all environments; document backup/restore and PITR implications with large vector indexes.\n",
      "    - Add observability for index health, bloat, vacuum/analyze cadence, and query plans.\n",
      "  - Review:\n",
      "    - Schedule a scaling and architecture review after initial production adoption and again upon approaching defined thresholds.\n",
      "✅ Successfully saved artifact to: artifacts/adr_001_database_choice.md\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"templates/adr_template.md\")\n",
    "\n",
    "synthesis_prompt = f\"\"\"You are a Staff Engineer responsible for documenting key architectural decisions.\n",
    "\n",
    "Your task is to populate the provided ADR template to formally document the decision to **use PostgreSQL with the pgvector extension** for our project.\n",
    "\n",
    "Use the research provided below to fill in the 'Context' and 'Consequences' sections of the template. Be thorough and objective, summarizing the key points from the research.\n",
    "\n",
    "--- ADR TEMPLATE ---\n",
    "{adr_template}\n",
    "--- END TEMPLATE ---\n",
    "\n",
    "--- RESEARCH CONTEXT ---\n",
    "{db_research_output}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "The final ADR should be complete and ready for review.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    final_adr = get_completion(synthesis_prompt, client, model_name, api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"artifacts/adr_001_database_choice.md\")\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
