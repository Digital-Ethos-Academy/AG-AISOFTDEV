{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs (Solution)\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for the ADR generation lab. It demonstrates how to use an LLM for comparative research and then synthesize that research into a structured, formal document.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'huggingface' with model 'deepseek-ai/DeepSeek-V3.1'\n",
      "✅ LLM Client configured: Using 'anthropic' with model 'claude-opus-4-1-20250805'\n",
      "✅ LLM Client configured: Using 'openai' with model 'gpt-5-2025-08-07'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different tasks using models from different providers.\n",
    "# - Template generation: use a HuggingFace instruction-following model\n",
    "# - Research/comparison: use an Anthropic model\n",
    "# - Synthesis (final ADR): use another HuggingFace model to demonstrate multi-provider usage\n",
    "template_client, template_model_name, template_api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "research_client, research_model_name, research_api_provider = setup_llm_client(model_name=\"claude-opus-4-1-20250805\")\n",
    "synthesis_client, synthesis_model_name, synthesis_api_provider = setup_llm_client(model_name=\"gpt-5-2025-08-07\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks the LLM to generate a standard markdown template for an ADR. The key is to be specific about the sections required (`Title`, `Status`, `Context`, `Decision`, `Consequences`), which guides the LLM to produce a well-structured and reusable template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "# Title: [Short, Descriptive Title for the Decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
      "\n",
      "## Context\n",
      "[Describe the issue or opportunity that prompted this decision. Explain the forces at play, such as technical, product, or project constraints. This should be a succinct summary of the problem being solved.]\n",
      "\n",
      "## Decision\n",
      "[Clearly state the architectural direction that has been chosen. Use active voice and be specific. For example: \"We will use...\" or \"The system shall...\"]\n",
      "\n",
      "## Consequences\n",
      "[List the outcomes, both positive and negative, of this decision. This section should cover the impact on the codebase, team processes, performance, security, and future maintainability. Mention any required follow-up tasks or future work.]\n",
      "\n",
      "*   **Positive:**\n",
      "    *   [Benefit 1]\n",
      "    *   [Benefit 2]\n",
      "\n",
      "*   **Negative:**\n",
      "    *   [Trade-off 1]\n",
      "    *   [Trade-off 2]\n",
      "\n",
      "*   **Related Work:**\n",
      "    *   [Task or issue to be created or tracked]\n",
      "✅ Successfully saved artifact to: artifacts/templates/adr_template.md\n"
     ]
    }
   ],
   "source": [
    "adr_template_prompt = \"\"\"You are a principal engineer who champions clear documentation. Generate a concise, reusable markdown template for an Architectural Decision Record (ADR).\n",
    "\n",
    "The template must include the following sections:\n",
    "- # Title: [A short, descriptive title for the decision]\n",
    "- **Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
    "- ## Context\n",
    "  - [Describe the problem, the driving forces, and the constraints.]\n",
    "- ## Decision\n",
    "  - [State the chosen solution clearly and concisely.]\n",
    "- ## Consequences\n",
    "  - [List the positive outcomes, negative trade-offs, and any future work required.]\"\"\"\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "enhanced_adr_template_prompt = prompt_enhancer(adr_template_prompt)\n",
    "adr_template_content = get_completion(enhanced_adr_template_prompt, template_client, template_model_name, template_api_provider)\n",
    "print(adr_template_content)\n",
    "\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"templates/adr_template.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Explanation:**\n",
    "This prompt leverages the LLM's vast training data to perform a comparative analysis. By instructing it to be an \"unbiased research assistant\" and asking for \"pros and cons for each,\" we guide the model to provide a balanced view rather than a simple recommendation. This produces a more valuable and objective input for our own decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "## Approach 1 — PostgreSQL + pgvector\n",
      "\n",
      "**Pros**\n",
      "- **Unified data layer** reduces operational complexity by maintaining embeddings alongside relational data, eliminating synchronization overhead and consistency challenges\n",
      "- **Lower infrastructure costs** through consolidation—no additional database licensing, hosting, or monitoring stack required\n",
      "- **Superior hybrid query flexibility** via native SQL joins between vector similarity results and structured metadata/filters without cross-system orchestration\n",
      "- **Mature operational tooling** leverages existing PostgreSQL expertise for backups, replication, monitoring, and disaster recovery\n",
      "- **ACID compliance** ensures transactional consistency when updating documents and their corresponding embeddings atomically\n",
      "\n",
      "**Cons**\n",
      "- **Limited vector-specific optimizations** compared to purpose-built solutions (e.g., no GPU acceleration, fewer index types like IVF or LSH)\n",
      "- **Performance ceiling** at scale—HNSW index rebuilds become expensive beyond ~1M high-dimensional vectors on standard hardware\n",
      "- **Restricted embedding dimensionality** (pgvector caps at 2000 dimensions) may constrain future model choices\n",
      "- **Manual index tuning burden** requires deep understanding of HNSW parameters (m, ef_construction) for optimal recall/speed tradeoffs\n",
      "- **Lack of built-in ML features** like automatic re-ranking, query expansion, or embedding generation pipelines\n",
      "\n",
      "## Approach 2 — Dedicated Vector Database\n",
      "\n",
      "**Pros**\n",
      "- **Superior vector search performance** through specialized indexing algorithms (HNSW, IVF-PQ, ScaNN) optimized for approximate nearest neighbor operations\n",
      "- **Built-in ML integrations** often include embedding generation APIs, cross-encoder re-ranking, and semantic caching out-of-the-box\n",
      "- **Better scalability headroom** with horizontal sharding and distributed search designed for billions of vectors\n",
      "- **Advanced retrieval features** like multi-vector search, hybrid sparse-dense retrieval, and learned query routing\n",
      "- **Reduced PostgreSQL load** keeps transactional workload isolated from compute-intensive similarity searches\n",
      "\n",
      "**Cons**\n",
      "- **Increased operational complexity** from managing two data stores with different backup, monitoring, and security requirements\n",
      "- **Data synchronization overhead** requires CDC pipelines or dual-writes to maintain consistency between systems\n",
      "- **Higher total infrastructure costs** from additional compute, storage, and potential vendor licensing fees\n",
      "- **Limited query flexibility** for complex filters—most vector DBs lack SQL's expressiveness for multi-table joins and aggregations\n",
      "- **Vendor lock-in risk** with proprietary query languages and index formats that complicate future migrations\n",
      "- **Team expertise gap** requires learning new operational patterns, troubleshooting techniques, and capacity planning models\n",
      "\n",
      "## When to Choose Which\n",
      "\n",
      "**Choose PostgreSQL + pgvector** when your document corpus remains under 1M items, queries require complex SQL joins with business data, and your team prioritizes operational simplicity over bleeding-edge vector search performance. Ideal for SMEs with strong PostgreSQL expertise and limited DevOps bandwidth.\n",
      "\n",
      "**Choose a dedicated vector database** when approaching 5M+ documents, requiring sub-100ms P99 latency at scale, or needing advanced ML features like hybrid retrieval. Best when you have dedicated search infrastructure expertise and can justify the operational overhead through measurable user experience improvements.\n"
     ]
    }
   ],
   "source": [
    "db_research_prompt = \"\"\"You are an unbiased research assistant. Your task is to provide a balanced technical comparison for a software development team.\n",
    "\n",
    "For the use case of a new hire onboarding tool that needs a semantic search feature, compare and contrast the following two approaches:\n",
    "\n",
    "1.  **Approach 1:** Using PostgreSQL with the `pgvector` extension.\n",
    "2.  **Approach 2:** Using a specialized, dedicated vector database (e.g., ChromaDB, FAISS, Weaviate).\n",
    "\n",
    "Please provide a summary of the pros and cons for each approach. Consider factors like operational complexity, cost, query flexibility, and scalability for a small-to-medium sized enterprise application.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "enhanced_db_research_prompt = prompt_enhancer(db_research_prompt)\n",
    "db_research_output = get_completion(enhanced_db_research_prompt, research_client, research_model_name, research_api_provider)\n",
    "print(db_research_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Explanation:**\n",
    "This prompt demonstrates a powerful synthesis task. We provide the LLM with two key inputs: unstructured information (the research) and a desired structure (the template). The agent's job is to merge them, creating a polished, formal document. This is a repeatable pattern for turning raw analysis into professional documentation. By assigning the persona of a Staff Engineer, we guide the LLM to adopt a formal and authoritative tone suitable for an official project artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "# Title: Adopt PostgreSQL with pgvector for Vector Storage and Search\n",
      "\n",
      "**Status:** Accepted\n",
      "\n",
      "## Context\n",
      "We need to persist and search vector embeddings alongside relational data while minimizing operational overhead and infrastructure cost. Consolidating on PostgreSQL allows us to use a unified data layer with mature tooling and ACID guarantees, and to express hybrid queries (vector similarity combined with metadata filters) in SQL. Our near-term corpus size is expected to remain under one million items.\n",
      "\n",
      "This choice trades peak vector search performance for simplicity: pgvector lacks GPU acceleration and some vector-specific optimizations; HNSW index builds and rebuilds become costly beyond roughly one million vectors on standard hardware; and pgvector currently caps embedding dimensions at 2000. Parameter tuning and operational maintenance will be required. Given our constraints and priorities, the guideline favors Postgres + pgvector when SQL joins and operational simplicity outweigh maximal performance.\n",
      "\n",
      "## Decision\n",
      "We will use PostgreSQL with the pgvector extension as the system of record for embeddings and similarity search—employing HNSW indexes and SQL-based hybrid queries—optimized for corpora under one million items.\n",
      "\n",
      "## Consequences\n",
      "* **Positive:**\n",
      "    * Unified data layer reduces operational complexity and eliminates cross-system synchronization.\n",
      "    * Lower infrastructure cost by consolidating on PostgreSQL instead of operating a separate vector database.\n",
      "    * Ability to write hybrid queries that combine vector similarity with relational filters via SQL joins.\n",
      "    * Leverages mature PostgreSQL ecosystem for backup, replication, monitoring, and observability.\n",
      "    * ACID guarantees enable atomic updates of primary data and corresponding embeddings.\n",
      "* **Negative:**\n",
      "    * Fewer vector-specific optimizations and no GPU acceleration may limit peak recall/latency at scale.\n",
      "    * HNSW index builds/rebuilds become costly beyond ~1M vectors on standard hardware, impacting maintenance windows.\n",
      "    * 2000-dimensional cap may constrain future embedding model choices or require dimensionality reduction.\n",
      "    * Ongoing manual tuning required for HNSW parameters (e.g., m, ef_construction, ef_search) and index maintenance.\n",
      "    * No built-in ML features (e.g., re-ranking, embedding pipelines); these must be implemented at the application or service layer.\n",
      "* **Related Work:**\n",
      "    * Select embedding model and schema that respect the 2000-dimension limit; document vector distance metric (e.g., cosine).\n",
      "    * Define HNSW indexing strategy and defaults; implement monitoring and automation for index build/rebuild and bloat control.\n",
      "    * Benchmark recall/latency targets and tune parameters; document thresholds and SLOs.\n",
      "    * Plan for growth beyond ~1M vectors (partitioning/sharding) and evaluate dedicated vector databases if scale or performance demands change.\n",
      "    * Implement an application-level re-ranking pipeline and embedding generation workflow as separate services.\n",
      "    * Create operational runbooks covering backup/restore, replication, autovacuum, and capacity planning.\n",
      "✅ Successfully saved artifact to: artifacts/adr_001_database_choice.md\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"templates/adr_template.md\")\n",
    "\n",
    "synthesis_prompt = f\"\"\"You are a Staff Engineer responsible for documenting key architectural decisions.\n",
    "\n",
    "Your task is to populate the provided ADR template to formally document the decision to **use PostgreSQL with the pgvector extension** for our project.\n",
    "\n",
    "Use the research provided below to fill in the 'Context' and 'Consequences' sections of the template. Be thorough and objective, summarizing the key points from the research.\n",
    "\n",
    "--- ADR TEMPLATE ---\n",
    "{adr_template}\n",
    "--- END TEMPLATE ---\n",
    "\n",
    "--- RESEARCH CONTEXT ---\n",
    "{db_research_output}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "The final ADR should be complete and ready for review.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    enhanced_synthesis_prompt = prompt_enhancer(synthesis_prompt)\n",
    "    final_adr = get_completion(enhanced_synthesis_prompt, synthesis_client, synthesis_model_name, synthesis_api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"artifacts/adr_001_database_choice.md\")\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
