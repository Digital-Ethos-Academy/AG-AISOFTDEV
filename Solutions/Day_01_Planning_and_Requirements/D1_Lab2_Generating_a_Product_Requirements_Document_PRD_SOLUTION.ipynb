{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Lab 2: Generating a Product Requirements Document (PRD) (Solution)\n",
    "\n",
    "**Objective:** Use the structured `day1_user_stories.json` artifact from the previous lab to generate a formal, comprehensive Product Requirements Document (PRD) in markdown format.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook demonstrates how to synthesize detailed, low-level requirements (user stories) into a high-level planning document (the PRD). It also introduces the advanced concept of using code (Pydantic models) to define and validate the structure of documentation.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We begin by setting up our environment and loading the key artifact from Lab 1: `day1_user_stories.json`. The `load_artifact` helper function reads the file content, and `json.loads` parses the JSON string into a Python list of dictionaries, making it ready for use in our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-19 21:51:45,352 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    # Assumes the notebook is in 'labs/Day_01_.../'\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    # Fallback for different execution environments\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize the LLM client. You can change the model here.\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the artifact from Lab 1\n",
    "user_stories_data = load_artifact(\"day1_user_stories.json\")\n",
    "if user_stories_data:\n",
    "    # Convert to JSON string if it's already a Python object\n",
    "    if isinstance(user_stories_data, (list, dict)):\n",
    "        user_stories_str = json.dumps(user_stories_data, indent=2)\n",
    "    else:\n",
    "        user_stories_str = user_stories_data\n",
    "else:\n",
    "    print(\"Warning: Could not load user stories. Lab may not function correctly.\")\n",
    "    user_stories_data = []\n",
    "    user_stories_str = \"[]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a Simple PRD\n",
    "\n",
    "**Explanation:**\n",
    "This prompt serves as a baseline. We provide the LLM with the user stories and ask it to summarize them into a few key sections. This is a simple synthesis task that demonstrates the LLM's ability to extract and reorganize information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Simple PRD ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 21:51:45,797 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Simple PRD prompt:\n",
      " <prompt>\n",
      "\n",
      "  <persona>\n",
      "  You are a Senior Product Manager and Technical Writer with deep expertise in creating concise, executive-ready Product Requirements Documents (PRDs) for SaaS onboarding tools.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "  Below is the complete set of user stories that define the needs of our target personas for a new-hire onboarding platform.\n",
      "\n",
      "  <user_stories>\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"persona\": \"The Empowered New Hire\",\n",
      "      \"user_story\": \"As the Empowered New Hire, I want a personalized 30-60-90 day task and learning plan, so that I can clearly understand my responsibilities and expectations at each stage of my onboarding.\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"Given I have logged into the onboarding portal, When I view my dashboard, Then I can see a personalized 30-60-90 day plan tailored to my role.\",\n",
      "        \"Given I complete a task in my plan, When I mark it as done, Then the system updates my progress and adjusts future tasks if necessary.\",\n",
      "        \"Given I am in the 60-day phase, When I view my plan, Then I can see tasks and learning objectives specific to this period.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"persona\": \"The Performance Enabler\",\n",
      "      \"user_story\": \"As the Performance Enabler, I want a manager dashboard for tracking new-hire progress and engagement, so that I can provide timely support and ensure successful integration.\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"Given a new hire is onboarded, When I access the manager dashboard, Then I can view their progress and engagement metrics in real time.\",\n",
      "        \"Given a new hire completes a milestone, When I check the dashboard, Then I receive a notification with their achievement details.\",\n",
      "        \"Given I need to support a new hire, When I view their engagement data, Then I can identify areas where they might need additional help.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"persona\": \"The Onboarding Orchestrator\",\n",
      "      \"user_story\": \"As the Onboarding Orchestrator, I want an automated onboarding buddy and mentor matching system, so that new hires can quickly connect with experienced colleagues for guidance.\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"Given a new hire is added to the system, When they complete their profile, Then the system automatically matches them with a buddy and mentor based on role and interests.\",\n",
      "        \"Given a match is made, When the new hire logs in, Then they can see their buddy and mentor's contact information and schedule an introduction meeting.\",\n",
      "        \"Given a mentor is assigned, When they accept the match, Then the system sends a confirmation to both the mentor and the new hire.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"persona\": \"The Empowered New Hire\",\n",
      "      \"user_story\": \"As the Empowered New Hire, I want access to a centralized knowledge base with searchable company resources, so that I can quickly find the information I need to succeed in my role.\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"Given I have a question about company policies, When I search the knowledge base, Then I can find relevant articles and resources.\",\n",
      "        \"Given I need information on a specific tool, When I enter the tool's name in the search bar, Then the system displays detailed guides and FAQs.\",\n",
      "        \"Given new resources are added, When I access the knowledge base, Then I can see the latest updates and additions highlighted.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"persona\": \"The Performance Enabler\",\n",
      "      \"user_story\": \"As the Performance Enabler, I want scheduled check-in prompts for managers and new hires, so that I can ensure regular communication and address any concerns promptly.\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"Given a new hire is onboarded, When a check-in is due, Then both the manager and new hire receive a prompt to schedule a meeting.\",\n",
      "        \"Given a check-in is completed, When I log the meeting notes, Then the system updates the new hire's progress record.\",\n",
      "        \"Given a check-in is missed, When the system detects this, Then it sends a reminder to both the manager and the new hire.\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "  </user_stories>\n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "  1. Think step by step:\n",
      "     a. Extract the overarching goals and pain points from the user stories.  \n",
      "     b. Identify and summarize each unique user persona.  \n",
      "     c. Map each user story to a distinct feature.\n",
      "\n",
      "  2. Generate a Product Requirements Document (PRD) in Markdown that includes ONLY the following top-level sections:\n",
      "     • Introduction  \n",
      "     • User Personas  \n",
      "     • Features / User Stories\n",
      "\n",
      "  3. Formatting rules:\n",
      "     • Use H1 (#) for the document title “Product Requirements Document”.  \n",
      "     • Use H2 (##) for each top-level section.  \n",
      "     • Under “Features / User Stories”, list each user story as a bullet (–) and nest its acceptance criteria as sub-bullets (•).  \n",
      "     • Keep the total length under 800 words.  \n",
      "     • Maintain clear, professional language suitable for executives, designers, and engineers.  \n",
      "     • Do NOT invent new features; rely solely on the provided user stories.\n",
      "\n",
      "  4. After reasoning, output ONLY the final PRD with no additional commentary.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "  Markdown\n",
      "  </output_format>\n",
      "\n",
      "</prompt>\n",
      "# Product Requirements Document\n",
      "\n",
      "## Introduction\n",
      "\n",
      "This document outlines the requirements for a new-hire onboarding platform designed to streamline and enhance the employee integration experience. The platform aims to provide new hires with clear, structured guidance, empower managers with real-time progress insights, and automate key administrative tasks for the onboarding team. By addressing the core needs of new employees, managers, and program coordinators, this tool will accelerate time-to-productivity, improve engagement, and ensure a consistent, high-quality onboarding process for every new team member.\n",
      "\n",
      "## User Personas\n",
      "\n",
      "*   **The Empowered New Hire:** A new employee who is motivated to succeed but requires clear direction and easy access to information. Their goal is to understand expectations, build connections, and contribute effectively as quickly as possible.\n",
      "*   **The Performance Enabler:** A hiring manager responsible for a new hire's success. They need visibility into their new report's progress and engagement to provide timely support, remove blockers, and facilitate a smooth integration into the team.\n",
      "*   **The Onboarding Orchestrator:** An HR or People Operations professional who manages the overall onboarding program. They seek to create a scalable, consistent, and engaging experience by automating manual tasks and fostering connections across the organization.\n",
      "\n",
      "## Features / User Stories\n",
      "\n",
      "– **As the Empowered New Hire, I want a personalized 30-60-90 day task and learning plan, so that I can clearly understand my responsibilities and expectations at each stage of my onboarding.**\n",
      "  • Given I have logged into the onboarding portal, When I view my dashboard, Then I can see a personalized 30-60-90 day plan tailored to my role.\n",
      "  • Given I complete a task in my plan, When I mark it as done, Then the system updates my progress and adjusts future tasks if necessary.\n",
      "  • Given I am in the 60-day phase, When I view my plan, Then I can see tasks and learning objectives specific to this period.\n",
      "\n",
      "– **As the Performance Enabler, I want a manager dashboard for tracking new-hire progress and engagement, so that I can provide timely support and ensure successful integration.**\n",
      "  • Given a new hire is onboarded, When I access the manager dashboard, Then I can view their progress and engagement metrics in real time.\n",
      "  • Given a new hire completes a milestone, When I check the dashboard, Then I receive a notification with their achievement details.\n",
      "  • Given I need to support a new hire, When I view their engagement data, Then I can identify areas where they might need additional help.\n",
      "\n",
      "– **As the Onboarding Orchestrator, I want an automated onboarding buddy and mentor matching system, so that new hires can quickly connect with experienced colleagues for guidance.**\n",
      "  • Given a new hire is added to the system, When they complete their profile, Then the system automatically matches them with a buddy and mentor based on role and interests.\n",
      "  • Given a match is made, When the new hire logs in, Then they can see their buddy and mentor's contact information and schedule an introduction meeting.\n",
      "  • Given a mentor is assigned, When they accept the match, Then the system sends a confirmation to both the mentor and the new hire.\n",
      "\n",
      "– **As the Empowered New Hire, I want access to a centralized knowledge base with searchable company resources, so that I can quickly find the information I need to succeed in my role.**\n",
      "  • Given I have a question about company policies, When I search the knowledge base, Then I can find relevant articles and resources.\n",
      "  • Given I need information on a specific tool, When I enter the tool's name in the search bar, Then the system displays detailed guides and FAQs.\n",
      "  • Given new resources are added, When I access the knowledge base, Then I can see the latest updates and additions highlighted.\n",
      "\n",
      "– **As the Performance Enabler, I want scheduled check-in prompts for managers and new hires, so that I can ensure regular communication and address any concerns promptly.**\n",
      "  • Given a new hire is onboarded, When a check-in is due, Then both the manager and new hire receive a prompt to schedule a meeting.\n",
      "  • Given a check-in is completed, When I log the meeting notes, Then the system updates the new hire's progress record.\n",
      "  • Given a check-in is missed, When the system detects this, Then it sends a reminder to both the manager and the new hire.\n",
      "# Product Requirements Document\n",
      "\n",
      "## Introduction\n",
      "\n",
      "This document outlines the requirements for a new-hire onboarding platform designed to streamline and enhance the employee integration experience. The platform aims to provide new hires with clear, structured guidance, empower managers with real-time progress insights, and automate key administrative tasks for the onboarding team. By addressing the core needs of new employees, managers, and program coordinators, this tool will accelerate time-to-productivity, improve engagement, and ensure a consistent, high-quality onboarding process for every new team member.\n",
      "\n",
      "## User Personas\n",
      "\n",
      "*   **The Empowered New Hire:** A new employee who is motivated to succeed but requires clear direction and easy access to information. Their goal is to understand expectations, build connections, and contribute effectively as quickly as possible.\n",
      "*   **The Performance Enabler:** A hiring manager responsible for a new hire's success. They need visibility into their new report's progress and engagement to provide timely support, remove blockers, and facilitate a smooth integration into the team.\n",
      "*   **The Onboarding Orchestrator:** An HR or People Operations professional who manages the overall onboarding program. They seek to create a scalable, consistent, and engaging experience by automating manual tasks and fostering connections across the organization.\n",
      "\n",
      "## Features / User Stories\n",
      "\n",
      "– **As the Empowered New Hire, I want a personalized 30-60-90 day task and learning plan, so that I can clearly understand my responsibilities and expectations at each stage of my onboarding.**\n",
      "  • Given I have logged into the onboarding portal, When I view my dashboard, Then I can see a personalized 30-60-90 day plan tailored to my role.\n",
      "  • Given I complete a task in my plan, When I mark it as done, Then the system updates my progress and adjusts future tasks if necessary.\n",
      "  • Given I am in the 60-day phase, When I view my plan, Then I can see tasks and learning objectives specific to this period.\n",
      "\n",
      "– **As the Performance Enabler, I want a manager dashboard for tracking new-hire progress and engagement, so that I can provide timely support and ensure successful integration.**\n",
      "  • Given a new hire is onboarded, When I access the manager dashboard, Then I can view their progress and engagement metrics in real time.\n",
      "  • Given a new hire completes a milestone, When I check the dashboard, Then I receive a notification with their achievement details.\n",
      "  • Given I need to support a new hire, When I view their engagement data, Then I can identify areas where they might need additional help.\n",
      "\n",
      "– **As the Onboarding Orchestrator, I want an automated onboarding buddy and mentor matching system, so that new hires can quickly connect with experienced colleagues for guidance.**\n",
      "  • Given a new hire is added to the system, When they complete their profile, Then the system automatically matches them with a buddy and mentor based on role and interests.\n",
      "  • Given a match is made, When the new hire logs in, Then they can see their buddy and mentor's contact information and schedule an introduction meeting.\n",
      "  • Given a mentor is assigned, When they accept the match, Then the system sends a confirmation to both the mentor and the new hire.\n",
      "\n",
      "– **As the Empowered New Hire, I want access to a centralized knowledge base with searchable company resources, so that I can quickly find the information I need to succeed in my role.**\n",
      "  • Given I have a question about company policies, When I search the knowledge base, Then I can find relevant articles and resources.\n",
      "  • Given I need information on a specific tool, When I enter the tool's name in the search bar, Then the system displays detailed guides and FAQs.\n",
      "  • Given new resources are added, When I access the knowledge base, Then I can see the latest updates and additions highlighted.\n",
      "\n",
      "– **As the Performance Enabler, I want scheduled check-in prompts for managers and new hires, so that I can ensure regular communication and address any concerns promptly.**\n",
      "  • Given a new hire is onboarded, When a check-in is due, Then both the manager and new hire receive a prompt to schedule a meeting.\n",
      "  • Given a check-in is completed, When I log the meeting notes, Then the system updates the new hire's progress record.\n",
      "  • Given a check-in is missed, When the system detects this, Then it sends a reminder to both the manager and the new hire.\n"
     ]
    }
   ],
   "source": [
    "simple_prd_prompt = f\"\"\"\n",
    "You are a Product Manager writing a Product Requirements Document (PRD) for a new hire onboarding tool.\n",
    "\n",
    "Use the following JSON data containing user stories as your primary source of information:\n",
    "<user_stories>\n",
    "{user_stories_str}\n",
    "</user_stories>\n",
    "\n",
    "Generate a PRD in markdown format with the following sections:\n",
    "1. **Introduction:** A brief overview of the project's purpose.\n",
    "2. **User Personas:** A summary of the key users involved.\n",
    "3. **Features / User Stories:** A list of the user stories and their acceptance criteria.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Simple PRD ---\")\n",
    "if user_stories_data:\n",
    "    # Enhance the prompt to improve structure and consistency\n",
    "    enhanced_simple_prd_prompt = prompt_enhancer(simple_prd_prompt)\n",
    "    print(\"Enhanced Simple PRD prompt:\\n\", enhanced_simple_prd_prompt)\n",
    "    simple_prd_output = get_completion(enhanced_simple_prd_prompt, client, model_name, api_provider)\n",
    "    print(simple_prd_output)\n",
    "else:\n",
    "    print(\"Skipping PRD generation because user stories are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a PRD from a Template\n",
    "\n",
    "**Explanation:**\n",
    "This is a more advanced and practical task. Providing a template gives us much greater control over the final output. The LLM's task shifts from creative writing to structured content generation. We instruct it to fill in every section, which forces it to infer logical content for sections like \"Success Metrics\" and \"Out of Scope\" based on the provided requirements. This is a powerful pattern for creating consistent documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArtifactNotFoundError",
     "evalue": "Artifact not found: /Users/agaleana/repos/AG-AISOFTDEV/artifacts/templates/prd_template.md",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArtifactNotFoundError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the PRD template from the 'templates' directory.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m prd_template_content = \u001b[43mload_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemplates/prd_template.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# If load_artifact failed to return content, try reading the file directly from disk using project_root.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prd_template_content:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/AG-AISOFTDEV/utils/artifacts.py:202\u001b[39m, in \u001b[36mload_artifact\u001b[39m\u001b[34m(filename, base_dir, subdir, as_, encoding)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_artifact\u001b[39m(\n\u001b[32m    183\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    184\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m     encoding: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    189\u001b[39m ) -> Union[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, Any]:\n\u001b[32m    190\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load content from the artifacts directory.\u001b[39;00m\n\u001b[32m    191\u001b[39m \n\u001b[32m    192\u001b[39m \u001b[33;03m    Raises\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m    'hello'\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     path = \u001b[43mresolve_artifact_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmust_exist\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m as_ == \u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    206\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m path.read_bytes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/AG-AISOFTDEV/utils/artifacts.py:113\u001b[39m, in \u001b[36mresolve_artifact_path\u001b[39m\u001b[34m(filename, base_dir, subdir, must_exist)\u001b[39m\n\u001b[32m    109\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ArtifactSecurityError(\n\u001b[32m    110\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResolved path \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m escapes artifacts dir \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m         )\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m must_exist \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m final.exists():\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ArtifactNotFoundError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArtifact not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m final\n",
      "\u001b[31mArtifactNotFoundError\u001b[39m: Artifact not found: /Users/agaleana/repos/AG-AISOFTDEV/artifacts/templates/prd_template.md"
     ]
    }
   ],
   "source": [
    "# Load the PRD template from the 'templates' directory.\n",
    "prd_template_content = load_artifact(\"templates/prd_template.md\")\n",
    "\n",
    "# If load_artifact failed to return content, try reading the file directly from disk using project_root.\n",
    "if not prd_template_content:\n",
    "    template_path = os.path.join(project_root, \"templates\", \"prd_template.md\")\n",
    "    if os.path.exists(template_path):\n",
    "        try:\n",
    "            with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                prd_template_content = f.read()\n",
    "            print(f\"✅ Loaded PRD template directly from {template_path} ({len(prd_template_content)} characters)\")\n",
    "        except Exception as e:\n",
    "            print(\"❌ ERROR: Failed to read template file directly:\", e)\n",
    "            print(\"   Current working directory:\", os.getcwd())\n",
    "            prd_template_content = \"\"\n",
    "    else:\n",
    "        print(\"❌ ERROR: Could not find PRD template at expected path:\", template_path)\n",
    "        print(\"   Please ensure the file exists and is readable.\")\n",
    "        print(\"   Current working directory:\", os.getcwd())\n",
    "        prd_template_content = \"\"\n",
    "else:\n",
    "    print(f\"✅ Successfully loaded PRD template ({len(prd_template_content)} characters)\")\n",
    "\n",
    "# Build the template-based prompt\n",
    "template_prd_prompt = f\"\"\"\n",
    "You are a Senior Product Manager responsible for creating a detailed and formal Product Requirements Document (PRD).\n",
    "\n",
    "Your task is to populate the provided PRD template using the information from the user stories JSON.\n",
    "\n",
    "<prd_template>\n",
    "{prd_template_content}\n",
    "</prd_template>\n",
    "\n",
    "<user_stories_json>\n",
    "{user_stories_str}\n",
    "</user_stories_json>\n",
    "\n",
    "Fill out every section of the template. For sections like 'Success Metrics' or 'Out of Scope', you must infer reasonable content based on the user stories and the overall project goal of creating a new hire onboarding tool.\n",
    "The final output should be the completed PRD in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating PRD from Template ---\")\n",
    "if user_stories_data and prd_template_content:\n",
    "    # Enhance the template-based prompt to improve fidelity and structure\n",
    "    enhanced_template_prd_prompt = prompt_enhancer(template_prd_prompt)\n",
    "    print(\"Enhanced Template PRD prompt:\\n\", enhanced_template_prd_prompt)\n",
    "    prd_from_template_output = get_completion(enhanced_template_prd_prompt, client, model_name, api_provider)\n",
    "    print(prd_from_template_output)\n",
    "else:\n",
    "    print(\"Skipping PRD generation because user stories or template are missing.\")\n",
    "    if not user_stories_data:\n",
    "        print(\"   - User stories data is missing\")\n",
    "    if not prd_template_content:\n",
    "        print(\"   - PRD template is missing\")\n",
    "    prd_from_template_output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check template file existence and path resolution\n",
    "print(\"=== TEMPLATE DIAGNOSTIC ===\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Check if templates directory exists\n",
    "templates_dir = os.path.join(project_root, \"templates\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Templates directory: {templates_dir}\")\n",
    "print(f\"Templates directory exists: {os.path.exists(templates_dir)}\")\n",
    "\n",
    "if os.path.exists(templates_dir):\n",
    "    print(\"Contents of templates directory:\")\n",
    "    for item in os.listdir(templates_dir):\n",
    "        full_path = os.path.join(templates_dir, item)\n",
    "        print(f\"  - {item} ({'file' if os.path.isfile(full_path) else 'directory'})\")\n",
    "\n",
    "# Check the specific template file\n",
    "template_file = os.path.join(project_root, \"templates\", \"prd_template.md\")\n",
    "print(f\"\\nPRD template file path: {template_file}\")\n",
    "print(f\"PRD template file exists: {os.path.exists(template_file)}\")\n",
    "\n",
    "if os.path.exists(template_file):\n",
    "    print(f\"File size: {os.path.getsize(template_file)} bytes\")\n",
    "else:\n",
    "    print(\"❌ Template file not found!\")\n",
    "    \n",
    "print(\"==========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Programmatic Validation with Pydantic\n",
    "\n",
    "**Explanation:**\n",
    "This is the most advanced challenge. We are now using an LLM to write *code that validates documents*. Generating a Pydantic model turns our document's structure into a testable, code-based standard. This is a form of 'documentation-as-code' that allows for automated governance, ensuring all future PRDs conform to the same reliable format.\n",
    "\n",
    "1.  **Prompting for Code:** We give the LLM the PRD template and ask it to generate a Pydantic model. Pydantic is a data validation library, and using it to define our document structure turns that structure into a testable, reusable standard.\n",
    "2.  **Saving the Model:** We save the generated Python code to a specific location (`app/validation_models/prd_model.py`). This isn't just a temporary script; it's a formal part of our application's codebase, intended to be used for future validation tasks.\n",
    "3.  **Saving the PRD:** Finally, we save the markdown PRD generated in the intermediate step. This becomes the official `day1_prd.md` artifact for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydantic_model_prompt = f\"\"\"\n",
    "You are a Python developer specializing in data validation with Pydantic.\n",
    "\n",
    "Based on the following markdown PRD template, generate a single Pydantic model class named `ProductRequirementsDocument` that represents its structure.\n",
    "\n",
    "<prd_template>\n",
    "{prd_template_content}\n",
    "</prd_template>\n",
    "\n",
    "The model should have fields that correspond to the main sections of the template. Use appropriate Python types (e.g., str, List, Dict) from the `typing` library.\n",
    "Ensure you include the necessary imports from `pydantic` and `typing`.\n",
    "Only output the raw Python code for the model, without any explanation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pydantic Model for PRD ---\")\n",
    "\n",
    "if prd_template_content:\n",
    "    # Enhance the prompt to encourage precise, import-ready Python code\n",
    "    enhanced_pydantic_model_prompt = prompt_enhancer(pydantic_model_prompt)\n",
    "    print(\"Enhanced Pydantic Model prompt:\\n\", enhanced_pydantic_model_prompt)\n",
    "    pydantic_model_code = get_completion(enhanced_pydantic_model_prompt, client, model_name, api_provider)\n",
    "    \n",
    "    if pydantic_model_code:\n",
    "        # Use our standardized cleaning function\n",
    "        cleaned_code = clean_llm_output(pydantic_model_code, language='python')\n",
    "        \n",
    "        print(\"\\n--- Generated Pydantic Model ---\")\n",
    "        print(cleaned_code)\n",
    "\n",
    "        # Save the generated Pydantic model code to a file.\n",
    "        model_path = \"app/validation_models/prd_model.py\"\n",
    "        save_artifact(cleaned_code, model_path)\n",
    "    else:\n",
    "        print(\"Warning: Pydantic model generation failed, get_completion returned None.\")\n",
    "else:\n",
    "    print(\"Skipping Pydantic model generation because template is missing.\")\n",
    "\n",
    "# Finally, save the completed PRD from the intermediate challenge as our official artifact\n",
    "if prd_from_template_output:\n",
    "    save_artifact(prd_from_template_output, \"day1_prd.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now taken the structured user stories from the first lab and synthesized them into a formal Product Requirements Document. You also created a Pydantic model to enforce the structure of this document, introducing automated governance into your workflow. The `day1_prd.md` artifact will be the primary input for Day 2, where we will begin designing our system's architecture and database.\n",
    "\n",
    "> **Key Takeaway:** Using an LLM to populate a pre-defined template is a powerful pattern for creating consistent, high-quality documentation at scale. It combines the LLM's language skills with your required structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
