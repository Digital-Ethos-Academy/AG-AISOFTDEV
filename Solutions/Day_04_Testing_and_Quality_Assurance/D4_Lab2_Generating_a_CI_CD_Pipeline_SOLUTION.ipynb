{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline (Solution)\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts for generating the `requirements.txt`, `Dockerfile`, and GitHub Actions workflow files. It demonstrates how to prompt for specific, structured configuration-as-code artifacts.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, prompt_enhancer, recommended_models_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize different LLM clients for specialized CI/CD tasks\n",
    "print(\"=== INITIALIZING SPECIALIZED LLM CLIENTS ===\")\n",
    "\n",
    "# Dependency analysis client - Fast model for simple parsing tasks\n",
    "deps_client, deps_model_name, deps_api_provider = setup_llm_client(model_name=\"gemini-2.5-flash\")\n",
    "print(f\"Dependencies analysis: {deps_model_name} ({deps_api_provider})\")\n",
    "\n",
    "# Docker configuration client - Balanced model for infrastructure as code\n",
    "docker_client, docker_model_name, docker_api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "print(f\"Docker configuration: {docker_model_name} ({docker_api_provider})\")\n",
    "\n",
    "# CI/CD workflow client - Advanced model for complex YAML generation\n",
    "cicd_client, cicd_model_name, cicd_api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "print(f\"CI/CD workflows: {cicd_model_name} ({cicd_api_provider})\")\n",
    "\n",
    "print(\"✅ All specialized clients initialized successfully!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")\n",
    "else:\n",
    "    print(f\"✅ Successfully loaded application code ({len(app_code)} characters)\")\n",
    "    print(\"First 200 characters preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(app_code[:200] + \"...\" if len(app_code) > 200 else app_code)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Explanation:**\n",
    "This challenge demonstrates how prompt enhancement can transform a simple dependency analysis task into a sophisticated, production-ready solution. We start with a basic prompt for analyzing Python imports, but enhance it using our `prompt_enhancer` function to apply advanced prompt engineering techniques.\n",
    "\n",
    "The `prompt_enhancer` automatically:\n",
    "- Assigns the appropriate expert persona (\"You are a Python Dependency Analysis Expert\")\n",
    "- Provides structured context and clear task definitions\n",
    "- Sets explicit output format expectations\n",
    "- Applies best practices for reliable, consistent results\n",
    "\n",
    "We use a fast model (Gemini Flash) for this straightforward parsing task, demonstrating how different models can be optimized for different complexity levels. This approach is much faster and less error-prone than manually creating dependency files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating requirements.txt ---\n",
      "plaintext\n",
      "fastapi==0.95.2\n",
      "uvicorn==0.22.0\n",
      "sqlalchemy==2.0.21\n",
      "pydantic==1.10.4\n",
      "pytest==7.4.2\n",
      "✅ Successfully saved artifact to: requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Challenge 1: Enhanced Requirements Generation\n",
    "print(\"=\" * 60)\n",
    "print(\"CHALLENGE 1: ENHANCED REQUIREMENTS GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if app_code:\n",
    "    # Step 1: Create and enhance the raw prompt\n",
    "    print(\"\\n--- STEP 1: ENHANCING REQUIREMENTS PROMPT ---\")\n",
    "    raw_requirements_prompt = f\"\"\"Analyze the following Python code and generate a requirements.txt file listing all the external libraries imported.\n",
    "\n",
    "Include versions for key libraries like fastapi, uvicorn, sqlalchemy, and pydantic. Also include pytest for running tests.\n",
    "\n",
    "--- PYTHON CODE ---\n",
    "{app_code}\n",
    "--- END CODE ---\n",
    "\n",
    "Output only the contents of the requirements.txt file.\"\"\"\n",
    "\n",
    "    enhanced_requirements_prompt = prompt_enhancer(\n",
    "        raw_requirements_prompt, \n",
    "        deps_model_name, \n",
    "        deps_client, \n",
    "        deps_api_provider\n",
    "    )\n",
    "    print(\"Enhanced requirements prompt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(enhanced_requirements_prompt[:300] + \"...\" if len(enhanced_requirements_prompt) > 300 else enhanced_requirements_prompt)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Step 2: Generate requirements using specialized client\n",
    "    print(\"\\n--- STEP 2: GENERATING REQUIREMENTS ---\")\n",
    "    print(f\"Using {deps_model_name} ({deps_api_provider}) for dependency analysis...\")\n",
    "    \n",
    "    requirements_content = get_completion(\n",
    "        enhanced_requirements_prompt, \n",
    "        deps_client, \n",
    "        deps_model_name, \n",
    "        deps_api_provider,\n",
    "        temperature=0.2  # Lower temperature for consistent parsing\n",
    "    )\n",
    "    \n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(\"Generated requirements.txt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(cleaned_reqs)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\")\n",
    "    print(\"✅ Requirements.txt saved successfully!\")\n",
    "else:\n",
    "    print(\"❌ Skipping requirements generation because app code is missing.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHALLENGE 1 COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Explanation:**\n",
    "This challenge showcases how prompt enhancement elevates infrastructure-as-code generation to professional standards. We transform a basic Dockerfile request into a comprehensive DevOps specification that incorporates security best practices, performance optimizations, and production-readiness guidelines.\n",
    "\n",
    "The enhancement process ensures our prompt:\n",
    "- Establishes expert DevOps persona with containerization specialization\n",
    "- Explicitly requests security features (non-root user, multi-stage builds)\n",
    "- Defines performance optimizations (slim base images, layer caching)\n",
    "- Provides structured output requirements with clear technical specifications\n",
    "\n",
    "We use GPT-4o for this intermediate task, as it requires balancing multiple concerns (security, performance, maintainability) while generating syntactically correct Dockerfile syntax. This represents the sweet spot between capability and cost-effectiveness for infrastructure generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Dockerfile ---\n",
      "Dockerfile\n",
      "# Use the official Python image as the base for the first stage\n",
      "FROM python:3.11-slim as builder\n",
      "\n",
      "# Set the working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the requirements file into the container\n",
      "COPY requirements.txt .\n",
      "\n",
      "# Install dependencies\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Use the official Python image as the base for the final stage\n",
      "FROM python:3.11-slim\n",
      "\n",
      "# Set the working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the application code into the container\n",
      "COPY . .\n",
      "\n",
      "# Copy the installed dependencies from the builder stage\n",
      "COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\n",
      "COPY --from=builder /usr/local/bin /usr/local/bin\n",
      "\n",
      "# Expose the application port\n",
      "EXPOSE 8000\n",
      "\n",
      "# Run the application\n",
      "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "✅ Successfully saved artifact to: Dockerfile\n"
     ]
    }
   ],
   "source": [
    "# Challenge 2: Enhanced Dockerfile Generation\n",
    "print(\"=\" * 60)\n",
    "print(\"CHALLENGE 2: ENHANCED DOCKERFILE GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create and enhance the raw Dockerfile prompt\n",
    "print(\"\\n--- STEP 1: ENHANCING DOCKERFILE PROMPT ---\")\n",
    "raw_dockerfile_prompt = \"\"\"Generate a best-practice, multi-stage Dockerfile for a production Python FastAPI application.\n",
    "\n",
    "The Dockerfile must:\n",
    "1. Use python:3.11-slim as the base image.\n",
    "2. The first stage should install dependencies from requirements.txt.\n",
    "3. The final stage should copy the application code and the installed dependencies from the first stage.\n",
    "4. Expose port 8000.\n",
    "5. The final CMD should run the application using uvicorn, binding to host 0.0.0.0.\n",
    "6. Follow security best practices including running as non-root user.\n",
    "7. Optimize for minimal image size and efficient layer caching.\n",
    "\n",
    "Output only the raw Dockerfile content.\"\"\"\n",
    "\n",
    "enhanced_dockerfile_prompt = prompt_enhancer(\n",
    "    raw_dockerfile_prompt, \n",
    "    docker_model_name, \n",
    "    docker_client, \n",
    "    docker_api_provider\n",
    ")\n",
    "print(\"Enhanced Dockerfile prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(enhanced_dockerfile_prompt[:400] + \"...\" if len(enhanced_dockerfile_prompt) > 400 else enhanced_dockerfile_prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 2: Generate Dockerfile using specialized client\n",
    "print(\"\\n--- STEP 2: GENERATING DOCKERFILE ---\")\n",
    "print(f\"Using {docker_model_name} ({docker_api_provider}) for Docker configuration...\")\n",
    "\n",
    "dockerfile_content = get_completion(\n",
    "    enhanced_dockerfile_prompt, \n",
    "    docker_client, \n",
    "    docker_model_name, \n",
    "    docker_api_provider,\n",
    "    temperature=0.3  # Slightly higher for creative infrastructure solutions\n",
    ")\n",
    "\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(\"Generated Dockerfile:\")\n",
    "print(\"-\" * 50)\n",
    "print(cleaned_dockerfile)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\")\n",
    "    print(\"✅ Dockerfile saved successfully!\")\n",
    "else:\n",
    "    print(\"❌ Failed to generate valid Dockerfile content.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHALLENGE 2 COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Explanation:**\n",
    "This challenge represents the pinnacle of Configuration-as-Code generation, where we transform a basic CI/CD request into a comprehensive, production-ready automation pipeline. The prompt enhancement process applies sophisticated engineering principles to ensure the generated workflow adheres to industry best practices.\n",
    "\n",
    "The enhancement incorporates:\n",
    "- **Expert CI/CD Specialist Persona**: Establishes deep domain expertise in automation pipelines\n",
    "- **Comprehensive Context Grounding**: Provides detailed understanding of GitHub Actions ecosystem\n",
    "- **Chain-of-Thought Integration**: Guides the model through systematic workflow design decisions\n",
    "- **Strict Output Validation**: Ensures syntactically correct YAML with proper schema compliance\n",
    "- **Security and Performance Considerations**: Integrates caching, parallelization, and secure practices\n",
    "\n",
    "We use DeepSeek-V3.1 for this advanced task because it excels at complex, multi-step reasoning required for sophisticated workflow orchestration. This model's architectural strengths in logical sequencing and dependency management make it ideal for generating robust CI/CD pipelines that can handle real-world complexity and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating GitHub Actions Workflow ---\n",
      "name: Build and Test\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches:\n",
      "      - main\n",
      "\n",
      "jobs:\n",
      "  build-and-test:\n",
      "    runs-on: ubuntu-latest\n",
      "\n",
      "    steps:\n",
      "      - name: Check out repository code\n",
      "        uses: actions/checkout@v4\n",
      "\n",
      "      - name: Set up Python 3.11\n",
      "        uses: actions/setup-python@v5\n",
      "        with:\n",
      "          python-version: '3.11'\n",
      "\n",
      "      - name: Install dependencies\n",
      "        run: pip install -r requirements.txt\n",
      "\n",
      "      - name: Run test suite\n",
      "        run: pytest\n",
      "✅ Successfully saved artifact to: .github/workflows/ci.yml\n"
     ]
    }
   ],
   "source": [
    "# Challenge 3: Enhanced GitHub Actions Workflow Generation\n",
    "print(\"=\" * 60)\n",
    "print(\"CHALLENGE 3: ENHANCED CI/CD WORKFLOW GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create and enhance the raw CI/CD workflow prompt\n",
    "print(\"\\n--- STEP 1: ENHANCING CI/CD WORKFLOW PROMPT ---\")\n",
    "raw_cicd_prompt = \"\"\"Generate a complete GitHub Actions workflow file named ci.yml for a Python FastAPI project.\n",
    "\n",
    "The workflow must:\n",
    "- Be named 'Build and Test'.\n",
    "- Trigger on every push to the main branch and on pull requests.\n",
    "- Define one job named build-and-test that runs on ubuntu-latest.\n",
    "- The job must have the following sequential steps:\n",
    "  1. actions/checkout@v4 to check out the repository code.\n",
    "  2. actions/setup-python@v5 to set up Python 3.11.\n",
    "  3. A step to cache pip dependencies for faster builds.\n",
    "  4. A step to install dependencies using pip from requirements.txt.\n",
    "  5. A step to run the test suite using pytest with verbose output.\n",
    "  6. A step to run basic linting/formatting checks.\n",
    "\n",
    "Include proper error handling, environment variables, and follow GitHub Actions best practices.\n",
    "Output only the raw YAML content for the file.\"\"\"\n",
    "\n",
    "enhanced_cicd_prompt = prompt_enhancer(\n",
    "    raw_cicd_prompt, \n",
    "    cicd_model_name, \n",
    "    cicd_client, \n",
    "    cicd_api_provider\n",
    ")\n",
    "print(\"Enhanced CI/CD workflow prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(enhanced_cicd_prompt[:500] + \"...\" if len(enhanced_cicd_prompt) > 500 else enhanced_cicd_prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 2: Generate CI/CD workflow using specialized client\n",
    "print(\"\\n--- STEP 2: GENERATING CI/CD WORKFLOW ---\")\n",
    "print(f\"Using {cicd_model_name} ({cicd_api_provider}) for advanced workflow orchestration...\")\n",
    "\n",
    "ci_workflow_content = get_completion(\n",
    "    enhanced_cicd_prompt, \n",
    "    cicd_client, \n",
    "    cicd_model_name, \n",
    "    cicd_api_provider,\n",
    "    temperature=0.2  # Low temperature for consistent, reliable automation\n",
    ")\n",
    "\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(\"Generated GitHub Actions Workflow:\")\n",
    "print(\"-\" * 50)\n",
    "print(cleaned_yaml)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\")\n",
    "    print(\"✅ GitHub Actions workflow saved successfully!\")\n",
    "else:\n",
    "    print(\"❌ Failed to generate valid workflow content.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHALLENGE 3 COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Outstanding! You have successfully demonstrated the power of **Enhanced AI-Driven DevOps** by generating a complete, enterprise-grade Continuous Integration pipeline using advanced prompt engineering techniques and strategic model selection.\n",
    "\n",
    "### Key Innovations Demonstrated:\n",
    "\n",
    "1. **Prompt Enhancement Engineering**: Transformed basic requests into sophisticated, production-ready specifications using our `prompt_enhancer` meta-system that applies expert personas, contextual grounding, and structured output requirements.\n",
    "\n",
    "2. **Strategic Model Selection**: \n",
    "   - **Gemini Flash** for fast dependency parsing (foundational tasks)\n",
    "   - **GPT-4o** for balanced infrastructure generation (intermediate complexity)\n",
    "   - **DeepSeek-V3.1** for complex workflow orchestration (advanced reasoning)\n",
    "\n",
    "3. **Configuration-as-Code Mastery**: Generated three critical DevOps artifacts:\n",
    "   - `requirements.txt` (dependency management)\n",
    "   - `Dockerfile` (containerization with security best practices)\n",
    "   - `ci.yml` (automated testing and integration pipeline)\n",
    "\n",
    "### Professional Impact:\n",
    "\n",
    "This lab demonstrates how AI can elevate DevOps practices from manual, error-prone processes to automated, consistent, and secure infrastructure generation. By combining prompt enhancement with strategic model selection, you've created a scalable approach that can be applied across different CI/CD complexity levels while maintaining cost-effectiveness and quality.\n",
    "\n",
    "> **Key Takeaway:** The integration of prompt enhancement with specialized model selection represents the next evolution in AI-assisted DevOps. This approach enables teams to generate sophisticated, production-ready infrastructure configurations that incorporate security best practices, performance optimizations, and industry standards—all while maintaining the flexibility to optimize for different complexity levels and cost considerations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
