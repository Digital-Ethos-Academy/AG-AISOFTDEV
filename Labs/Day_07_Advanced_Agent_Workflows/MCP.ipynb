{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP) Tutorial with LangChain/LangGraph\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to MCP](#introduction)\n",
    "2. [Core Concepts and Architecture](#core-concepts)\n",
    "3. [Setting Up Your Environment](#setup)\n",
    "4. [Basic MCP Examples](#basic-examples)\n",
    "5. [Integrating with LangChain](#langchain-integration)\n",
    "6. [Advanced Examples with LangGraph](#langgraph-examples)\n",
    "7. [Building Custom MCP Servers](#custom-servers)\n",
    "8. [Best Practices and Patterns](#best-practices)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You have access to the tracking system at {trackin_url}.\n",
    "Current user is {user_id} with the following permissions: {permissions}\"\"\"\n",
    "\n",
    "mcp_context= {\n",
    "    \"request\": {\n",
    "        \"intent\": \"track_shipment\",\n",
    "        \"shipping_id\": \"12345\"\n",
    "    },\n",
    "    \"context\":{\n",
    "        \"resources\":[\n",
    "            \"uri\": \"...\",\n",
    "            \"uri\": \"...\"\n",
    "        ]\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to MCP <a id=\"introduction\"></a>\n",
    "\n",
    "The Model Context Protocol (MCP) is an open protocol that standardizes how AI assistants connect with external data sources and tools. Think of it as a universal adapter that allows AI models to interact with various services in a consistent way.\n",
    "\n",
    "### Why MCP Matters\n",
    "\n",
    "Before MCP, integrating AI models with external tools required custom implementations for each service. MCP provides:\n",
    "- **Standardization**: One protocol for all integrations\n",
    "- **Interoperability**: Tools built for one AI system work with others\n",
    "- **Security**: Built-in safety features for tool execution\n",
    "- **Flexibility**: Support for various data sources and operations\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **MCP Servers**: Expose resources, tools, and prompts to AI systems\n",
    "2. **MCP Clients**: Connect to servers and use their capabilities\n",
    "3. **Resources**: Data sources that can be read by the AI\n",
    "4. **Tools**: Functions the AI can execute\n",
    "5. **Prompts**: Pre-configured templates for common tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Concepts and Architecture <a id=\"core-concepts\"></a>\n",
    "\n",
    "### MCP Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────┐     MCP Protocol     ┌─────────────┐\n",
    "│   AI Model  │ ◄──────────────────► │ MCP Server  │\n",
    "│  (Client)   │                      │             │\n",
    "└─────────────┘                      └─────────────┘\n",
    "       │                                     │\n",
    "       │                                     │\n",
    "       ▼                                     ▼\n",
    "┌─────────────┐                      ┌─────────────┐\n",
    "│  LangChain  │                      │  External   │\n",
    "│  Framework  │                      │  Services   │\n",
    "└─────────────┘                      └─────────────┘\n",
    "```\n",
    "\n",
    "### Protocol Features\n",
    "\n",
    "MCP uses JSON-RPC 2.0 for communication and supports:\n",
    "- **Bidirectional communication**: Both client and server can initiate requests\n",
    "- **Streaming**: For real-time data updates\n",
    "- **Progress tracking**: Monitor long-running operations\n",
    "- **Error handling**: Standardized error responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up Your Environment <a id=\"setup\"></a>\n",
    "\n",
    "Let's start by installing the necessary packages and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mcp in /opt/anaconda3/lib/python3.11/site-packages (1.12.3.dev2+959d4e3)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.11/site-packages (0.3.26)\n",
      "Collecting langchain-mcp\n",
      "  Downloading langchain_mcp-0.2.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: anthropic in /opt/anaconda3/lib/python3.11/site-packages (0.58.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: anyio>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (4.7.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (4.23.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (2.5.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (2.11.7)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (2.1.3)\n",
      "Requirement already satisfied: starlette>=0.27 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (0.46.2)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from mcp) (0.32.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-mcp) (4.14.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (0.6.0)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (0.2.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from anthropic) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio>=4.5->mcp) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.27->mcp) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.27->mcp) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->mcp) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp) (0.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.1)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.8.0->mcp) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.8.0->mcp) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.8.0->mcp) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn>=0.23.1->mcp) (8.2.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Downloading langchain_mcp-0.2.1-py3-none-any.whl (4.1 kB)\n",
      "Installing collected packages: langchain-mcp\n",
      "Successfully installed langchain-mcp-0.2.1\n",
      "Requirement already satisfied: uvicorn in /opt/anaconda3/lib/python3.11/site-packages (0.32.1)\n",
      "Requirement already satisfied: fastapi in /opt/anaconda3/lib/python3.11/site-packages (0.115.14)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/anaconda3/lib/python3.11/site-packages (from fastapi) (0.46.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from fastapi) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/anaconda3/lib/python3.11/site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.7.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install mcp langchain langchain-mcp langgraph anthropic python-dotenv\n",
    "\n",
    "# For MCP server development\n",
    "!pip install uvicorn fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# MCP imports\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_mcp import create_mcp_tools\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Set up environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic MCP Examples <a id=\"basic-examples\"></a>\n",
    "\n",
    "Let's start with basic examples to understand how MCP works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Connecting to an MCP Server\n",
    "\n",
    "First, let's create a simple connection to an MCP server. This example shows the fundamental pattern of establishing a client-server connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def connect_to_mcp_server(server_command: List[str]):\n",
    "    \"\"\"\n",
    "    Establishes a connection to an MCP server.\n",
    "    \n",
    "    Args:\n",
    "        server_command: Command to start the MCP server (e.g., ['node', 'server.js'])\n",
    "    \n",
    "    Returns:\n",
    "        ClientSession: Active MCP client session\n",
    "    \"\"\"\n",
    "    # Create server parameters\n",
    "    server_params = StdioServerParameters(\n",
    "        command=server_command[0],\n",
    "        args=server_command[1:] if len(server_command) > 1 else [],\n",
    "        env=None  # Use current environment\n",
    "    )\n",
    "    \n",
    "    # Start the client connection\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Get server information\n",
    "            info = await session.get_server_info()\n",
    "            print(f\"Connected to: {info.name}\")\n",
    "            print(f\"Version: {info.version}\")\n",
    "            \n",
    "            return session\n",
    "\n",
    "# Example usage (requires an actual MCP server)\n",
    "# await connect_to_mcp_server(['python', 'my_mcp_server.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Working with MCP Resources\n",
    "\n",
    "Resources in MCP are data sources that the AI can read. Let's explore how to list and read resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def explore_mcp_resources(session: ClientSession):\n",
    "    \"\"\"\n",
    "    Explores available resources from an MCP server.\n",
    "    \n",
    "    This function demonstrates:\n",
    "    1. Listing all available resources\n",
    "    2. Reading specific resource content\n",
    "    3. Handling resource metadata\n",
    "    \"\"\"\n",
    "    # List all available resources\n",
    "    resources = await session.list_resources()\n",
    "    \n",
    "    print(\"Available Resources:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for resource in resources:\n",
    "        print(f\"\\nResource: {resource.uri}\")\n",
    "        print(f\"Name: {resource.name}\")\n",
    "        print(f\"Description: {resource.description}\")\n",
    "        print(f\"MIME Type: {resource.mimeType}\")\n",
    "        \n",
    "        # Read the resource content\n",
    "        try:\n",
    "            content = await session.read_resource(resource.uri)\n",
    "            print(f\"Content Preview: {content.text[:200]}...\" if len(content.text) > 200 else f\"Content: {content.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading resource: {e}\")\n",
    "\n",
    "# Example of a mock resource for demonstration\n",
    "class MockResource:\n",
    "    def __init__(self, uri: str, name: str, description: str, content: str):\n",
    "        self.uri = uri\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.mimeType = \"text/plain\"\n",
    "        self.content = content\n",
    "\n",
    "# Simulate resource exploration\n",
    "mock_resources = [\n",
    "    MockResource(\n",
    "        \"file:///data/customers.json\",\n",
    "        \"Customer Database\",\n",
    "        \"JSON file containing customer information\",\n",
    "        '{\"customers\": [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]}'\n",
    "    ),\n",
    "    MockResource(\n",
    "        \"api://weather/current\",\n",
    "        \"Current Weather\",\n",
    "        \"Real-time weather data\",\n",
    "        '{\"temperature\": 72, \"conditions\": \"sunny\", \"humidity\": 45}'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Simulated Resource Exploration:\")\n",
    "for resource in mock_resources:\n",
    "    print(f\"\\nResource: {resource.uri}\")\n",
    "    print(f\"Content: {resource.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Using MCP Tools\n",
    "\n",
    "Tools in MCP are functions that the AI can execute. They're similar to function calling in LLMs but standardized across different systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_mcp_tools(session: ClientSession):\n",
    "    \"\"\"\n",
    "    Demonstrates how to work with MCP tools.\n",
    "    \n",
    "    Tools are executable functions that the AI can call.\n",
    "    Each tool has:\n",
    "    - A unique name\n",
    "    - Input schema (JSON Schema format)\n",
    "    - Description for the AI to understand its purpose\n",
    "    \"\"\"\n",
    "    # List available tools\n",
    "    tools = await session.list_tools()\n",
    "    \n",
    "    print(\"Available Tools:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for tool in tools:\n",
    "        print(f\"\\nTool: {tool.name}\")\n",
    "        print(f\"Description: {tool.description}\")\n",
    "        print(f\"Input Schema: {json.dumps(tool.inputSchema, indent=2)}\")\n",
    "        \n",
    "        # Example: Call a tool\n",
    "        if tool.name == \"calculate\":\n",
    "            # Prepare tool arguments based on the schema\n",
    "            arguments = {\n",
    "                \"operation\": \"add\",\n",
    "                \"a\": 10,\n",
    "                \"b\": 20\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                result = await session.call_tool(tool.name, arguments=arguments)\n",
    "                print(f\"\\nTool Result: {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error calling tool: {e}\")\n",
    "\n",
    "# Simulate tool demonstration\n",
    "print(\"Simulated MCP Tools:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "example_tools = [\n",
    "    {\n",
    "        \"name\": \"search_database\",\n",
    "        \"description\": \"Search customer database with filters\",\n",
    "        \"inputSchema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "                \"filters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"status\": {\"type\": \"string\", \"enum\": [\"active\", \"inactive\"]},\n",
    "                        \"created_after\": {\"type\": \"string\", \"format\": \"date\"}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_email\",\n",
    "        \"description\": \"Send an email to a recipient\",\n",
    "        \"inputSchema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\"type\": \"string\", \"format\": \"email\"},\n",
    "                \"subject\": {\"type\": \"string\"},\n",
    "                \"body\": {\"type\": \"string\"}\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "for tool in example_tools:\n",
    "    print(f\"\\nTool: {tool['name']}\")\n",
    "    print(f\"Description: {tool['description']}\")\n",
    "    print(f\"Schema: {json.dumps(tool['inputSchema'], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integrating with LangChain <a id=\"langchain-integration\"></a>\n",
    "\n",
    "Now let's explore how to integrate MCP with LangChain, which allows us to use MCP tools within LangChain agents and chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a mock MCP tool adapter for demonstration\n",
    "class MCPToolAdapter:\n",
    "    \"\"\"\n",
    "    Adapts MCP tools to work with LangChain.\n",
    "    This demonstrates the pattern used by langchain-mcp-adapters.\n",
    "    \"\"\"\n",
    "    def __init__(self, mcp_tool_config):\n",
    "        self.name = mcp_tool_config[\"name\"]\n",
    "        self.description = mcp_tool_config[\"description\"]\n",
    "        self.schema = mcp_tool_config[\"inputSchema\"]\n",
    "    \n",
    "    def to_langchain_tool(self) -> Tool:\n",
    "        \"\"\"\n",
    "        Converts an MCP tool to a LangChain Tool.\n",
    "        \"\"\"\n",
    "        def tool_func(**kwargs):\n",
    "            # In a real implementation, this would call the MCP server\n",
    "            return f\"Executed {self.name} with args: {kwargs}\"\n",
    "        \n",
    "        return Tool(\n",
    "            name=self.name,\n",
    "            description=self.description,\n",
    "            func=tool_func\n",
    "        )\n",
    "\n",
    "# Create LangChain tools from MCP tools\n",
    "mcp_tools_config = [\n",
    "    {\n",
    "        \"name\": \"weather_check\",\n",
    "        \"description\": \"Get current weather for a location\",\n",
    "        \"inputSchema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculate_distance\",\n",
    "        \"description\": \"Calculate distance between two cities\",\n",
    "        \"inputSchema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city1\": {\"type\": \"string\"},\n",
    "                \"city2\": {\"type\": \"string\"}\n",
    "            },\n",
    "            \"required\": [\"city1\", \"city2\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert MCP tools to LangChain tools\n",
    "langchain_tools = []\n",
    "for config in mcp_tools_config:\n",
    "    adapter = MCPToolAdapter(config)\n",
    "    langchain_tools.append(adapter.to_langchain_tool())\n",
    "\n",
    "print(\"Created LangChain tools from MCP:\")\n",
    "for tool in langchain_tools:\n",
    "    print(f\"- {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MCP Tools with LangChain Agents\n",
    "\n",
    "Now let's create a LangChain agent that can use MCP tools. This demonstrates how MCP enables tool use across different AI frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "# For demonstration, we'll use a mock LLM\n",
    "# In practice, you'd use a real model like ChatAnthropic or ChatOpenAI\n",
    "mock_llm = FakeListLLM(\n",
    "    responses=[\n",
    "        \"I need to check the weather in New York.\",\n",
    "        \"I'll calculate the distance between New York and Los Angeles.\",\n",
    "        \"Based on my findings, the weather in New York is sunny and the distance to LA is approximately 2,800 miles.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create an agent with MCP tools\n",
    "def create_mcp_agent(tools: List[Tool], llm):\n",
    "    \"\"\"\n",
    "    Creates a LangChain agent equipped with MCP tools.\n",
    "    \n",
    "    This demonstrates the integration pattern where:\n",
    "    1. MCP tools are converted to LangChain tools\n",
    "    2. The agent can dynamically decide which tools to use\n",
    "    3. Tool results are incorporated into the agent's reasoning\n",
    "    \"\"\"\n",
    "    # Create a custom prompt that explains MCP tools\n",
    "    prompt_template = \"\"\"\n",
    "You are an AI assistant with access to Model Context Protocol (MCP) tools.\n",
    "These tools allow you to interact with external services and data sources.\n",
    "\n",
    "Available tools:\n",
    "{tools}\n",
    "\n",
    "Use these tools to help answer the user's questions accurately.\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"input\", \"tools\", \"agent_scratchpad\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Create and test the agent\n",
    "mcp_agent = create_mcp_agent(langchain_tools, mock_llm)\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nAgent with MCP Tools Example:\")\n",
    "print(\"=\" * 60)\n",
    "# In a real scenario, you would run:\n",
    "# response = mcp_agent.run(\"What's the weather in New York and how far is it from LA?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world MCP Integration Pattern\n",
    "\n",
    "Here's how you would actually integrate MCP with LangChain using the official adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def integrate_mcp_with_langchain():\n",
    "    \"\"\"\n",
    "    Demonstrates the actual integration pattern using langchain-mcp-adapters.\n",
    "    \n",
    "    This shows:\n",
    "    1. Connecting to an MCP server\n",
    "    2. Converting MCP tools to LangChain tools\n",
    "    3. Using them in a LangChain application\n",
    "    \"\"\"\n",
    "    # This is the pattern you'd use with real MCP servers\n",
    "    from langchain_mcp import create_mcp_tools\n",
    "    \n",
    "    # Connect to MCP server\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"npx\",\n",
    "        args=[\"@modelcontextprotocol/server-filesystem\", \"/path/to/files\"]\n",
    "    )\n",
    "    \n",
    "    # In practice, you'd do something like:\n",
    "    # async with stdio_client(server_params) as (read, write):\n",
    "    #     async with ClientSession(read, write) as session:\n",
    "    #         await session.initialize()\n",
    "    #         \n",
    "    #         # Create LangChain tools from MCP session\n",
    "    #         tools = await create_mcp_tools(session)\n",
    "    #         \n",
    "    #         # Use tools in your LangChain application\n",
    "    #         agent = create_mcp_agent(tools, llm)\n",
    "    #         result = await agent.arun(\"Search for customer data files\")\n",
    "    \n",
    "    # For demonstration, let's show the expected structure\n",
    "    print(\"MCP + LangChain Integration Pattern:\")\n",
    "    print(\"1. Connect to MCP server\")\n",
    "    print(\"2. Initialize session\")\n",
    "    print(\"3. Convert MCP tools to LangChain tools\")\n",
    "    print(\"4. Use in agents, chains, or graphs\")\n",
    "    print(\"5. MCP handles the actual execution\")\n",
    "\n",
    "# Show the integration pattern\n",
    "await integrate_mcp_with_langchain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Examples with LangGraph <a id=\"langgraph-examples\"></a>\n",
    "\n",
    "LangGraph allows us to build more complex, stateful applications. Let's see how MCP tools can be integrated into LangGraph workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import operator\n",
    "\n",
    "# Define the state for our graph\n",
    "class MCPAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State definition for our MCP-powered agent.\n",
    "    \n",
    "    This state tracks:\n",
    "    - Message history\n",
    "    - Tool calls and results\n",
    "    - Current task status\n",
    "    \"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    current_task: str\n",
    "    task_completed: bool\n",
    "    tool_results: List[Dict[str, Any]]\n",
    "\n",
    "# Create nodes for our graph\n",
    "def task_analyzer(state: MCPAgentState) -> MCPAgentState:\n",
    "    \"\"\"\n",
    "    Analyzes the current task and determines which MCP tools to use.\n",
    "    \n",
    "    This node demonstrates how to:\n",
    "    1. Parse user intent\n",
    "    2. Map intent to available MCP tools\n",
    "    3. Prepare for tool execution\n",
    "    \"\"\"\n",
    "    task = state[\"current_task\"]\n",
    "    \n",
    "    # Analyze task (in practice, you'd use an LLM here)\n",
    "    analysis = {\n",
    "        \"weather\": \"weather_check\",\n",
    "        \"distance\": \"calculate_distance\",\n",
    "        \"search\": \"search_database\",\n",
    "        \"email\": \"send_email\"\n",
    "    }\n",
    "    \n",
    "    # Determine required tools\n",
    "    required_tools = []\n",
    "    for keyword, tool in analysis.items():\n",
    "        if keyword in task.lower():\n",
    "            required_tools.append(tool)\n",
    "    \n",
    "    # Update state\n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(content=f\"I'll use these MCP tools: {required_tools}\")\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "def mcp_tool_executor(state: MCPAgentState) -> MCPAgentState:\n",
    "    \"\"\"\n",
    "    Executes MCP tools based on the task analysis.\n",
    "    \n",
    "    In a real implementation, this would:\n",
    "    1. Connect to the MCP server\n",
    "    2. Call the appropriate tools\n",
    "    3. Return results to the state\n",
    "    \"\"\"\n",
    "    # Simulate tool execution\n",
    "    mock_results = [\n",
    "        {\"tool\": \"weather_check\", \"result\": \"New York: 72°F, Sunny\"},\n",
    "        {\"tool\": \"calculate_distance\", \"result\": \"NYC to LA: 2,789 miles\"}\n",
    "    ]\n",
    "    \n",
    "    state[\"tool_results\"] = mock_results\n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(content=\"MCP tools executed successfully\")\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "def result_synthesizer(state: MCPAgentState) -> MCPAgentState:\n",
    "    \"\"\"\n",
    "    Synthesizes results from MCP tools into a coherent response.\n",
    "    \"\"\"\n",
    "    results = state[\"tool_results\"]\n",
    "    \n",
    "    # Create summary\n",
    "    summary = \"Based on MCP tool results:\\n\"\n",
    "    for result in results:\n",
    "        summary += f\"- {result['result']}\\n\"\n",
    "    \n",
    "    state[\"messages\"].append(AIMessage(content=summary))\n",
    "    state[\"task_completed\"] = True\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "def create_mcp_langgraph():\n",
    "    \"\"\"\n",
    "    Creates a LangGraph workflow that integrates MCP tools.\n",
    "    \n",
    "    This demonstrates:\n",
    "    1. Stateful processing with MCP\n",
    "    2. Conditional routing based on tool availability\n",
    "    3. Error handling and retries\n",
    "    \"\"\"\n",
    "    # Initialize the graph\n",
    "    workflow = StateGraph(MCPAgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"analyze_task\", task_analyzer)\n",
    "    workflow.add_node(\"execute_mcp_tools\", mcp_tool_executor)\n",
    "    workflow.add_node(\"synthesize_results\", result_synthesizer)\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.set_entry_point(\"analyze_task\")\n",
    "    workflow.add_edge(\"analyze_task\", \"execute_mcp_tools\")\n",
    "    workflow.add_edge(\"execute_mcp_tools\", \"synthesize_results\")\n",
    "    \n",
    "    # Conditional edge: check if task is completed\n",
    "    def should_continue(state: MCPAgentState) -> str:\n",
    "        if state[\"task_completed\"]:\n",
    "            return END\n",
    "        return \"analyze_task\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"synthesize_results\",\n",
    "        should_continue\n",
    "    )\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create and visualize the workflow\n",
    "mcp_graph = create_mcp_langgraph()\n",
    "print(\"MCP LangGraph workflow created!\")\n",
    "\n",
    "# Example usage\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Check weather in NYC and distance to LA\")],\n",
    "    \"current_task\": \"Check weather in NYC and distance to LA\",\n",
    "    \"task_completed\": False,\n",
    "    \"tool_results\": []\n",
    "}\n",
    "\n",
    "# In practice, you would run:\n",
    "# final_state = mcp_graph.invoke(initial_state)\n",
    "# print(final_state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex MCP Workflow Example\n",
    "\n",
    "Let's create a more sophisticated example that shows how MCP can power a multi-step research assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ResearchPhase(str, Enum):\n",
    "    \"\"\"Phases of our research workflow\"\"\"\n",
    "    PLANNING = \"planning\"\n",
    "    GATHERING = \"gathering\"\n",
    "    ANALYZING = \"analyzing\"\n",
    "    REPORTING = \"reporting\"\n",
    "    COMPLETE = \"complete\"\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    \"\"\"Enhanced state for research workflows\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    research_topic: str\n",
    "    current_phase: ResearchPhase\n",
    "    research_plan: List[str]\n",
    "    gathered_data: Dict[str, Any]\n",
    "    analysis_results: Dict[str, Any]\n",
    "    final_report: Optional[str]\n",
    "    mcp_resources_used: List[str]\n",
    "    mcp_tools_used: List[str]\n",
    "\n",
    "def research_planner(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Plans research using available MCP resources and tools.\n",
    "    \n",
    "    This demonstrates how to:\n",
    "    1. Query MCP for available resources\n",
    "    2. Select appropriate tools for the research\n",
    "    3. Create an execution plan\n",
    "    \"\"\"\n",
    "    topic = state[\"research_topic\"]\n",
    "    \n",
    "    # In practice, you'd query the MCP server for available resources\n",
    "    available_resources = [\n",
    "        \"database://customers\",\n",
    "        \"api://market_data\",\n",
    "        \"file://reports/2024\"\n",
    "    ]\n",
    "    \n",
    "    available_tools = [\n",
    "        \"search_web\",\n",
    "        \"analyze_sentiment\",\n",
    "        \"generate_summary\",\n",
    "        \"create_visualization\"\n",
    "    ]\n",
    "    \n",
    "    # Create research plan\n",
    "    plan = [\n",
    "        f\"1. Search MCP resources for '{topic}'\",\n",
    "        f\"2. Use web search tool for recent information\",\n",
    "        f\"3. Analyze sentiment of gathered data\",\n",
    "        f\"4. Generate comprehensive summary\",\n",
    "        f\"5. Create visualizations of key findings\"\n",
    "    ]\n",
    "    \n",
    "    state[\"research_plan\"] = plan\n",
    "    state[\"mcp_resources_used\"] = available_resources[:2]  # Select relevant resources\n",
    "    state[\"mcp_tools_used\"] = available_tools[:3]  # Select relevant tools\n",
    "    state[\"current_phase\"] = ResearchPhase.GATHERING\n",
    "    \n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(content=f\"Research plan created. Using {len(state['mcp_resources_used'])} MCP resources and {len(state['mcp_tools_used'])} tools.\")\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "def data_gatherer(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Gathers data from MCP resources.\n",
    "    \"\"\"\n",
    "    # Simulate gathering data from MCP resources\n",
    "    gathered_data = {\n",
    "        \"database_results\": [\n",
    "            {\"id\": 1, \"relevance\": \"high\", \"data\": \"Customer feedback on topic\"},\n",
    "            {\"id\": 2, \"relevance\": \"medium\", \"data\": \"Historical trends\"}\n",
    "        ],\n",
    "        \"api_results\": {\n",
    "            \"market_trends\": \"positive\",\n",
    "            \"competitor_analysis\": \"3 main competitors identified\"\n",
    "        },\n",
    "        \"web_search_results\": [\n",
    "            \"Recent news article 1\",\n",
    "            \"Industry report 2024\",\n",
    "            \"Expert opinion piece\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    state[\"gathered_data\"] = gathered_data\n",
    "    state[\"current_phase\"] = ResearchPhase.ANALYZING\n",
    "    \n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(content=\"Data gathering complete. Retrieved data from all MCP sources.\")\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "def data_analyzer(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Analyzes gathered data using MCP tools.\n",
    "    \"\"\"\n",
    "    # Simulate analysis using MCP tools\n",
    "    analysis_results = {\n",
    "        \"sentiment_analysis\": {\n",
    "            \"overall\": \"positive\",\n",
    "            \"confidence\": 0.85,\n",
    "            \"key_themes\": [\"innovation\", \"growth\", \"customer satisfaction\"]\n",
    "        },\n",
    "        \"trend_analysis\": {\n",
    "            \"direction\": \"upward\",\n",
    "            \"growth_rate\": \"15% YoY\",\n",
    "            \"forecast\": \"continued growth expected\"\n",
    "        },\n",
    "        \"key_insights\": [\n",
    "            \"Strong market position\",\n",
    "            \"Customer satisfaction above industry average\",\n",
    "            \"Opportunity for expansion in emerging markets\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    state[\"analysis_results\"] = analysis_results\n",
    "    state[\"current_phase\"] = ResearchPhase.REPORTING\n",
    "    \n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(content=\"Analysis complete. Key insights identified.\")\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "def report_generator(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Generates final report using MCP tools.\n",
    "    \"\"\"\n",
    "    # Generate comprehensive report\n",
    "    report = f\"\"\"\n",
    "# Research Report: {state['research_topic']}\n",
    "\n",
    "## Executive Summary\n",
    "This research utilized {len(state['mcp_resources_used'])} MCP resources and {len(state['mcp_tools_used'])} MCP tools to analyze the topic comprehensively.\n",
    "\n",
    "## Key Findings\n",
    "- Overall sentiment: {state['analysis_results']['sentiment_analysis']['overall']}\n",
    "- Growth trend: {state['analysis_results']['trend_analysis']['growth_rate']}\n",
    "- Market position: Strong\n",
    "\n",
    "## Detailed Analysis\n",
    "{json.dumps(state['analysis_results'], indent=2)}\n",
    "\n",
    "## Data Sources\n",
    "- MCP Resources: {', '.join(state['mcp_resources_used'])}\n",
    "- MCP Tools: {', '.join(state['mcp_tools_used'])}\n",
    "\n",
    "## Recommendations\n",
    "1. Leverage positive market sentiment\n",
    "2. Expand into identified growth areas\n",
    "3. Continue monitoring using MCP tools\n",
    "\"\"\"\n",
    "    \n",
    "    state[\"final_report\"] = report\n",
    "    state[\"current_phase\"] = ResearchPhase.COMPLETE\n",
    "    \n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(content=\"Research complete. Final report generated.\")\n",
    "    )\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Create the research workflow\n",
    "def create_research_workflow():\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    workflow.add_node(\"plan\", research_planner)\n",
    "    workflow.add_node(\"gather\", data_gatherer)\n",
    "    workflow.add_node(\"analyze\", data_analyzer)\n",
    "    workflow.add_node(\"report\", report_generator)\n",
    "    \n",
    "    # Set up edges\n",
    "    workflow.set_entry_point(\"plan\")\n",
    "    workflow.add_edge(\"plan\", \"gather\")\n",
    "    workflow.add_edge(\"gather\", \"analyze\")\n",
    "    workflow.add_edge(\"analyze\", \"report\")\n",
    "    \n",
    "    # Add conditional routing\n",
    "    def route_next(state: ResearchState) -> str:\n",
    "        if state[\"current_phase\"] == ResearchPhase.COMPLETE:\n",
    "            return END\n",
    "        return \"plan\"  # For iterative research\n",
    "    \n",
    "    workflow.add_conditional_edges(\"report\", route_next)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create and demonstrate the workflow\n",
    "research_app = create_research_workflow()\n",
    "print(\"Advanced MCP Research Workflow created!\")\n",
    "print(\"\\nThis workflow demonstrates:\")\n",
    "print(\"- Multi-phase processing with MCP\")\n",
    "print(\"- Resource and tool selection\")\n",
    "print(\"- Data gathering from multiple MCP sources\")\n",
    "print(\"- Analysis using MCP tools\")\n",
    "print(\"- Report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building Custom MCP Servers <a id=\"custom-servers\"></a>\n",
    "\n",
    "Let's explore how to build your own MCP server that can be used by any MCP-compatible client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a custom MCP server implementation\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class MCPResource:\n",
    "    \"\"\"Represents an MCP resource\"\"\"\n",
    "    uri: str\n",
    "    name: str\n",
    "    description: str\n",
    "    mime_type: str = \"application/json\"\n",
    "\n",
    "@dataclass\n",
    "class MCPTool:\n",
    "    \"\"\"Represents an MCP tool\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: Dict[str, Any]\n",
    "\n",
    "class CustomMCPServer:\n",
    "    \"\"\"\n",
    "    A custom MCP server implementation.\n",
    "    \n",
    "    This demonstrates the key components of an MCP server:\n",
    "    1. Resource management\n",
    "    2. Tool implementation\n",
    "    3. Protocol handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, version: str):\n",
    "        self.name = name\n",
    "        self.version = version\n",
    "        self.resources: Dict[str, MCPResource] = {}\n",
    "        self.tools: Dict[str, MCPTool] = {}\n",
    "        self._setup_default_resources()\n",
    "        self._setup_default_tools()\n",
    "    \n",
    "    def _setup_default_resources(self):\n",
    "        \"\"\"Initialize default resources\"\"\"\n",
    "        # Add a sample database resource\n",
    "        self.resources[\"db://users\"] = MCPResource(\n",
    "            uri=\"db://users\",\n",
    "            name=\"User Database\",\n",
    "            description=\"Access to user information\",\n",
    "            mime_type=\"application/json\"\n",
    "        )\n",
    "        \n",
    "        # Add a configuration resource\n",
    "        self.resources[\"config://settings\"] = MCPResource(\n",
    "            uri=\"config://settings\",\n",
    "            name=\"Application Settings\",\n",
    "            description=\"Current application configuration\",\n",
    "            mime_type=\"application/json\"\n",
    "        )\n",
    "    \n",
    "    def _setup_default_tools(self):\n",
    "        \"\"\"Initialize default tools\"\"\"\n",
    "        # User lookup tool\n",
    "        self.tools[\"lookup_user\"] = MCPTool(\n",
    "            name=\"lookup_user\",\n",
    "            description=\"Look up a user by ID or email\",\n",
    "            input_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"identifier\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"User ID or email address\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"identifier\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Analytics tool\n",
    "        self.tools[\"calculate_metrics\"] = MCPTool(\n",
    "            name=\"calculate_metrics\",\n",
    "            description=\"Calculate user engagement metrics\",\n",
    "            input_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"metric_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"engagement\", \"retention\", \"growth\"],\n",
    "                        \"description\": \"Type of metric to calculate\"\n",
    "                    },\n",
    "                    \"time_period\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"day\", \"week\", \"month\"],\n",
    "                        \"description\": \"Time period for calculation\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"metric_type\", \"time_period\"]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    async def handle_list_resources(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Handle list_resources request\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"uri\": resource.uri,\n",
    "                \"name\": resource.name,\n",
    "                \"description\": resource.description,\n",
    "                \"mimeType\": resource.mime_type\n",
    "            }\n",
    "            for resource in self.resources.values()\n",
    "        ]\n",
    "    \n",
    "    async def handle_read_resource(self, uri: str) -> Dict[str, Any]:\n",
    "        \"\"\"Handle read_resource request\"\"\"\n",
    "        if uri not in self.resources:\n",
    "            raise ValueError(f\"Resource not found: {uri}\")\n",
    "        \n",
    "        # Simulate reading resource data\n",
    "        if uri == \"db://users\":\n",
    "            return {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"uri\": uri,\n",
    "                        \"mimeType\": \"application/json\",\n",
    "                        \"text\": json.dumps([\n",
    "                            {\"id\": \"001\", \"name\": \"Alice\", \"email\": \"alice@example.com\"},\n",
    "                            {\"id\": \"002\", \"name\": \"Bob\", \"email\": \"bob@example.com\"}\n",
    "                        ])\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        elif uri == \"config://settings\":\n",
    "            return {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"uri\": uri,\n",
    "                        \"mimeType\": \"application/json\",\n",
    "                        \"text\": json.dumps({\n",
    "                            \"theme\": \"dark\",\n",
    "                            \"language\": \"en\",\n",
    "                            \"features\": {\"analytics\": True, \"export\": True}\n",
    "                        })\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    async def handle_list_tools(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Handle list_tools request\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"inputSchema\": tool.input_schema\n",
    "            }\n",
    "            for tool in self.tools.values()\n",
    "        ]\n",
    "    \n",
    "    async def handle_call_tool(self, name: str, arguments: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Handle call_tool request\"\"\"\n",
    "        if name not in self.tools:\n",
    "            raise ValueError(f\"Tool not found: {name}\")\n",
    "        \n",
    "        # Implement tool logic\n",
    "        if name == \"lookup_user\":\n",
    "            identifier = arguments[\"identifier\"]\n",
    "            # Simulate user lookup\n",
    "            result = {\n",
    "                \"id\": \"001\",\n",
    "                \"name\": \"Alice\",\n",
    "                \"email\": \"alice@example.com\",\n",
    "                \"last_login\": \"2024-01-15T10:30:00Z\"\n",
    "            }\n",
    "            return [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": json.dumps(result, indent=2)\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        elif name == \"calculate_metrics\":\n",
    "            metric_type = arguments[\"metric_type\"]\n",
    "            time_period = arguments[\"time_period\"]\n",
    "            # Simulate metric calculation\n",
    "            result = {\n",
    "                \"metric\": metric_type,\n",
    "                \"period\": time_period,\n",
    "                \"value\": 85.7,\n",
    "                \"change\": \"+12.3%\",\n",
    "                \"trend\": \"increasing\"\n",
    "            }\n",
    "            return [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": json.dumps(result, indent=2)\n",
    "                }\n",
    "            ]\n",
    "\n",
    "# Demonstrate the custom server\n",
    "custom_server = CustomMCPServer(\"Custom Analytics Server\", \"1.0.0\")\n",
    "\n",
    "print(\"Custom MCP Server Created!\")\n",
    "print(f\"Name: {custom_server.name}\")\n",
    "print(f\"Version: {custom_server.version}\")\n",
    "print(f\"\\nResources: {len(custom_server.resources)}\")\n",
    "for uri, resource in custom_server.resources.items():\n",
    "    print(f\"  - {uri}: {resource.name}\")\n",
    "print(f\"\\nTools: {len(custom_server.tools)}\")\n",
    "for name, tool in custom_server.tools.items():\n",
    "    print(f\"  - {name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete MCP Server with Protocol Implementation\n",
    "\n",
    "Here's a more complete example showing how to implement the MCP protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "class MCPProtocolServer:\n",
    "    \"\"\"\n",
    "    Full MCP protocol implementation for a custom server.\n",
    "    \n",
    "    This shows:\n",
    "    1. JSON-RPC 2.0 message handling\n",
    "    2. Request/response patterns\n",
    "    3. Error handling\n",
    "    4. Streaming support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, server: CustomMCPServer):\n",
    "        self.server = server\n",
    "        self.request_handlers = {\n",
    "            \"initialize\": self.handle_initialize,\n",
    "            \"resources/list\": self.handle_list_resources,\n",
    "            \"resources/read\": self.handle_read_resource,\n",
    "            \"tools/list\": self.handle_list_tools,\n",
    "            \"tools/call\": self.handle_call_tool,\n",
    "            \"prompts/list\": self.handle_list_prompts,\n",
    "            \"completion/complete\": self.handle_completion\n",
    "        }\n",
    "    \n",
    "    async def handle_initialize(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle initialization request\"\"\"\n",
    "        return {\n",
    "            \"protocolVersion\": \"2024-10-07\",\n",
    "            \"capabilities\": {\n",
    "                \"resources\": {},\n",
    "                \"tools\": {},\n",
    "                \"prompts\": {}\n",
    "            },\n",
    "            \"serverInfo\": {\n",
    "                \"name\": self.server.name,\n",
    "                \"version\": self.server.version\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    async def handle_list_resources(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle list resources request\"\"\"\n",
    "        resources = await self.server.handle_list_resources()\n",
    "        return {\"resources\": resources}\n",
    "    \n",
    "    async def handle_read_resource(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle read resource request\"\"\"\n",
    "        uri = params.get(\"uri\")\n",
    "        if not uri:\n",
    "            raise ValueError(\"Missing required parameter: uri\")\n",
    "        \n",
    "        return await self.server.handle_read_resource(uri)\n",
    "    \n",
    "    async def handle_list_tools(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle list tools request\"\"\"\n",
    "        tools = await self.server.handle_list_tools()\n",
    "        return {\"tools\": tools}\n",
    "    \n",
    "    async def handle_call_tool(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle tool call request\"\"\"\n",
    "        name = params.get(\"name\")\n",
    "        arguments = params.get(\"arguments\", {})\n",
    "        \n",
    "        if not name:\n",
    "            raise ValueError(\"Missing required parameter: name\")\n",
    "        \n",
    "        content = await self.server.handle_call_tool(name, arguments)\n",
    "        return {\"content\": content}\n",
    "    \n",
    "    async def handle_list_prompts(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle list prompts request\"\"\"\n",
    "        # Example prompts\n",
    "        prompts = [\n",
    "            {\n",
    "                \"name\": \"analyze_user\",\n",
    "                \"description\": \"Analyze user behavior and provide insights\",\n",
    "                \"arguments\": [\n",
    "                    {\n",
    "                        \"name\": \"user_id\",\n",
    "                        \"description\": \"ID of the user to analyze\",\n",
    "                        \"required\": True\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        return {\"prompts\": prompts}\n",
    "    \n",
    "    async def handle_completion(self, params: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:\n",
    "        \"\"\"Handle completion request with streaming\"\"\"\n",
    "        # This demonstrates streaming responses\n",
    "        prompt = params.get(\"messages\", [])\n",
    "        \n",
    "        # Simulate streaming completion\n",
    "        response_chunks = [\n",
    "            \"Based on the analysis, \",\n",
    "            \"the user shows high engagement \",\n",
    "            \"with an average session duration of 15 minutes.\"\n",
    "        ]\n",
    "        \n",
    "        for chunk in response_chunks:\n",
    "            yield {\n",
    "                \"type\": \"content\",\n",
    "                \"content\": {\"type\": \"text\", \"text\": chunk}\n",
    "            }\n",
    "            await asyncio.sleep(0.1)  # Simulate processing time\n",
    "    \n",
    "    async def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a JSON-RPC request\"\"\"\n",
    "        method = request.get(\"method\")\n",
    "        params = request.get(\"params\", {})\n",
    "        request_id = request.get(\"id\")\n",
    "        \n",
    "        try:\n",
    "            if method not in self.request_handlers:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "            \n",
    "            handler = self.request_handlers[method]\n",
    "            result = await handler(params)\n",
    "            \n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"result\": result\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": request_id,\n",
    "                \"error\": {\n",
    "                    \"code\": -32603,\n",
    "                    \"message\": str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Example of how to run the server\n",
    "print(\"MCP Protocol Server Implementation\")\n",
    "print(\"==================================\\n\")\n",
    "print(\"This server implementation provides:\")\n",
    "print(\"1. Full JSON-RPC 2.0 support\")\n",
    "print(\"2. Resource management\")\n",
    "print(\"3. Tool execution\")\n",
    "print(\"4. Prompt templates\")\n",
    "print(\"5. Streaming completions\")\n",
    "print(\"\\nTo run this server:\")\n",
    "print(\"1. Save as 'my_mcp_server.py'\")\n",
    "print(\"2. Run with: python my_mcp_server.py\")\n",
    "print(\"3. Connect from Claude, LangChain, or any MCP client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Patterns <a id=\"best-practices\"></a>\n",
    "\n",
    "Let's explore best practices for working with MCP in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices Implementation Examples\n",
    "\n",
    "class MCPBestPractices:\n",
    "    \"\"\"\n",
    "    Demonstrates best practices for MCP implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    async def secure_tool_execution(tool_name: str, arguments: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Best Practice 1: Input Validation and Sanitization\n",
    "        \n",
    "        Always validate and sanitize inputs before executing tools.\n",
    "        \"\"\"\n",
    "        # Validate tool name\n",
    "        allowed_tools = [\"search\", \"calculate\", \"analyze\"]\n",
    "        if tool_name not in allowed_tools:\n",
    "            raise ValueError(f\"Unauthorized tool: {tool_name}\")\n",
    "        \n",
    "        # Sanitize arguments\n",
    "        sanitized_args = {}\n",
    "        for key, value in arguments.items():\n",
    "            if isinstance(value, str):\n",
    "                # Remove potential injection attacks\n",
    "                sanitized_value = value.replace(\";\", \"\").replace(\"--\", \"\")\n",
    "                sanitized_args[key] = sanitized_value[:1000]  # Limit length\n",
    "            else:\n",
    "                sanitized_args[key] = value\n",
    "        \n",
    "        # Execute with timeout\n",
    "        try:\n",
    "            result = await asyncio.wait_for(\n",
    "                execute_tool(tool_name, sanitized_args),\n",
    "                timeout=30.0  # 30 second timeout\n",
    "            )\n",
    "            return result\n",
    "        except asyncio.TimeoutError:\n",
    "            raise TimeoutError(f\"Tool {tool_name} execution timed out\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def implement_rate_limiting():\n",
    "        \"\"\"\n",
    "        Best Practice 2: Rate Limiting\n",
    "        \n",
    "        Implement rate limiting to prevent abuse.\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        from time import time\n",
    "        \n",
    "        class RateLimiter:\n",
    "            def __init__(self, max_requests: int, window_seconds: int):\n",
    "                self.max_requests = max_requests\n",
    "                self.window_seconds = window_seconds\n",
    "                self.requests = defaultdict(list)\n",
    "            \n",
    "            def is_allowed(self, client_id: str) -> bool:\n",
    "                now = time()\n",
    "                # Clean old requests\n",
    "                self.requests[client_id] = [\n",
    "                    req_time for req_time in self.requests[client_id]\n",
    "                    if now - req_time < self.window_seconds\n",
    "                ]\n",
    "                \n",
    "                # Check rate limit\n",
    "                if len(self.requests[client_id]) >= self.max_requests:\n",
    "                    return False\n",
    "                \n",
    "                # Record request\n",
    "                self.requests[client_id].append(now)\n",
    "                return True\n",
    "        \n",
    "        return RateLimiter(max_requests=100, window_seconds=60)\n",
    "    \n",
    "    @staticmethod\n",
    "    async def implement_caching():\n",
    "        \"\"\"\n",
    "        Best Practice 3: Caching for Performance\n",
    "        \n",
    "        Cache frequently accessed resources and tool results.\n",
    "        \"\"\"\n",
    "        from functools import lru_cache\n",
    "        from hashlib import sha256\n",
    "        \n",
    "        class MCPCache:\n",
    "            def __init__(self, ttl_seconds: int = 300):\n",
    "                self.cache = {}\n",
    "                self.ttl = ttl_seconds\n",
    "            \n",
    "            def _get_cache_key(self, tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "                \"\"\"Generate cache key from tool call\"\"\"\n",
    "                key_data = f\"{tool_name}:{json.dumps(arguments, sort_keys=True)}\"\n",
    "                return sha256(key_data.encode()).hexdigest()\n",
    "            \n",
    "            async def get_or_execute(self, tool_name: str, arguments: Dict[str, Any], executor):\n",
    "                \"\"\"Get from cache or execute tool\"\"\"\n",
    "                cache_key = self._get_cache_key(tool_name, arguments)\n",
    "                \n",
    "                # Check cache\n",
    "                if cache_key in self.cache:\n",
    "                    cached_result, timestamp = self.cache[cache_key]\n",
    "                    if time() - timestamp < self.ttl:\n",
    "                        return cached_result\n",
    "                \n",
    "                # Execute and cache\n",
    "                result = await executor(tool_name, arguments)\n",
    "                self.cache[cache_key] = (result, time())\n",
    "                return result\n",
    "        \n",
    "        return MCPCache()\n",
    "    \n",
    "    @staticmethod\n",
    "    def implement_error_handling():\n",
    "        \"\"\"\n",
    "        Best Practice 4: Comprehensive Error Handling\n",
    "        \n",
    "        Handle errors gracefully with proper logging.\n",
    "        \"\"\"\n",
    "        import logging\n",
    "        \n",
    "        class MCPErrorHandler:\n",
    "            def __init__(self):\n",
    "                self.logger = logging.getLogger('mcp_server')\n",
    "            \n",
    "            async def safe_execute(self, func, *args, **kwargs):\n",
    "                \"\"\"Execute function with comprehensive error handling\"\"\"\n",
    "                try:\n",
    "                    return await func(*args, **kwargs)\n",
    "                \n",
    "                except ValueError as e:\n",
    "                    self.logger.warning(f\"Validation error: {e}\")\n",
    "                    return {\n",
    "                        \"error\": {\n",
    "                            \"type\": \"validation_error\",\n",
    "                            \"message\": str(e)\n",
    "                        }\n",
    "                    }\n",
    "                \n",
    "                except TimeoutError as e:\n",
    "                    self.logger.error(f\"Timeout error: {e}\")\n",
    "                    return {\n",
    "                        \"error\": {\n",
    "                            \"type\": \"timeout_error\",\n",
    "                            \"message\": \"Operation timed out\"\n",
    "                        }\n",
    "                    }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    self.logger.exception(f\"Unexpected error: {e}\")\n",
    "                    return {\n",
    "                        \"error\": {\n",
    "                            \"type\": \"internal_error\",\n",
    "                            \"message\": \"An internal error occurred\"\n",
    "                        }\n",
    "                    }\n",
    "        \n",
    "        return MCPErrorHandler()\n",
    "\n",
    "# Demonstrate best practices\n",
    "print(\"MCP Best Practices Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n1. **Security**\")\n",
    "print(\"   - Always validate and sanitize inputs\")\n",
    "print(\"   - Implement authentication and authorization\")\n",
    "print(\"   - Use timeouts for tool execution\")\n",
    "print(\"\\n2. **Performance**\")\n",
    "print(\"   - Implement caching for expensive operations\")\n",
    "print(\"   - Use connection pooling for resources\")\n",
    "print(\"   - Consider async/streaming for large data\")\n",
    "print(\"\\n3. **Reliability**\")\n",
    "print(\"   - Comprehensive error handling\")\n",
    "print(\"   - Graceful degradation\")\n",
    "print(\"   - Proper logging and monitoring\")\n",
    "print(\"\\n4. **Scalability**\")\n",
    "print(\"   - Rate limiting to prevent abuse\")\n",
    "print(\"   - Horizontal scaling support\")\n",
    "print(\"   - Resource pooling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Patterns\n",
    "\n",
    "Here are common patterns for integrating MCP into your applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Factory Pattern for MCP Tools\n",
    "class MCPToolFactory:\n",
    "    \"\"\"\n",
    "    Factory pattern for creating MCP tools dynamically.\n",
    "    \n",
    "    This pattern is useful when you need to:\n",
    "    - Create tools based on configuration\n",
    "    - Support multiple tool providers\n",
    "    - Enable plugin architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tool_creators = {}\n",
    "    \n",
    "    def register_tool_creator(self, tool_type: str, creator):\n",
    "        \"\"\"Register a tool creator function\"\"\"\n",
    "        self.tool_creators[tool_type] = creator\n",
    "    \n",
    "    def create_tool(self, tool_config: Dict[str, Any]) -> MCPTool:\n",
    "        \"\"\"Create a tool based on configuration\"\"\"\n",
    "        tool_type = tool_config.get(\"type\")\n",
    "        if tool_type not in self.tool_creators:\n",
    "            raise ValueError(f\"Unknown tool type: {tool_type}\")\n",
    "        \n",
    "        return self.tool_creators[tool_type](tool_config)\n",
    "\n",
    "# Pattern 2: Adapter Pattern for Different AI Frameworks\n",
    "class MCPFrameworkAdapter:\n",
    "    \"\"\"\n",
    "    Adapter pattern for integrating MCP with different AI frameworks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def to_langchain(self, mcp_tool: MCPTool) -> Tool:\n",
    "        \"\"\"Convert MCP tool to LangChain tool\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def to_openai_function(self, mcp_tool: MCPTool) -> Dict[str, Any]:\n",
    "        \"\"\"Convert MCP tool to OpenAI function calling format\"\"\"\n",
    "        return {\n",
    "            \"name\": mcp_tool.name,\n",
    "            \"description\": mcp_tool.description,\n",
    "            \"parameters\": mcp_tool.input_schema\n",
    "        }\n",
    "    \n",
    "    def to_anthropic_tool(self, mcp_tool: MCPTool) -> Dict[str, Any]:\n",
    "        \"\"\"Convert MCP tool to Anthropic tool format\"\"\"\n",
    "        return {\n",
    "            \"name\": mcp_tool.name,\n",
    "            \"description\": mcp_tool.description,\n",
    "            \"input_schema\": mcp_tool.input_schema\n",
    "        }\n",
    "\n",
    "# Pattern 3: Chain of Responsibility for Request Processing\n",
    "class MCPRequestHandler:\n",
    "    \"\"\"\n",
    "    Chain of responsibility pattern for processing MCP requests.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.next_handler = None\n",
    "    \n",
    "    def set_next(self, handler):\n",
    "        self.next_handler = handler\n",
    "        return handler\n",
    "    \n",
    "    async def handle(self, request: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        if self.can_handle(request):\n",
    "            return await self.process(request)\n",
    "        elif self.next_handler:\n",
    "            return await self.next_handler.handle(request)\n",
    "        return None\n",
    "    \n",
    "    def can_handle(self, request: Dict[str, Any]) -> bool:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    async def process(self, request: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Common MCP Integration Patterns:\")\n",
    "print(\"\\n1. Factory Pattern: Dynamic tool creation\")\n",
    "print(\"2. Adapter Pattern: Framework integration\")\n",
    "print(\"3. Chain of Responsibility: Request processing\")\n",
    "print(\"4. Observer Pattern: Event handling\")\n",
    "print(\"5. Strategy Pattern: Tool execution strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've learned about the Model Context Protocol and how to integrate it with LangChain and LangGraph. Here's what we covered:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **MCP Fundamentals**\n",
    "   - Standardized protocol for AI-tool integration\n",
    "   - Resources, tools, and prompts as core concepts\n",
    "   - JSON-RPC 2.0 communication\n",
    "\n",
    "2. **LangChain Integration**\n",
    "   - Converting MCP tools to LangChain tools\n",
    "   - Using MCP in agents and chains\n",
    "   - Handling stateful interactions\n",
    "\n",
    "3. **LangGraph Advanced Patterns**\n",
    "   - Building complex workflows with MCP\n",
    "   - State management across tool calls\n",
    "   - Multi-phase processing\n",
    "\n",
    "4. **Custom Server Development**\n",
    "   - Implementing your own MCP servers\n",
    "   - Protocol handling\n",
    "   - Best practices for production\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Explore Official MCP Servers**\n",
    "   - File system server\n",
    "   - Database connectors\n",
    "   - API integrations\n",
    "\n",
    "2. **Build Your Own Tools**\n",
    "   - Start with simple tools\n",
    "   - Add to existing applications\n",
    "   - Share with the community\n",
    "\n",
    "3. **Advanced Topics**\n",
    "   - Streaming and real-time updates\n",
    "   - Multi-server orchestration\n",
    "   - Security and authentication\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Official MCP Documentation: https://docs.anthropic.com/en/docs/mcp\n",
    "- Python SDK: https://github.com/modelcontextprotocol/python-sdk\n",
    "- LangChain MCP Adapters: https://github.com/langchain-ai/langchain-mcp-adapters\n",
    "- Community Examples: https://github.com/modelcontextprotocol/examples\n",
    "\n",
    "Happy building with MCP! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
