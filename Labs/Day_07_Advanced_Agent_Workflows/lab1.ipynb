{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7 - Lab 1: Agent Interoperability with the Model Context Protocol (MCP)\n\n",
    "**Objective:** Build a LangChain agent that can use tools exposed by a standard MCP server, demonstrating how MCP provides a universal adapter for AI tool integration.\n\n",
    "**Introduction:**\n",
    "For AI agents to be truly useful, they need to interact with the outside world: files, databases, and APIs. But how can an agent work with hundreds of different tools without custom code for each one? The Model Context Protocol (MCP) is an open standard designed to solve this problem. It acts as a universal adapter.\n\n",
    "In this lab, you will connect to a (mock) MCP server that provides tools for interacting with a filesystem. You will then use the `langchain-mcp` adapter library to seamlessly integrate these external tools into a LangChain agent."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Setup\n\n",
    "We'll install the necessary libraries and set up our LangChain LLM client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mcp langchain-mcp langchain_openai\n",
    "import sys, os, json, datetime\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n\n",
    "from utils import setup_llm_client\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mcp import create_mcp_tools\n",
    "from langchain_core.tools import Tool\n\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "llm = ChatOpenAI(model=model_name, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Simulating an MCP Server\n\n",
    "In a real-world scenario, an MCP server would be a separate, long-running process. To keep this lab self-contained, we will simulate one. This `MockMCPSession` class mimics a connection to an MCP server that offers two tools: `read_file` and `get_system_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockMCPSession:\n",
    "    \"\"\"A mock MCP client session to simulate a connection to an MCP server.\"\"\"\n",
    "    async def list_tools(self):\n",
    "        print(\"--> Mock MCP Server: list_tools() called\")\n",
    "        return [\n",
    "            {\n",
    "                'name': 'read_file',\n",
    "                'description': 'Reads the content of a specified file.',\n",
    "                'inputSchema': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'path': {'type': 'string', 'description': 'The path to the file.'}\n",
    "                    },\n",
    "                    'required': ['path']\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'get_system_time',\n",
    "                'description': 'Returns the current system time.',\n",
    "                'inputSchema': {'type': 'object', 'properties': {}}\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    async def call_tool(self, name, arguments):\n",
    "        print(f\"--> Mock MCP Server: call_tool('{name}') called with {arguments}\")\n",
    "        if name == 'read_file':\n",
    "            # For safety, we'll only allow reading the README.md\n",
    "            if arguments.get('path') == 'README.md':\n",
    "                try:\n",
    "                    with open(os.path.join(project_root, 'README.md'), 'r') as f:\n",
    "                        return f.read()\n",
    "                except FileNotFoundError:\n",
    "                    return 'Error: README.md not found.'\n",
    "            else:\n",
    "                return f\"Error: Access denied to file: {arguments.get('path')}\"\n",
    "        elif name == 'get_system_time':\n",
    "            return datetime.datetime.now().isoformat()\n",
    "        else:\n",
    "            return 'Error: Tool not found'\n\n",
    "mock_session = MockMCPSession()\n",
    "print(\"Mock MCP Server Session created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Challenge 1 (Foundational): Discover and List MCP Tools\n\n",
    "**Task:** Use the `mock_session` to discover the tools offered by the MCP server and print their details.\n\n",
    "**Instructions:**\n",
    "1. Call the `list_tools()` method on the `mock_session` object.\n",
    "2. Loop through the results and print the name and description of each available tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import asyncio\n\n",
    "async def list_mcp_tools():\n",
    "    print(\"--- Discovering MCP Tools ---\")\n",
    "    # TODO: Call the list_tools method on the mock_session\n",
    "    available_tools = [] # Your code here\n\n",
    "    # TODO: Loop through available_tools and print the name and description of each tool\n",
    "    # Your code here\n\n",
    "# In a notebook, we can run async code like this:\n",
    "await list_mcp_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Challenge 2 (Intermediate): Convert MCP Tools to LangChain Tools\n\n",
    "**Task:** Use the `create_mcp_tools` function from the `langchain-mcp` library to automatically convert the MCP tool definitions into a list of LangChain `Tool` objects.\n\n",
    "**Instructions:**\n",
    "1. Call the `create_mcp_tools` function, passing it our `mock_session`.\n",
    "2. This function is asynchronous, so you will need to `await` its result.\n",
    "3. Store the result in a variable called `langchain_tools`.\n",
    "4. Print the `langchain_tools` list to see the LangChain-compatible tool objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def convert_tools():\n",
    "    print(\"\\n--- Converting MCP tools to LangChain Tools ---\")\n",
    "    # TODO: Call create_mcp_tools with the mock_session\n",
    "    langchain_tools = [] # Your code here\n\n",
    "    print(f\"Successfully converted {len(langchain_tools)} tools.\")\n",
    "    print(langchain_tools)\n",
    "    return langchain_tools\n\n",
    "langchain_tools = await convert_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Challenge 3 (Advanced): Build and Use an Agent with MCP Tools\n\n",
    "**Task:** Now that you have a list of standard LangChain tools, build a LangChain agent that can use them to answer questions.\n\n",
    "**Instructions:**\n",
    "1. Create a `ChatPromptTemplate` for your agent. It must include a placeholder for `{agent_scratchpad}`.\n",
    "2. Use `create_tool_calling_agent` to create the agent, passing the `llm`, the `langchain_tools` from the previous step, and your prompt.\n",
    "3. Create an `AgentExecutor` to run the agent.\n",
    "4. Invoke the agent with a question that requires using one of the MCP tools, like `\"What is the current system time?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: 1. Define the prompt template for the agent\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant with access to system tools.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    # Your placeholder here\n",
    "])\n\n",
    "# TODO: 2. Create the tool-calling agent\n",
    "agent = None # Your code here\n\n",
    "# TODO: 3. Create the AgentExecutor\n",
    "agent_executor = None # Your code here\n\n",
    "print(\"\\n--- Invoking agent to get system time ---\")\n",
    "# TODO: 4. Invoke the agent with a question about the time\n",
    "time_result = None # Your code here\n",
    "print(f\"Final Answer: {time_result['output']}\")\n\n",
    "print(\"\\n--- Invoking agent to read a file ---\")\n",
    "# Bonus: Invoke the agent again to read the README.md file\n",
    "file_result = agent_executor.invoke({\"input\": \"What is the content of the README.md file?\"})\n",
    "print(f\"Final Answer: {file_result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lab Conclusion\n\n",
    "Congratulations! You have successfully used the Model Context Protocol to integrate external tools into a LangChain agent. You saw how MCP provides a standard way for a server to describe its tools, and how the `langchain-mcp` library can automatically discover and adapt these tools for your agent. This is a key architectural pattern for building complex, interoperable multi-agent systems that can securely interact with a wide range of enterprise tools."
   ]
  }
 ]
}