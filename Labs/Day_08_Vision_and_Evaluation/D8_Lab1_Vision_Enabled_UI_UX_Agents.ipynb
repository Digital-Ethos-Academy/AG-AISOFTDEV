{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8 - Lab 1: Vision-Enabled UI/UX Agents\n",
    "\n",
    "**Objective:** Use multi-modal vision models to generate a frontend UI from a design image, and then use a second agent to perform an automated design review.\n",
    "\n",
    "**Estimated Time:** 90 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 8! Today, we'll explore one of the most exciting advancements in AI: vision. We will use a vision-capable model to act as a frontend developer, translating a design image directly into code. Then, we will create a second \"UI/UX Critic\" agent to automate the design review process, demonstrating a complete, AI-assisted frontend workflow.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "For this lab, we need to ensure we are using a vision-capable model. We will configure our `utils.py` helper to use a model like OpenAI's `gpt-4o` or Google's `gemini-2.5-pro`.\n",
    "\n",
    "**Model Selection:**\n",
    "This lab requires a vision-capable model. Excellent choices include `gpt-4o`, `gemini-2.5-pro`, or `deepseek-ai/DeepSeek-VL2`.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_vision_completion()`: A specialized function to send an image and a text prompt to a vision model.\n",
    "- `get_completion()`: To send text-only prompts for the refactoring step.\n",
    "- `save_artifact()`: To save the generated code and the design review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Client configured: Using 'google' with model 'imagen-3.0-generate-002'\n",
      "‚úÖ Using imagen-3.0-generate-002 for image generation\n",
      "‚úÖ LLM Client configured: Using 'openai' with model 'gpt-4o'\n",
      "‚úÖ Using gpt-4o for vision tasks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_vision_completion, get_image_generation_completion, get_completion, save_artifact, clean_llm_output, recommended_models_table\n",
    "from IPython.display import Image, display, Code\n",
    "\n",
    "image_client, image_model_name, image_api_provider = setup_llm_client(model_name=\"imagen-3.0-generate-002\")\n",
    "print(f\"‚úÖ Using {image_model_name} for image generation\")\n",
    "\n",
    "# Ensure you select a vision-capable model\n",
    "vision_client, vision_model_name, vision_api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "print(f\"‚úÖ Using {vision_model_name} for vision tasks\")\n",
    "\n",
    "\n",
    "if not image_model_name or not vision_model_name:\n",
    "    print(\"Could not set up a valid LLM client. Please check your .env file and utils.py configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Design Screenshot\n",
    "\n",
    "This is the design we want our AI agent to build. It's a simple login form component. We will provide the URL to this image directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using imagen-3.0-generate-002 for image generation...\n",
      "Generating image... This may take a moment.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "‚è≥ Generating image..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Image generation failed: Google image generation failed: 404 models/imagen-3.0-generate-002 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "üí° Tip: Try using 'dall-e-3' model for reliable image generation\n"
     ]
    }
   ],
   "source": [
    "screen_generation_prompt = \"an onboarding tool homepage screen with modern UI design, clean layout, welcome message, and navigation elements\"\n",
    "\n",
    "print(f\"Using {image_model_name} for image generation...\")\n",
    "file_path, image_url = get_image_generation_completion(screen_generation_prompt, image_client, image_model_name, image_api_provider)\n",
    "\n",
    "# Display the generated image if successful\n",
    "if file_path and image_url:\n",
    "    print(f\"‚úÖ Generated image saved to: {file_path}\")\n",
    "    display(Image(url=image_url))\n",
    "elif image_url:\n",
    "    print(f\"‚ùå Image generation failed: {image_url}\")\n",
    "else:\n",
    "    print(\"‚ùå Image generation returned no result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_form_url = \"https://i.imgur.com/s42SYz6.png\"\n",
    "display(Image(url=login_form_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a Monolithic UI Component\n",
    "\n",
    "**Task:** Use the vision model to generate a single, self-contained React component that replicates the design from the image.\n",
    "\n",
    "> **Tip for Vision Prompts:** Be specific about the output format. Telling the model you want 'React' and 'Tailwind CSS' is crucial. The more specific your technical constraints, the better the generated code will be.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the vision model to act as an expert frontend developer.\n",
    "2.  The prompt should instruct the model to analyze the image at the provided URL.\n",
    "3.  Ask it to generate a single React component using Tailwind CSS for styling.\n",
    "4.  The output should be a single block of JSX code.\n",
    "\n",
    "**Expected Quality:** A single file's worth of React code that, when rendered, visually approximates the login form in the screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the specified image using the vision-capable LLM\n",
    "image_path = \"/Users/agaleana/repos/AG-AISOFTDEV/artifacts/screens/image_1756391135.png\"\n",
    "\n",
    "explain_image_prompt = f\"\"\"\n",
    "You are an expert frontend developer and UI/UX designer. Analyze the provided image and produce a clear, actionable explanation for a developer who will implement the UI.\n",
    "\n",
    "Please respond in markdown and include the following sections:\n",
    "\n",
    "1) Summary: One-sentence summary of what this screen is (purpose).\n",
    "2) Visual Breakdown: Enumerate the visible UI elements (e.g., header, logo, form fields, buttons, icons, images) and approximate positions (top/left/center, grouping).\n",
    "3) Style Details: Describe colors, font styles (weight/size/line-height if evident), spacing, borders/radii, shadows, and alignment cues.\n",
    "4) Interaction & Behavior: Identify interactive elements and the likely behaviors (hover/focus states, validation, keyboard affordances).\n",
    "5) Accessibility Notes: Potential accessibility issues and recommended fixes (labels, contrast, focus order).\n",
    "6) Implementation Plan (React + Tailwind): Suggest component names and a short mapping of each component to Tailwind utilities or structure (e.g., <LoginForm> -> container: flex, gap-4, ...). Keep this as a concise checklist.\n",
    "\n",
    "Keep the output developer-focused and actionable.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Requesting image explanation from vision model ---\")\n",
    "if vision_model_name:\n",
    "    explanation = get_vision_completion(explain_image_prompt, image_path, vision_client, vision_model_name, vision_api_provider)\n",
    "    cleaned_explanation = clean_llm_output(explanation, language=\"markdown\")\n",
    "    display(Code(cleaned_explanation, language=\"markdown\"))\n",
    "    # Save the explanation for later reference\n",
    "    save_artifact(cleaned_explanation, \"artifacts/image_explanation.md\")\n",
    "else:\n",
    "    print(\"Skipping image explanation because no vision model is configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate a single React component from the image.\n",
    "generate_ui_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Monolithic UI Component ---\")\n",
    "if vision_model_name:\n",
    "    generated_monolithic_code = get_vision_completion(generate_ui_prompt, login_form_url, vision_client, vision_model_name, vision_api_provider)\n",
    "    cleaned_code = clean_llm_output(generated_monolithic_code, language='jsx')\n",
    "    display(Code(cleaned_code, language='jsx'))\n",
    "else:\n",
    "    print(\"Skipping UI generation because no valid model is configured.\")\n",
    "    cleaned_code = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Refactoring into Reusable Components\n",
    "\n",
    "**Task:** A single, large component is not good practice. Now, prompt the LLM to refactor the monolithic code it just generated into smaller, reusable sub-components.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the monolithic JSX code from the previous step as context.\n",
    "3.  Instruct the LLM to act as a senior frontend developer who champions clean code.\n",
    "4.  Ask it to refactor the code by creating smaller, reusable components (e.g., `<StyledButton>`, `<InputWithIcon>`).\n",
    "5.  The final output should be the complete code with the new, smaller components defined and used within the main `Login` component.\n",
    "\n",
    "**Expected Quality:** A well-structured React file that demonstrates the component-based architecture, which is a fundamental best practice in modern frontend development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to refactor the monolithic code into smaller components.\n",
    "refactor_ui_prompt = f\"\"\"\n",
    "# Your prompt here. Remember to provide the code generated in the previous step as context.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Refactoring UI into Components ---\")\n",
    "if cleaned_code:\n",
    "    refactored_code = get_completion(refactor_ui_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_code = clean_llm_output(refactored_code, language='jsx')\n",
    "    display(Code(cleaned_refactored_code, language='jsx'))\n",
    "else:\n",
    "    print(\"Skipping refactoring because monolithic code was not generated.\")\n",
    "    cleaned_refactored_code = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): The AI UI/UX Critic Agent\n",
    "\n",
    "**Task:** Create a new \"UI/UX Critic\" agent. This agent will be given both the original design image and the generated code, and its job is to perform an automated design review.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a final, complex prompt for a new agent.\n",
    "2.  The prompt should instruct the agent to act as a meticulous UI/UX designer.\n",
    "3.  Provide the agent with two pieces of context: the URL of the original design image and the final, refactored React code.\n",
    "4.  The agent's task is to compare the code's likely rendered output to the design image and list any visual inconsistencies in spacing, font size, color, or layout.\n",
    "\n",
    "**Expected Quality:** A critical design review in markdown format. This demonstrates a powerful AI-on-AI workflow, where one AI generates work and another AI validates it, automating a time-consuming QA step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt for the UI/UX Critic agent.\n",
    "critic_agent_prompt = f\"\"\"\n",
    "# Your prompt here. Provide both the image URL and the refactored code as context.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Invoking UI/UX Critic Agent ---\")\n",
    "if cleaned_refactored_code:\n",
    "    design_review = get_vision_completion(critic_agent_prompt, login_form_url, client, model_name, api_provider)\n",
    "    display(Code(design_review, language='markdown'))\n",
    "    save_artifact(design_review, \"artifacts/design_review.md\")\n",
    "else:\n",
    "    print(\"Skipping critic agent because refactored code is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic! You have completed a full, end-to-end frontend development workflow using multiple AI agents. You used a vision-powered agent to generate code from a design, a refactoring agent to improve the code's structure, and a critic agent to perform an automated design review. This powerful combination of skills can dramatically accelerate the process of turning visual ideas into functional user interfaces.\n",
    "\n",
    "> **Key Takeaway:** The workflow of **Generate -> Refactor -> Critique** is a powerful AI-assisted development pattern. Using specialized agents for each step allows you to rapidly create a first draft, improve its quality, and then automatically check it for correctness, significantly speeding up the iteration cycle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
