{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8 - Lab 1: Vision-Enabled UI/UX Agents\n",
    "\n",
    "**Objective:** Use multi-modal vision models to generate a frontend UI from a design image, and then use a second agent to perform an automated design review.\n",
    "\n",
    "**Estimated Time:** 90 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 8! Today, we'll explore one of the most exciting advancements in AI: vision. We will use a vision-capable model to act as a frontend developer, translating a design image directly into code. Then, we will create a second \"UI/UX Critic\" agent to automate the design review process, demonstrating a complete, AI-assisted frontend workflow.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "For this lab, we need to ensure we are using a vision-capable model. We will configure our `utils.py` helper to use a model like OpenAI's `gpt-4o` or Google's `gemini-2.5-pro`.\n",
    "\n",
    "**Model Selection:**\n",
    "This lab requires a vision-capable model. Excellent choices include `gpt-4o`, `gemini-2.5-pro`, or `deepseek-ai/DeepSeek-VL2`.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_vision_completion()`: A specialized function to send an image and a text prompt to a vision model.\n",
    "- `get_completion()`: To send text-only prompts for the refactoring step.\n",
    "- `save_artifact()`: To save the generated code and the design review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_vision_completion, get_completion, save_artifact, clean_llm_output\n",
    "from IPython.display import Image, display, Code\n",
    "\n",
    "# Ensure you select a vision-capable model\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "if not model_name:\n",
    "    print(\"Could not set up a valid LLM client. Please check your .env file and utils.py configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Design Screenshot\n",
    "\n",
    "This is the design we want our AI agent to build. It's a simple login form component. We will provide the URL to this image directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_form_url = \"https://i.imgur.com/s42SYz6.png\"\n",
    "display(Image(url=login_form_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a Monolithic UI Component\n",
    "\n",
    "**Task:** Use the vision model to generate a single, self-contained React component that replicates the design from the image.\n",
    "\n",
    "> **Tip for Vision Prompts:** Be specific about the output format. Telling the model you want 'React' and 'Tailwind CSS' is crucial. The more specific your technical constraints, the better the generated code will be.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the vision model to act as an expert frontend developer.\n",
    "2.  The prompt should instruct the model to analyze the image at the provided URL.\n",
    "3.  Ask it to generate a single React component using Tailwind CSS for styling.\n",
    "4.  The output should be a single block of JSX code.\n",
    "\n",
    "**Expected Quality:** A single file's worth of React code that, when rendered, visually approximates the login form in the screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate a single React component from the image.\n",
    "generate_ui_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Monolithic UI Component ---\")\n",
    "if model_name:\n",
    "    generated_monolithic_code = get_vision_completion(generate_ui_prompt, login_form_url, client, model_name, api_provider)\n",
    "    cleaned_code = clean_llm_output(generated_monolithic_code, language='jsx')\n",
    "    display(Code(cleaned_code, language='jsx'))\n",
    "else:\n",
    "    print(\"Skipping UI generation because no valid model is configured.\")\n",
    "    cleaned_code = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Refactoring into Reusable Components\n",
    "\n",
    "**Task:** A single, large component is not good practice. Now, prompt the LLM to refactor the monolithic code it just generated into smaller, reusable sub-components.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the monolithic JSX code from the previous step as context.\n",
    "3.  Instruct the LLM to act as a senior frontend developer who champions clean code.\n",
    "4.  Ask it to refactor the code by creating smaller, reusable components (e.g., `<StyledButton>`, `<InputWithIcon>`).\n",
    "5.  The final output should be the complete code with the new, smaller components defined and used within the main `Login` component.\n",
    "\n",
    "**Expected Quality:** A well-structured React file that demonstrates the component-based architecture, which is a fundamental best practice in modern frontend development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to refactor the monolithic code into smaller components.\n",
    "refactor_ui_prompt = f\"\"\"\n",
    "# Your prompt here. Remember to provide the code generated in the previous step as context.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Refactoring UI into Components ---\")\n",
    "if cleaned_code:\n",
    "    refactored_code = get_completion(refactor_ui_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_code = clean_llm_output(refactored_code, language='jsx')\n",
    "    display(Code(cleaned_refactored_code, language='jsx'))\n",
    "else:\n",
    "    print(\"Skipping refactoring because monolithic code was not generated.\")\n",
    "    cleaned_refactored_code = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): The AI UI/UX Critic Agent\n",
    "\n",
    "**Task:** Create a new \"UI/UX Critic\" agent. This agent will be given both the original design image and the generated code, and its job is to perform an automated design review.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a final, complex prompt for a new agent.\n",
    "2.  The prompt should instruct the agent to act as a meticulous UI/UX designer.\n",
    "3.  Provide the agent with two pieces of context: the URL of the original design image and the final, refactored React code.\n",
    "4.  The agent's task is to compare the code's likely rendered output to the design image and list any visual inconsistencies in spacing, font size, color, or layout.\n",
    "\n",
    "**Expected Quality:** A critical design review in markdown format. This demonstrates a powerful AI-on-AI workflow, where one AI generates work and another AI validates it, automating a time-consuming QA step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt for the UI/UX Critic agent.\n",
    "critic_agent_prompt = f\"\"\"\n",
    "# Your prompt here. Provide both the image URL and the refactored code as context.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Invoking UI/UX Critic Agent ---\")\n",
    "if cleaned_refactored_code:\n",
    "    design_review = get_vision_completion(critic_agent_prompt, login_form_url, client, model_name, api_provider)\n",
    "    display(Code(design_review, language='markdown'))\n",
    "    save_artifact(design_review, \"artifacts/design_review.md\")\n",
    "else:\n",
    "    print(\"Skipping critic agent because refactored code is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic! You have completed a full, end-to-end frontend development workflow using multiple AI agents. You used a vision-powered agent to generate code from a design, a refactoring agent to improve the code's structure, and a critic agent to perform an automated design review. This powerful combination of skills can dramatically accelerate the process of turning visual ideas into functional user interfaces.\n",
    "\n",
    "> **Key Takeaway:** The workflow of **Generate -> Refactor -> Critique** is a powerful AI-assisted development pattern. Using specialized agents for each step allows you to rapidly create a first draft, improve its quality, and then automatically check it for correctness, significantly speeding up the iteration cycle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}