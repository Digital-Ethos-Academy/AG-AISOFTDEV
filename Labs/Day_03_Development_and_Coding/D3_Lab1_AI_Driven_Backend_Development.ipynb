{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Lab 1: AI-Driven Backend Development\n",
    "\n",
    "**Objective:** Generate a complete FastAPI backend application, including Pydantic and SQLAlchemy models, and then perform the critical engineering task of integrating the generated code with the live SQLite database created on Day 2.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 3! With our requirements and architecture defined, it's time to write code. In this lab, you will act as a senior developer guiding an AI co-pilot. Your task is to generate the full backend API for the Onboarding Tool. This involves not just generating code, but also connecting it to the live database we created yesterday, moving from a prototype to a functional, data-driven application.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We'll set up our environment and load the `schema.sql` artifact from Day 2. This SQL file contains the `CREATE TABLE` statements that define our database structure, which is the perfect context to provide the LLM for code generation.\n",
    "\n",
    "**Model Selection:**\n",
    "For code generation, models specifically fine-tuned for coding are ideal. `gpt-4.1`, `o3`, or `codex-mini` are excellent choices. Experiment to see which one gives you the cleanest code.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read the SQL schema.\n",
    "- `save_artifact()`: To save the generated Python code.\n",
    "- `clean_llm_output()`: To remove markdown fences from the generated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the SQL schema from Day 2\n",
    "sql_schema = load_artifact(\"artifacts/schema.sql\")\n",
    "if not sql_schema:\n",
    "    print(\"Warning: Could not load schema.sql. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges\n",
    "\n",
    "Follow the challenges below to build and connect your API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating Code with In-Memory Logic\n",
    "\n",
    "**Task:** Generate all the necessary Python code for a FastAPI application, but with simple in-memory data storage for now. This allows us to generate and validate the code's structure before adding database complexity.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a detailed prompt that asks the LLM to act as a senior Python developer.\n",
    "2.  Provide the `sql_schema` as context.\n",
    "3.  Instruct the LLM to generate three key components:\n",
    "    * **Pydantic Models:** For API data validation (request/response bodies).\n",
    "    * **FastAPI Endpoints:** Full CRUD (Create, Read, Update, Delete) endpoints for the `users` table.\n",
    "    * **In-Memory Database:** A simple Python list to act as a temporary, fake database.\n",
    "4.  The final output should be a single Python script for a `main_in_memory.py` file.\n",
    "5.  Save the generated code to `app/main_in_memory.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate a complete FastAPI application with in-memory data storage.\n",
    "in_memory_api_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating FastAPI app with in-memory database ---\")\n",
    "if sql_schema:\n",
    "    generated_api_code = get_completion(in_memory_api_prompt, client, model_name, api_provider)\n",
    "    cleaned_code = clean_llm_output(generated_api_code, language='python')\n",
    "    print(cleaned_code)\n",
    "    save_artifact(cleaned_code, \"app/main_in_memory.py\")\n",
    "else:\n",
    "    print(\"Skipping API generation because schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Database Models and Session Code\n",
    "\n",
    "**Task:** Now, generate the specific SQLAlchemy code required to connect our application to the live `onboarding.db` SQLite database.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `sql_schema` as context again.\n",
    "3.  Instruct the LLM to generate two separate pieces of code:\n",
    "    * **SQLAlchemy Models:** Python classes that map to your database tables.\n",
    "    * **Database Session Management:** The boilerplate code to create a database engine, session maker, and a dependency function (`get_db`) for use in FastAPI.\n",
    "4.  The output should be two distinct, well-commented Python code blocks. We will integrate these manually in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate SQLAlchemy models and the database session/dependency code.\n",
    "db_code_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQLAlchemy Models and Session Code ---\")\n",
    "if sql_schema:\n",
    "    generated_db_code = get_completion(db_code_prompt, client, model_name, api_provider)\n",
    "    print(\"\\n--- Generated Database Code ---\")\n",
    "    print(generated_db_code)\n",
    "else:\n",
    "    print(\"Skipping DB code generation because schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Integrating Live Database Logic\n",
    "\n",
    "**Task:** This is the most critical engineering step of the lab. You will manually integrate the generated database code into the FastAPI application, replacing the in-memory logic with live database operations.\n",
    "\n",
    "**Instructions:**\n",
    "This task represents a significant jump in complexity. Follow these steps carefully in your IDE (like VS Code):\n",
    "\n",
    "1.  Create a new, empty file named `app/main.py`.\n",
    "2.  **First, copy the Pydantic models and the `app = FastAPI()` line** from your `app/main_in_memory.py` file and paste them into `app/main.py`.\n",
    "3.  **Next, paste the SQLAlchemy model classes and the `get_db` dependency function** you generated in Challenge 2 into your new `app/main.py`.\n",
    "4.  **Now, let's refactor the `POST /users/` endpoint.** Copy the endpoint function from the in-memory file, but replace the in-memory logic (e.g., `db.append()`) with the correct SQLAlchemy session calls: `db.add(db_user)`, `db.commit()`, and `db.refresh(db_user)`.\n",
    "5.  Repeat this refactoring process for the other endpoints (GET, PUT, DELETE), replacing list manipulations with the appropriate SQLAlchemy `db.query()` methods.\n",
    "\n",
    "This task requires you to act as the senior developer, stitching together the AI-generated components into a functional, cohesive whole. You may need to ask the LLM follow-up questions like, \"How do I write a SQLAlchemy query to find a user by ID?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Congratulations! You have successfully generated and assembled a complete, database-connected backend API. You used an LLM to generate the boilerplate for both the API endpoints and the database models, and then performed the crucial engineering task of integrating them. You now have a working `main.py` file in your `app` directory that can create, read, update, and delete data in a live database. In the next lab, we will write a comprehensive test suite for this API.\n",
    "\n",
    "> **Key Takeaway:** AI excels at generating boilerplate code (like models and endpoint structures), but the developer's critical role is in the final integration and wiring of these components into a coherent, working system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
