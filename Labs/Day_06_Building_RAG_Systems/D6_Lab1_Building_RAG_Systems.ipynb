{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Estimated Time:** 180 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 6! Today, we build one of the most powerful and common patterns for enterprise AI: a system that can answer questions about your private documents. We will use LangGraph to create a 'research team' of AI agents. Each agent will have a specific job, and LangGraph will act as the manager, orchestrating their collaboration to find the best possible answer.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We need several libraries for this lab. `langgraph` is the core orchestrator, `langchain` provides the building blocks, `faiss-cpu` is for our vector store, and `pypdf` is for loading documents.\n",
    "\n",
    "**Model Selection:**\n",
    "For RAG and agentic workflows, models with strong instruction-following and reasoning are best. `gpt-4.1`, `o3`, or `gemini-2.5-pro` are excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `load_artifact()`: To read the project documents that will form our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:49:01.611156Z",
     "start_time": "2025-11-04T19:48:58.241164Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu not found, installing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 14:49:01,609 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "An agent is only as smart as the information it can access. We will create a vector store containing all the project artifacts we've created so far. This will be our agent's 'knowledge base'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:49:54.467535Z",
     "start_time": "2025-11-04T19:49:49.661991Z"
    }
   },
   "source": "from langchain_community.vectorstores import FAISS\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\n\n# Install sentence-transformers if not available\nimport subprocess\ntry:\n    import sentence_transformers\nexcept ImportError:\n    print(\"Installing sentence-transformers...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sentence-transformers\"])\n\n# Use HuggingFace embeddings - local and reliable\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Initialize the local embeddings model\nprint(\"Loading embeddings model (this may take a moment on first run)...\")\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nprint(\"‚úÖ Embeddings model loaded\")\n\ndef create_knowledge_base(file_paths):\n    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n    all_docs = []\n    for path in file_paths:\n        full_path = os.path.join(project_root, path)\n        if os.path.exists(full_path):\n            loader = TextLoader(full_path)\n            docs = loader.load()\n            for doc in docs:\n                doc.metadata={\"source\": path} # Add source metadata\n            all_docs.extend(docs)\n        else:\n            print(f\"Warning: Artifact not found at {full_path}\")\n\n    if not all_docs:\n        print(\"No documents found to create knowledge base.\")\n        return None\n\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    splits = text_splitter.split_documents(all_docs)\n    \n    print(f\"Creating vector store from {len(splits)} document splits...\")\n    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n    return vectorstore.as_retriever()\n\nall_artifact_paths = [\"docs/prd/day1_prd.md\", \"database/schema.sql\", \"docs/adr/adr_001_database_choice.md\"]\nretriever = create_knowledge_base(all_artifact_paths)\nprint(\"‚úÖ Knowledge base created successfully!\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings model (this may take a moment on first run)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/2x6d6cb14xn9pwtdmrdcpsdh0000gp/T/ipykernel_85176/2985344843.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings model loaded\n",
      "Creating vector store from 43 document splits...\n",
      "‚úÖ Knowledge base created successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Task:** Build a simple LangGraph with two nodes: one to retrieve documents and one to generate an answer.\n",
    "\n",
    "> **Tip:** Think of `AgentState` as the shared 'whiteboard' for your agent team. Every agent (or 'node' in the graph) can read from and write to this state, allowing them to pass information to each other as they work on a problem.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Define the state for your graph using a `TypedDict`. It should contain keys for `question` and `documents`.\n",
    "2.  Create a \"Retriever\" node. This is a Python function that takes the state, uses the `retriever` to get relevant documents, and updates the state with the results.\n",
    "3.  Create a \"Generator\" node. This function takes the state, creates a prompt with the question and retrieved documents, calls the LLM, and stores the answer.\n",
    "4.  Build the `StateGraph`, add the nodes, and define the edges (`RETRIEVE` -> `GENERATE`).\n",
    "5.  Compile the graph and invoke it with a question about your project.\n",
    "\n",
    "**Expected Quality:** A functional graph that can answer a simple question (e.g., \"What is the purpose of this project?\") by retrieving context from the project artifacts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:50:13.783303Z",
     "start_time": "2025-11-04T19:50:02.157624Z"
    }
   },
   "source": "# Challenge 1: Simple RAG Graph with LangGraph\n\n# First, let's install and import required packages\nimport sys\nimport subprocess\n\ndef install_if_missing(package):\n    try:\n        __import__(package)\n    except ImportError:\n        print(f\"{package} not found, installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n\n# Install langchain-google-genai if using Google models\nif api_provider == \"google\":\n    install_if_missing('langchain-google-genai')\n\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, END\n\nprint(f\"üîß Setting up RAG system with {api_provider} - {model_name}\")\n\n# Step 1: Verify the retriever exists and works\nif retriever is None:\n    print(\"‚ùå ERROR: Retriever is not initialized. Please run Step 2 first!\")\n    raise ValueError(\"Retriever not found\")\n\nprint(\"‚úÖ Retriever verified\")\n\n# Step 2: Define the state that will be passed between nodes\n# Think of this as the \"shared memory\" for all nodes in the graph\nclass AgentState(TypedDict):\n    question: str           # The user's question\n    documents: list         # Retrieved documents will go here\n    answer: str            # Final answer will go here\n\n# Step 3: Initialize the LLM based on the provider\nprint(f\"ü§ñ Initializing LLM...\")\ntry:\n    if api_provider == \"google\":\n        from langchain_google_genai import ChatGoogleGenerativeAI\n        llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)\n    elif api_provider == \"openai\":\n        from langchain_openai import ChatOpenAI\n        llm = ChatOpenAI(model=model_name, temperature=0)\n    else:\n        # For other providers, try ChatOpenAI as fallback\n        from langchain_openai import ChatOpenAI\n        llm = ChatOpenAI(model=model_name, temperature=0)\n    print(f\"‚úÖ LLM initialized successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Error initializing LLM: {e}\")\n    raise\n\n# Step 4: Create the Retriever Node\ndef retrieve_node(state: AgentState) -> AgentState:\n    \"\"\"Retrieve relevant documents based on the question.\"\"\"\n    try:\n        question = state[\"question\"]\n        print(f\"\\nüîç RETRIEVE NODE: Searching for documents related to: '{question}'\")\n        \n        # Use the retriever we created earlier to find relevant documents\n        docs = retriever.invoke(question)\n        \n        print(f\"   Found {len(docs)} relevant documents\")\n        \n        # Return the updated state with the retrieved documents\n        return {\"documents\": docs}\n    except Exception as e:\n        print(f\"‚ùå Error in retrieve_node: {e}\")\n        return {\"documents\": []}\n\n# Step 5: Create the Generator Node\ndef generate_node(state: AgentState) -> AgentState:\n    \"\"\"Generate an answer based on the question and retrieved documents.\"\"\"\n    try:\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n        \n        if not documents:\n            return {\"answer\": \"I couldn't find any relevant documents to answer this question.\"}\n        \n        print(f\"\\n‚úçÔ∏è  GENERATE NODE: Creating answer using {len(documents)} documents\")\n        \n        # Format the documents into a context string\n        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n        \n        # Create a prompt that includes both the context and the question\n        prompt = f\"\"\"Based on the following context, please answer the question.\n        \nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n        \n        # Call the LLM to generate the answer\n        response = llm.invoke(prompt)\n        answer = response.content\n        \n        print(f\"   Generated answer (preview): {answer[:100]}...\")\n        \n        # Return the updated state with the answer\n        return {\"answer\": answer}\n    except Exception as e:\n        print(f\"‚ùå Error in generate_node: {e}\")\n        return {\"answer\": f\"Error generating answer: {str(e)}\"}\n\n# Step 6: Build the StateGraph\nprint(\"\\nüèóÔ∏è  Building the RAG graph...\")\n\n# Create a new StateGraph with our AgentState\nworkflow = StateGraph(AgentState)\n\n# Add our two nodes to the graph\nworkflow.add_node(\"retrieve\", retrieve_node)\nworkflow.add_node(\"generate\", generate_node)\n\n# Define the flow: Start -> Retrieve -> Generate -> End\nworkflow.set_entry_point(\"retrieve\")  # Start with the retrieve node\nworkflow.add_edge(\"retrieve\", \"generate\")  # After retrieve, go to generate\nworkflow.add_edge(\"generate\", END)  # After generate, we're done\n\n# Step 7: Compile the graph\napp = workflow.compile()\n\nprint(\"‚úÖ Graph compiled successfully!\")\n\n# Step 8: Test the graph with a question\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING THE RAG SYSTEM\")\nprint(\"=\"*60)\n\ntest_question = \"What is the purpose of this project?\"\n\ntry:\n    result = app.invoke({\n        \"question\": test_question,\n        \"documents\": [],\n        \"answer\": \"\"\n    })\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL RESULT\")\n    print(\"=\"*60)\n    print(f\"\\nQuestion: {test_question}\")\n    print(f\"\\nAnswer: {result['answer']}\")\n    print(f\"\\nDocuments used: {len(result['documents'])}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error running the graph: {e}\")\n    import traceback\n    traceback.print_exc()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-google-genai not found, installing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up RAG system with google - gemini-2.5-pro\n",
      "‚úÖ Retriever verified\n",
      "ü§ñ Initializing LLM...\n",
      "‚úÖ LLM initialized successfully\n",
      "\n",
      "üèóÔ∏è  Building the RAG graph...\n",
      "‚úÖ Graph compiled successfully!\n",
      "\n",
      "============================================================\n",
      "TESTING THE RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "üîç RETRIEVE NODE: Searching for documents related to: 'What is the purpose of this project?'\n",
      "   Found 4 relevant documents\n",
      "\n",
      "‚úçÔ∏è  GENERATE NODE: Creating answer using 4 documents\n",
      "   Generated answer (preview): Based on the context provided, the purpose of this project is to improve the company's new hire onbo...\n",
      "\n",
      "============================================================\n",
      "FINAL RESULT\n",
      "============================================================\n",
      "\n",
      "Question: What is the purpose of this project?\n",
      "\n",
      "Answer: Based on the context provided, the purpose of this project is to improve the company's new hire onboarding process by achieving four main goals:\n",
      "\n",
      "1.  **Accelerate New Hire Time-to-Productivity:** To get new employees contributing effectively in a shorter amount of time.\n",
      "2.  **Enhance the New Hire Experience:** To improve satisfaction for new employees during their first 30 days.\n",
      "3.  **Increase Operational Efficiency:** To reduce the time HR and managers spend creating and managing onboarding plans.\n",
      "4.  **Improve Company Compliance:** To ensure all new hires complete mandatory tasks, like security training, on time.\n",
      "\n",
      "Documents used: 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Task:** Add a second agent to your graph that acts as a \"Grader,\" deciding if the retrieved documents are relevant enough to answer the question.\n",
    "\n",
    "> **What is a conditional edge?** It's a decision point. After a node completes its task (like our 'Grader'), the conditional edge runs a function to decide which node to go to next. This allows your agent to change its plan based on new information.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Keep your `RETRIEVE` and `GENERATE` nodes from the previous challenge.\n",
    "2.  Create a new \"Grader\" node. This function takes the state (question and documents) and calls an LLM with a specific prompt: \"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\"\n",
    "3.  Add a **conditional edge** to your graph. After the `RETRIEVE` node, the graph should go to the `GRADE` node. After the `GRADE` node, it should check the grader's response. If 'yes', it proceeds to the `GENERATE` node. If 'no', it goes to an `END` node, concluding that it cannot answer the question.\n",
    "\n",
    "**Expected Quality:** A more robust graph that can gracefully handle cases where its knowledge base doesn't contain the answer, preventing it from hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:54:36.050284Z",
     "start_time": "2025-11-04T19:54:20.616319Z"
    }
   },
   "source": "# Challenge 2: RAG Graph with Grader Agent and Conditional Edges\n\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, END\n\nprint(\"üèóÔ∏è  Building RAG system with Grader agent...\")\n\n# Step 1: Define the AgentState (same as Challenge 1, but adding grade field)\nclass AgentState(TypedDict):\n    question: str           # The user's question\n    documents: list         # Retrieved documents\n    grade: str             # Grader's decision: 'yes' or 'no'\n    answer: str            # Final answer\n\n# Step 2: Initialize the LLM (reusing from Challenge 1)\nif api_provider == \"google\":\n    from langchain_google_genai import ChatGoogleGenerativeAI\n    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)\nelif api_provider == \"openai\":\n    from langchain_openai import ChatOpenAI\n    llm = ChatOpenAI(model=model_name, temperature=0)\nelse:\n    from langchain_openai import ChatOpenAI\n    llm = ChatOpenAI(model=model_name, temperature=0)\n\nprint(\"‚úÖ LLM initialized\")\n\n# Step 3: Create the Retriever Node (same as Challenge 1)\ndef retrieve_node(state: AgentState) -> AgentState:\n    \"\"\"Retrieve relevant documents based on the question.\"\"\"\n    question = state[\"question\"]\n    print(f\"\\nüîç RETRIEVE NODE: Searching for: '{question}'\")\n    \n    docs = retriever.invoke(question)\n    print(f\"   Found {len(docs)} documents\")\n    \n    return {\"documents\": docs}\n\n# Step 4: Create the NEW Grader Node\n# This agent evaluates if the retrieved documents are good enough to answer the question\ndef grade_node(state: AgentState) -> AgentState:\n    \"\"\"Grade whether the retrieved documents are relevant to the question.\"\"\"\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    \n    print(f\"\\nüìä GRADE NODE: Evaluating document relevance...\")\n    \n    # Create a context string from the documents\n    context = \"\\n\\n\".join([doc.page_content[:200] for doc in documents])  # Truncate for grading\n    \n    # Ask the LLM to grade the documents\n    grade_prompt = f\"\"\"Based on the question and the following documents, is the information sufficient to answer the question?\n\nQuestion: {question}\n\nDocuments:\n{context}\n\nAnswer with ONLY 'yes' or 'no' (lowercase, no punctuation).\"\"\"\n    \n    response = llm.invoke(grade_prompt)\n    grade = response.content.strip().lower()\n    \n    # Ensure we got a valid response\n    if grade not in ['yes', 'no']:\n        # Default to 'no' if unclear response\n        print(f\"   Unclear grade response: '{grade}', defaulting to 'no'\")\n        grade = 'no'\n    \n    print(f\"   Grade: {grade.upper()}\")\n    \n    return {\"grade\": grade}\n\n# Step 5: Create the Generator Node (same as Challenge 1)\ndef generate_node(state: AgentState) -> AgentState:\n    \"\"\"Generate an answer based on the question and retrieved documents.\"\"\"\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    \n    print(f\"\\n‚úçÔ∏è  GENERATE NODE: Creating answer...\")\n    \n    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n    \n    prompt = f\"\"\"Based on the following context, please answer the question.\n    \nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n    \n    response = llm.invoke(prompt)\n    answer = response.content\n    \n    print(f\"   Answer generated (preview): {answer[:80]}...\")\n    \n    return {\"answer\": answer}\n\n# Step 6: Create a function to decide where to route after grading\n# This is the \"conditional edge\" logic\ndef decide_to_generate(state: AgentState) -> str:\n    \"\"\"\n    Conditional edge function that determines the next node based on the grade.\n    \n    Returns:\n        - \"generate\" if grade is 'yes' (documents are good)\n        - \"end\" if grade is 'no' (documents are insufficient)\n    \"\"\"\n    grade = state.get(\"grade\", \"no\")\n    \n    if grade == \"yes\":\n        print(\"   ‚Üí Routing to GENERATE (documents are sufficient)\")\n        return \"generate\"\n    else:\n        print(\"   ‚Üí Routing to END (documents are insufficient)\")\n        return \"end\"\n\n# Step 7: Build the StateGraph with conditional edges\nworkflow = StateGraph(AgentState)\n\n# Add all three nodes\nworkflow.add_node(\"retrieve\", retrieve_node)\nworkflow.add_node(\"grade\", grade_node)\nworkflow.add_node(\"generate\", generate_node)\n\n# Define the flow with conditional routing:\n# Start ‚Üí Retrieve ‚Üí Grade ‚Üí (conditional: Generate or End)\nworkflow.set_entry_point(\"retrieve\")\n\n# After retrieve, always go to grade\nworkflow.add_edge(\"retrieve\", \"grade\")\n\n# After grade, use conditional edge to decide next step\n# The decide_to_generate function returns either \"generate\" or \"end\"\nworkflow.add_conditional_edges(\n    \"grade\",                    # From the grade node\n    decide_to_generate,         # Use this function to decide where to go\n    {\n        \"generate\": \"generate\",  # If function returns \"generate\", go to generate node\n        \"end\": END              # If function returns \"end\", end the workflow\n    }\n)\n\n# After generate, we're done\nworkflow.add_edge(\"generate\", END)\n\n# Step 8: Compile the graph\napp = workflow.compile()\n\nprint(\"‚úÖ Graph with Grader compiled successfully!\")\n\n# Step 9: Test with a question that SHOULD have good documents\nprint(\"\\n\" + \"=\"*70)\nprint(\"TEST 1: Question with relevant documents\")\nprint(\"=\"*70)\n\ntest_question_1 = \"What database was chosen for this project?\"\n\nresult_1 = app.invoke({\n    \"question\": test_question_1,\n    \"documents\": [],\n    \"grade\": \"\",\n    \"answer\": \"\"\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULT 1\")\nprint(\"=\"*70)\nprint(f\"Question: {test_question_1}\")\nprint(f\"Grade: {result_1.get('grade', 'N/A')}\")\nprint(f\"Answer: {result_1.get('answer', 'No answer generated - insufficient documents')}\")\n\n# Step 10: Test with a question that should NOT have good documents\nprint(\"\\n\" + \"=\"*70)\nprint(\"TEST 2: Question with irrelevant documents\")\nprint(\"=\"*70)\n\ntest_question_2 = \"What is the weather like on Mars?\"\n\nresult_2 = app.invoke({\n    \"question\": test_question_2,\n    \"documents\": [],\n    \"grade\": \"\",\n    \"answer\": \"\"\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULT 2\")\nprint(\"=\"*70)\nprint(f\"Question: {test_question_2}\")\nprint(f\"Grade: {result_2.get('grade', 'N/A')}\")\nprint(f\"Answer: {result_2.get('answer', 'No answer generated - insufficient documents')}\")\n\nprint(\"\\n‚úÖ Challenge 2 complete! The Grader prevents hallucinations by checking document relevance.\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  Building RAG system with Grader agent...\n",
      "‚úÖ LLM initialized\n",
      "‚úÖ Graph with Grader compiled successfully!\n",
      "\n",
      "======================================================================\n",
      "TEST 1: Question with relevant documents\n",
      "======================================================================\n",
      "\n",
      "üîç RETRIEVE NODE: Searching for: 'What database was chosen for this project?'\n",
      "   Found 4 documents\n",
      "\n",
      "üìä GRADE NODE: Evaluating document relevance...\n",
      "   Grade: YES\n",
      "   ‚Üí Routing to GENERATE (documents are sufficient)\n",
      "\n",
      "‚úçÔ∏è  GENERATE NODE: Creating answer...\n",
      "   Answer generated (preview): Based on the context provided, the database chosen for the project is **PostgreS...\n",
      "\n",
      "======================================================================\n",
      "RESULT 1\n",
      "======================================================================\n",
      "Question: What database was chosen for this project?\n",
      "Grade: yes\n",
      "Answer: Based on the context provided, the database chosen for the project is **PostgreSQL with the `pgvector` extension**.\n",
      "\n",
      "======================================================================\n",
      "TEST 2: Question with irrelevant documents\n",
      "======================================================================\n",
      "\n",
      "üîç RETRIEVE NODE: Searching for: 'What is the weather like on Mars?'\n",
      "   Found 4 documents\n",
      "\n",
      "üìä GRADE NODE: Evaluating document relevance...\n",
      "   Grade: NO\n",
      "   ‚Üí Routing to END (documents are insufficient)\n",
      "\n",
      "======================================================================\n",
      "RESULT 2\n",
      "======================================================================\n",
      "Question: What is the weather like on Mars?\n",
      "Grade: no\n",
      "Answer: \n",
      "\n",
      "‚úÖ Challenge 2 complete! The Grader prevents hallucinations by checking document relevance.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Task:** Build a sophisticated \"research team\" of specialized agents that includes a router to delegate tasks to the correct specialist.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Specialize your retriever:** Create two separate retrievers. One for the PRD (`prd_retriever`) and one for the technical documents (`tech_retriever` for schema and ADRs).\n",
    "2.  **Define the Agents:**\n",
    "    * `ProjectManagerAgent`: This will be the entry point and will act as a router. It uses an LLM to decide whether the user's question is about product requirements or technical details, and routes to the appropriate researcher.\n",
    "    * `PRDResearcherAgent`: A node that uses the `prd_retriever`.\n",
    "    * `TechResearcherAgent`: A node that uses the `tech_retriever`.\n",
    "    * `SynthesizerAgent`: A node that takes the collected documents from either researcher and synthesizes a final answer.\n",
    "3.  **Build the Graph:** Use conditional edges to orchestrate the flow: The entry point is the `ProjectManager`, which then routes to either the `PRD_RESEARCHER` or `TECH_RESEARCHER`. Both of those nodes should then route to the `SYNTHESIZE` node, which then goes to the `END`.\n",
    "\n",
    "**Expected Quality:** A highly advanced agentic system that mimics a real-world research workflow, including a router and specialist roles, to improve the accuracy and efficiency of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:58:10.413670Z",
     "start_time": "2025-11-04T19:57:22.172704Z"
    }
   },
   "source": "# Challenge 3: Multi-Agent Research Team with Specialized Retrievers and Router\n\nfrom typing import TypedDict, Literal\nfrom langgraph.graph import StateGraph, END\n\nprint(\"üèóÔ∏è  Building Multi-Agent Research Team...\")\n\n# Step 1: Create specialized retrievers for different document types\nprint(\"\\nüìö Creating specialized retrievers...\")\n\n# PRD Retriever - only searches product requirement documents\ndef create_prd_retriever():\n    \"\"\"Creates a retriever specialized for PRD documents.\"\"\"\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.document_loaders import TextLoader\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    \n    prd_paths = [\"docs/prd/day1_prd.md\"]\n    docs = []\n    \n    for path in prd_paths:\n        full_path = os.path.join(project_root, path)\n        if os.path.exists(full_path):\n            loader = TextLoader(full_path)\n            loaded_docs = loader.load()\n            for doc in loaded_docs:\n                doc.metadata = {\"source\": path, \"type\": \"prd\"}\n            docs.extend(loaded_docs)\n    \n    if docs:\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n        splits = splitter.split_documents(docs)\n        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n        print(f\"   ‚úÖ PRD Retriever created with {len(splits)} chunks\")\n        return vectorstore.as_retriever()\n    return None\n\n# Tech Retriever - only searches technical documents (schema, ADRs)\ndef create_tech_retriever():\n    \"\"\"Creates a retriever specialized for technical documents.\"\"\"\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.document_loaders import TextLoader\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    \n    tech_paths = [\"database/schema.sql\", \"docs/adr/adr_001_database_choice.md\"]\n    docs = []\n    \n    for path in tech_paths:\n        full_path = os.path.join(project_root, path)\n        if os.path.exists(full_path):\n            loader = TextLoader(full_path)\n            loaded_docs = loader.load()\n            for doc in loaded_docs:\n                doc.metadata = {\"source\": path, \"type\": \"technical\"}\n            docs.extend(loaded_docs)\n    \n    if docs:\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n        splits = splitter.split_documents(docs)\n        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n        print(f\"   ‚úÖ Tech Retriever created with {len(splits)} chunks\")\n        return vectorstore.as_retriever()\n    return None\n\n# Create both specialized retrievers\nprd_retriever = create_prd_retriever()\ntech_retriever = create_tech_retriever()\n\n# Step 2: Define the AgentState for the multi-agent system\nclass ResearchState(TypedDict):\n    question: str               # The user's question\n    route: str                  # Router's decision: 'prd' or 'technical'\n    documents: list             # Documents retrieved by specialist\n    answer: str                 # Final synthesized answer\n\n# Step 3: Initialize the LLM\nif api_provider == \"google\":\n    from langchain_google_genai import ChatGoogleGenerativeAI\n    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)\nelif api_provider == \"openai\":\n    from langchain_openai import ChatOpenAI\n    llm = ChatOpenAI(model=model_name, temperature=0)\nelse:\n    from langchain_openai import ChatOpenAI\n    llm = ChatOpenAI(model=model_name, temperature=0)\n\nprint(\"‚úÖ LLM initialized for multi-agent system\")\n\n# Step 4: Create the Project Manager Agent (Router)\ndef project_manager_node(state: ResearchState) -> ResearchState:\n    \"\"\"\n    Acts as a router to determine if the question is about:\n    - Product requirements (PRD) \n    - Technical implementation (database, architecture)\n    \"\"\"\n    question = state[\"question\"]\n    \n    print(f\"\\nüëî PROJECT MANAGER: Analyzing question type...\")\n    \n    # Ask LLM to classify the question\n    routing_prompt = f\"\"\"You are a project manager routing questions to the right specialist.\n\nQuestion: {question}\n\nIs this question about:\nA) Product requirements, features, business goals, or user needs (answer: 'prd')\nB) Technical implementation, database, architecture, or technical decisions (answer: 'technical')\n\nAnswer with ONLY 'prd' or 'technical' (lowercase, no punctuation).\"\"\"\n    \n    response = llm.invoke(routing_prompt)\n    route = response.content.strip().lower()\n    \n    # Validate response\n    if route not in ['prd', 'technical']:\n        print(f\"   Unclear route: '{route}', defaulting to 'technical'\")\n        route = 'technical'\n    \n    print(f\"   üìã Routing to: {route.upper()} specialist\")\n    \n    return {\"route\": route}\n\n# Step 5: Create the PRD Researcher Agent\ndef prd_researcher_node(state: ResearchState) -> ResearchState:\n    \"\"\"Specialist agent for product requirement questions.\"\"\"\n    question = state[\"question\"]\n    \n    print(f\"\\nüìä PRD RESEARCHER: Searching product documents...\")\n    \n    if prd_retriever:\n        docs = prd_retriever.invoke(question)\n        print(f\"   Found {len(docs)} PRD documents\")\n        return {\"documents\": docs}\n    else:\n        print(\"   ‚ö†Ô∏è  No PRD retriever available\")\n        return {\"documents\": []}\n\n# Step 6: Create the Tech Researcher Agent\ndef tech_researcher_node(state: ResearchState) -> ResearchState:\n    \"\"\"Specialist agent for technical questions.\"\"\"\n    question = state[\"question\"]\n    \n    print(f\"\\nüîß TECH RESEARCHER: Searching technical documents...\")\n    \n    if tech_retriever:\n        docs = tech_retriever.invoke(question)\n        print(f\"   Found {len(docs)} technical documents\")\n        return {\"documents\": docs}\n    else:\n        print(\"   ‚ö†Ô∏è  No tech retriever available\")\n        return {\"documents\": []}\n\n# Step 7: Create the Synthesizer Agent\ndef synthesizer_node(state: ResearchState) -> ResearchState:\n    \"\"\"\n    Takes documents from either researcher and synthesizes a final answer.\n    This is the final step where we generate the answer.\n    \"\"\"\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    route = state.get(\"route\", \"unknown\")\n    \n    print(f\"\\nüéØ SYNTHESIZER: Creating final answer from {route} specialist...\")\n    \n    if not documents:\n        return {\"answer\": f\"I couldn't find sufficient {route} information to answer this question.\"}\n    \n    # Create context from documents\n    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n    \n    # Generate comprehensive answer\n    synthesis_prompt = f\"\"\"You are a synthesizer agent. Based on the {route} documents provided, give a comprehensive answer.\n\nQuestion: {question}\n\nContext from {route} specialist:\n{context}\n\nProvide a clear, detailed answer:\"\"\"\n    \n    response = llm.invoke(synthesis_prompt)\n    answer = response.content\n    \n    print(f\"   ‚úÖ Answer synthesized (preview): {answer[:80]}...\")\n    \n    return {\"answer\": answer}\n\n# Step 8: Create routing function for conditional edges\ndef route_question(state: ResearchState) -> Literal[\"prd_researcher\", \"tech_researcher\"]:\n    \"\"\"\n    Determines which researcher to route to based on the project manager's decision.\n    \n    Returns:\n        - \"prd_researcher\" if question is about product requirements\n        - \"tech_researcher\" if question is about technical details\n    \"\"\"\n    route = state.get(\"route\", \"technical\")\n    \n    if route == \"prd\":\n        return \"prd_researcher\"\n    else:\n        return \"tech_researcher\"\n\n# Step 9: Build the Multi-Agent StateGraph\nprint(\"\\nüî® Assembling the research team...\")\n\nworkflow = StateGraph(ResearchState)\n\n# Add all agent nodes\nworkflow.add_node(\"project_manager\", project_manager_node)\nworkflow.add_node(\"prd_researcher\", prd_researcher_node)\nworkflow.add_node(\"tech_researcher\", tech_researcher_node)\nworkflow.add_node(\"synthesizer\", synthesizer_node)\n\n# Define the workflow:\n# Start ‚Üí Project Manager (router) ‚Üí [PRD or Tech Researcher] ‚Üí Synthesizer ‚Üí End\nworkflow.set_entry_point(\"project_manager\")\n\n# After project manager, route conditionally to the appropriate researcher\nworkflow.add_conditional_edges(\n    \"project_manager\",\n    route_question,\n    {\n        \"prd_researcher\": \"prd_researcher\",\n        \"tech_researcher\": \"tech_researcher\"\n    }\n)\n\n# Both researchers route to synthesizer\nworkflow.add_edge(\"prd_researcher\", \"synthesizer\")\nworkflow.add_edge(\"tech_researcher\", \"synthesizer\")\n\n# Synthesizer ends the workflow\nworkflow.add_edge(\"synthesizer\", END)\n\n# Step 10: Compile the graph\napp = workflow.compile()\n\nprint(\"‚úÖ Multi-Agent Research Team assembled!\")\n\n# Step 11: Test with a PRD question\nprint(\"\\n\" + \"=\"*70)\nprint(\"TEST 1: Product Requirements Question\")\nprint(\"=\"*70)\n\nprd_question = \"What are the main goals of this project?\"\n\nresult_1 = app.invoke({\n    \"question\": prd_question,\n    \"route\": \"\",\n    \"documents\": [],\n    \"answer\": \"\"\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULT 1\")\nprint(\"=\"*70)\nprint(f\"Question: {prd_question}\")\nprint(f\"Routed to: {result_1.get('route', 'N/A').upper()}\")\nprint(f\"Documents found: {len(result_1.get('documents', []))}\")\nprint(f\"\\nAnswer:\\n{result_1.get('answer', 'No answer generated')}\")\n\n# Step 12: Test with a technical question\nprint(\"\\n\" + \"=\"*70)\nprint(\"TEST 2: Technical Question\")\nprint(\"=\"*70)\n\ntech_question = \"What database technology was selected and why?\"\n\nresult_2 = app.invoke({\n    \"question\": tech_question,\n    \"route\": \"\",\n    \"documents\": [],\n    \"answer\": \"\"\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULT 2\")\nprint(\"=\"*70)\nprint(f\"Question: {tech_question}\")\nprint(f\"Routed to: {result_2.get('route', 'N/A').upper()}\")\nprint(f\"Documents found: {len(result_2.get('documents', []))}\")\nprint(f\"\\nAnswer:\\n{result_2.get('answer', 'No answer generated')}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ Challenge 3 Complete!\")\nprint(\"=\"*70)\nprint(\"You've built a sophisticated multi-agent research team that:\")\nprint(\"  1. Routes questions to specialized researchers\")\nprint(\"  2. Uses domain-specific retrievers\")\nprint(\"  3. Synthesizes comprehensive answers\")\nprint(\"  4. Mimics real-world research workflows\")\nprint(\"=\"*70)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  Building Multi-Agent Research Team...\n",
      "\n",
      "üìö Creating specialized retrievers...\n",
      "   ‚úÖ PRD Retriever created with 29 chunks\n",
      "   ‚úÖ Tech Retriever created with 14 chunks\n",
      "‚úÖ LLM initialized for multi-agent system\n",
      "\n",
      "üî® Assembling the research team...\n",
      "‚úÖ Multi-Agent Research Team assembled!\n",
      "\n",
      "======================================================================\n",
      "TEST 1: Product Requirements Question\n",
      "======================================================================\n",
      "\n",
      "üëî PROJECT MANAGER: Analyzing question type...\n",
      "   üìã Routing to: PRD specialist\n",
      "\n",
      "üìä PRD RESEARCHER: Searching product documents...\n",
      "   Found 4 PRD documents\n",
      "\n",
      "üéØ SYNTHESIZER: Creating final answer from prd specialist...\n",
      "   ‚úÖ Answer synthesized (preview): Based on the provided PRD documents, the main goals of this project are to creat...\n",
      "\n",
      "======================================================================\n",
      "RESULT 1\n",
      "======================================================================\n",
      "Question: What are the main goals of this project?\n",
      "Routed to: PRD\n",
      "Documents found: 4\n",
      "\n",
      "Answer:\n",
      "Based on the provided PRD documents, the main goals of this project are to create a centralized and measurable onboarding system that improves outcomes for new hires, increases efficiency for the company, and ensures regulatory compliance.\n",
      "\n",
      "Here is a detailed breakdown of the four primary goals:\n",
      "\n",
      "### 1. Accelerate New Hire Time-to-Productivity\n",
      "The project aims to get new employees contributing to the company's success more quickly. This moves beyond simple orientation to focus on enabling new hires to perform meaningful work faster.\n",
      "*   **Key Performance Indicator (KPI):** Time (in days) to complete the first \"Quick Win\" task.\n",
      "*   **Target:** Decrease the average time by 25% within six months post-launch.\n",
      "\n",
      "### 2. Enhance the New Hire Experience\n",
      "The goal is to create a consistently positive and welcoming onboarding process for all new employees, regardless of their department. This addresses the pain point of the HR Coordinator, Sarah, who worries about ad-hoc and inconsistent experiences.\n",
      "*   **Key Performance Indicator (KPI):** New Hire Satisfaction Score (measured via an NPS-style survey at 30 days).\n",
      "*   **Target:** Achieve a score of +50 or higher.\n",
      "\n",
      "### 3. Increase Operational Efficiency\n",
      "The project seeks to reduce the administrative burden on managers and HR staff who are currently responsible for creating and managing onboarding plans. By automating and centralizing the process, their time can be freed up for more strategic tasks.\n",
      "*   **Key Performance Indicator (KPI):** Time spent by managers and HR creating/managing onboarding plans.\n",
      "*   **Target:** Reduce this time by 40% (measured by qualitative surveys).\n",
      "\n",
      "### 4. Improve Company Compliance\n",
      "A critical goal is to ensure that all new hires complete mandatory training and paperwork in a timely manner. This directly addresses the HR Coordinator's challenge of having \"no centralized way to ensure mandatory compliance training is completed on time.\" The future development of a \"Compliance tracking dashboard\" further supports this objective.\n",
      "*   **Key Performance Indicator (KPI):** On-time completion rate for mandatory tasks (e.g., Security Training).\n",
      "*   **Target:** Achieve 100% completion within the first 30 days for all new hires.\n",
      "\n",
      "======================================================================\n",
      "TEST 2: Technical Question\n",
      "======================================================================\n",
      "\n",
      "üëî PROJECT MANAGER: Analyzing question type...\n",
      "   üìã Routing to: TECHNICAL specialist\n",
      "\n",
      "üîß TECH RESEARCHER: Searching technical documents...\n",
      "   Found 4 technical documents\n",
      "\n",
      "üéØ SYNTHESIZER: Creating final answer from technical specialist...\n",
      "   ‚úÖ Answer synthesized (preview): Based on the technical documents provided, here is a comprehensive answer regard...\n",
      "\n",
      "======================================================================\n",
      "RESULT 2\n",
      "======================================================================\n",
      "Question: What database technology was selected and why?\n",
      "Routed to: TECHNICAL\n",
      "Documents found: 4\n",
      "\n",
      "Answer:\n",
      "Based on the technical documents provided, here is a comprehensive answer regarding the selected database technology and the reasons for its selection.\n",
      "\n",
      "### Selected Database Technology\n",
      "\n",
      "The selected database technology is **PostgreSQL, enhanced with a vector extension** (such as `pgvector`).\n",
      "\n",
      "This solution allows the team to store and query vector embeddings directly within their existing relational database, rather than introducing a separate, specialized vector database.\n",
      "\n",
      "### Rationale for Selection\n",
      "\n",
      "The decision to use a vector-enabled PostgreSQL was driven by a strategic focus on architectural simplicity, operational efficiency, and data integrity, especially given the constraints of a small engineering team. The key reasons are detailed below:\n",
      "\n",
      "**1. Architectural Simplicity and Operational Efficiency**\n",
      "\n",
      "A primary driver for this choice was to minimize system complexity and operational overhead.\n",
      "\n",
      "*   **Unified Data Store:** It allows the team to maintain a single database for both standard relational data (e.g., user metadata, department, creation dates) and the vector embeddings required for semantic search. This avoids the complexity of managing two separate database systems.\n",
      "*   **Reduced Operational Burden:** By building on their existing PostgreSQL infrastructure, the team avoids the significant overhead associated with deploying, monitoring, backing up, and securing a new, specialized database. This was a critical advantage for a small team, as it allows them to leverage their current expertise and tools, eliminating the learning curve and maintenance costs of a new technology.\n",
      "\n",
      "**2. Guaranteed Data Integrity and Consistency**\n",
      "\n",
      "The co-location of vectors and their corresponding metadata within a single, transaction-safe environment was a crucial factor in meeting the high data integrity requirements of the B2B platform.\n",
      "\n",
      "*   **ACID Compliance:** Storing vectors and metadata together ensures that all updates are atomic and ACID-compliant. This completely eliminates an entire class of data synchronization problems that can occur when trying to keep two separate databases (one relational, one vector) in sync.\n",
      "*   **Transactional Safety:** Data remains consistent because changes to metadata and their associated vectors happen within the same transaction.\n",
      "\n",
      "**3. Powerful and Flexible Hybrid Search Capabilities**\n",
      "\n",
      "The solution was chosen to directly enable the core product driver: implementing a powerful hybrid search feature.\n",
      "\n",
      "*   **Atomic Hybrid Queries:** This architecture enables powerful, single-query hybrid searches that combine semantic (vector) search with traditional metadata filtering.\n",
      "*   **Seamless Integration:** Users can execute complex queries that filter results based on specific criteria before or after the semantic search is performed. For example, a single SQL query can find documents semantically similar to *\"time off policy\"* while simultaneously filtering for those belonging to the *\"Engineering\"* department and created in the last year, all in one efficient operation.\n",
      "\n",
      "**4. Leverage of a Mature and Reliable Ecosystem**\n",
      "\n",
      "By choosing PostgreSQL, the team benefits from a battle-tested and robust platform. This directly addresses the constraint that the solution must be highly reliable. The mature ecosystem provides extensive, well-established tools and community support for backups, high availability, security, and performance tuning.\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Challenge 3 Complete!\n",
      "======================================================================\n",
      "You've built a sophisticated multi-agent research team that:\n",
      "  1. Routes questions to specialized researchers\n",
      "  2. Uses domain-specific retrievers\n",
      "  3. Synthesizes comprehensive answers\n",
      "  4. Mimics real-world research workflows\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
