{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Complete Tutorial: Building a Customer Support Agent System\n",
    "\n",
    "Welcome to this comprehensive guide to LangGraph! We'll build a real-world customer support system that progressively demonstrates all core LangGraph capabilities.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **API Keys**: Create a `.env` file in your project root with your API keys:\n",
    "   ```\n",
    "   OPENAI_API_KEY=your-openai-key-here\n",
    "   ```\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "Imagine you're tasked with creating an AI-powered customer support system for a tech company. This system needs to:\n",
    "- Route tickets to the right department\n",
    "- Remember previous interactions\n",
    "- Escalate to humans when needed\n",
    "- Handle multiple agents working together\n",
    "- And much more!\n",
    "\n",
    "Let's start our journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langgraph langchain langchain-openai langchain-community python-dotenv colorama -q\n",
    "\n",
    "# If you encounter issues with SqliteSaver, try installing with extras:\n",
    "# !pip install \"langgraph[sqlite]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded .env from: /Users/agaleana/Documents/AI_Driven_Software_Engineering/.env\n",
      "Warning: SqliteSaver not found. Using MemorySaver as fallback.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional, Literal\n",
    "from datetime import datetime\n",
    "from colorama import Fore, Style, init\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import time\n",
    "\n",
    "# Load environment variables from project root\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Find project root and load .env\n",
    "try:\n",
    "    # Assumes the notebook is in 'labs/Day_01_.../' or similar structure\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    # Fallback for different execution environments\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Load .env file from project root\n",
    "dotenv_path = os.path.join(project_root, '.env')\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path)\n",
    "    print(f\"✅ Loaded .env from: {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"⚠️  No .env file found at: {dotenv_path}\")\n",
    "    print(\"Please create a .env file with your OPENAI_API_KEY\")\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, Graph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.errors import NodeInterrupt\n",
    "\n",
    "# Checkpoint imports - with fallback options\n",
    "try:\n",
    "    from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "except ImportError:\n",
    "    try:\n",
    "        from langgraph.checkpoint import SqliteSaver\n",
    "    except ImportError:\n",
    "        print(\"Warning: SqliteSaver not found. Using MemorySaver as fallback.\")\n",
    "        SqliteSaver = None\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize colorama for colored output\n",
    "init(autoreset=True)\n",
    "\n",
    "# Helper function to create checkpointer\n",
    "def create_checkpointer(db_path=\":memory:\"):\n",
    "    \"\"\"Create appropriate checkpointer based on availability\"\"\"\n",
    "    if SqliteSaver:\n",
    "        return SqliteSaver.from_conn_string(db_path)\n",
    "    else:\n",
    "        return MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking setup...\n",
      "\n",
      "✅ OpenAI API key loaded successfully\n",
      "✅ LangGraph imports successful\n",
      "⚠️  SqliteSaver not available - using MemorySaver\n",
      "\n",
      "🚀 Ready to start!\n"
     ]
    }
   ],
   "source": [
    "# Verify setup\n",
    "print(\"🔍 Checking setup...\\n\")\n",
    "\n",
    "# Check API key\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "if api_key:\n",
    "    print(\"✅ OpenAI API key loaded successfully\")\n",
    "else:\n",
    "    print(\"❌ OpenAI API key not found!\")\n",
    "    print(\"Please create a .env file in your project root with:\")\n",
    "    print(\"OPENAI_API_KEY=your-actual-api-key\")\n",
    "\n",
    "# Check LangGraph imports\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    print(\"✅ LangGraph imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ LangGraph import failed: {e}\")\n",
    "\n",
    "# Check SqliteSaver\n",
    "if SqliteSaver:\n",
    "    print(\"✅ SqliteSaver available\")\n",
    "else:\n",
    "    print(\"⚠️  SqliteSaver not available - using MemorySaver\")\n",
    "\n",
    "print(\"\\n🚀 Ready to start!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Basic Graph Structure\n",
    "\n",
    "Let's start with the foundation - understanding what a graph is in LangGraph. Think of it as a flowchart where each box (node) performs a specific task, and arrows (edges) show how data flows between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Testing Basic Graph:\n",
      "\n",
      "🔍 Analyzing ticket...\n",
      "✓ Routed to security department with high priority\n",
      "\n",
      "📋 Result: Department=security, Priority=high\n"
     ]
    }
   ],
   "source": [
    "# Define our state structure - this is like a shared notebook that all nodes can read and write to\n",
    "class TicketState(TypedDict):\n",
    "    \"\"\"State that flows through our support ticket graph\"\"\"\n",
    "    ticket_id: str\n",
    "    customer_message: str\n",
    "    department: Optional[str]\n",
    "    priority: Optional[str]\n",
    "    resolved: bool\n",
    "    messages: List[BaseMessage]  # Chat history\n",
    "\n",
    "\n",
    "# Our first simple node - analyzes the ticket\n",
    "def analyze_ticket(state: TicketState) -> TicketState:\n",
    "    \"\"\"Analyzes incoming ticket and determines department and priority\"\"\"\n",
    "    print(f\"{Fore.CYAN}🔍 Analyzing ticket...{Style.RESET_ALL}\")\n",
    "    \n",
    "    # In a real system, this would use AI to analyze the message\n",
    "    message = state[\"customer_message\"].lower()\n",
    "    \n",
    "    # Simple keyword-based routing\n",
    "    if \"password\" in message or \"login\" in message:\n",
    "        state[\"department\"] = \"security\"\n",
    "    elif \"payment\" in message or \"billing\" in message:\n",
    "        state[\"department\"] = \"billing\"\n",
    "    elif \"bug\" in message or \"error\" in message:\n",
    "        state[\"department\"] = \"technical\"\n",
    "    else:\n",
    "        state[\"department\"] = \"general\"\n",
    "    \n",
    "    # Determine priority\n",
    "    if \"urgent\" in message or \"asap\" in message:\n",
    "        state[\"priority\"] = \"high\"\n",
    "    else:\n",
    "        state[\"priority\"] = \"normal\"\n",
    "    \n",
    "    print(f\"{Fore.GREEN}✓ Routed to {state['department']} department with {state['priority']} priority{Style.RESET_ALL}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "# Create our first graph\n",
    "def create_basic_graph():\n",
    "    \"\"\"Creates a simple ticket routing graph\"\"\"\n",
    "    # Initialize the graph with our state type\n",
    "    graph = StateGraph(TicketState)\n",
    "    \n",
    "    # Add our analyze node\n",
    "    graph.add_node(\"analyze\", analyze_ticket)\n",
    "    \n",
    "    # Set the entry point\n",
    "    graph.set_entry_point(\"analyze\")\n",
    "    \n",
    "    # Connect analyze to END\n",
    "    graph.add_edge(\"analyze\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Test our basic graph\n",
    "print(\"🚀 Testing Basic Graph:\\n\")\n",
    "basic_graph = create_basic_graph()\n",
    "\n",
    "# Create a test ticket\n",
    "test_ticket = {\n",
    "    \"ticket_id\": \"TICKET-001\",\n",
    "    \"customer_message\": \"URGENT: I can't login to my account!\",\n",
    "    \"resolved\": False,\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "result = basic_graph.invoke(test_ticket)\n",
    "print(f\"\\n📋 Result: Department={result['department']}, Priority={result['priority']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Adding Models (LLMs)\n",
    "\n",
    "Now let's make our agent smarter by adding a language model. This transforms our simple keyword matching into intelligent understanding.\n",
    "\n",
    "**Note**: We're constructing messages directly rather than using ChatPromptTemplate to avoid issues with mixing f-strings and template placeholders. This approach is cleaner when we need to include dynamic state values in our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Testing LLM-Powered Graph:\n",
      "\n",
      "🤖 AI analyzing ticket...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Input to ChatPromptTemplate is missing variables {\\'\"department\"\\'}.  Expected: [\\'\"department\"\\', \\'message\\'] Received: [\\'message\\']\\nNote: if you intended {\"department\"} to be part of the string and not a variable, please escape it with double curly braces like: \\'{{\"department\"}}\\'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 68\u001b[0m\n\u001b[1;32m     59\u001b[0m llm_graph \u001b[38;5;241m=\u001b[39m create_llm_graph()\n\u001b[1;32m     61\u001b[0m test_ticket_complex \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticket_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTICKET-002\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_message\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI was charged twice for my subscription last month. This is really frustrating and I need this fixed immediately!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolved\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[1;32m     66\u001b[0m }\n\u001b[0;32m---> 68\u001b[0m result \u001b[38;5;241m=\u001b[39m llm_graph\u001b[38;5;241m.\u001b[39minvoke(test_ticket_complex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1551\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1553\u001b[0m     config,\n\u001b[1;32m   1554\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1555\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1556\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1557\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1558\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1560\u001b[0m ):\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1562\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1290\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1285\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1286\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1287\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1288\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1289\u001b[0m     ):\n\u001b[0;32m-> 1290\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1291\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1292\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1293\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1294\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1295\u001b[0m         ):\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     run_with_retry(t, retry_policy)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/utils/runnable.py:385\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langgraph/utils/runnable.py:167\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 167\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[96], line 25\u001b[0m, in \u001b[0;36manalyze_with_llm\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get LLM response\u001b[39;00m\n\u001b[1;32m     24\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_message\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Parse the response\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:3044\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   3043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3044\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3045\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3046\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/prompts/base.py:216\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    215\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_prompt_with_error_handling,\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    219\u001b[0m     config,\n\u001b[1;32m    220\u001b[0m     run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m     serialized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m    222\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:1939\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   1937\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1938\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 1939\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m   1940\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m                 func,\n\u001b[1;32m   1942\u001b[0m                 input_,\n\u001b[1;32m   1943\u001b[0m                 config,\n\u001b[1;32m   1944\u001b[0m                 run_manager,\n\u001b[1;32m   1945\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1946\u001b[0m             ),\n\u001b[1;32m   1947\u001b[0m         )\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1949\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/config.py:429\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    428\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/prompts/base.py:189\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m--> 189\u001b[0m     inner_input_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_input_)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/prompts/base.py:183\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m     example_key \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    178\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m to be part of the string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m         create_message(message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_PROMPT_INPUT)\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Input to ChatPromptTemplate is missing variables {\\'\"department\"\\'}.  Expected: [\\'\"department\"\\', \\'message\\'] Received: [\\'message\\']\\nNote: if you intended {\"department\"} to be part of the string and not a variable, please escape it with double curly braces like: \\'{{\"department\"}}\\'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT '"
     ]
    }
   ],
   "source": [
    "# Initialize our LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def analyze_with_llm(state: TicketState) -> TicketState:\n",
    "    \"\"\"Uses LLM to intelligently analyze tickets\"\"\"\n",
    "    print(f\"{Fore.CYAN}🤖 AI analyzing ticket...{Style.RESET_ALL}\")\n",
    "    \n",
    "    # Create a prompt for the LLM\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a customer support ticket analyzer. \n",
    "        Analyze the customer message and determine:\n",
    "        1. Department: security, billing, technical, or general\n",
    "        2. Priority: high, medium, or low\n",
    "        3. A brief summary of the issue\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {\"department\": \"...\", \"priority\": \"...\", \"summary\": \"...\"}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ])\n",
    "    \n",
    "    # Get LLM response\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"message\": state[\"customer_message\"]})\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        analysis = json.loads(response.content)\n",
    "        state[\"department\"] = analysis[\"department\"]\n",
    "        state[\"priority\"] = analysis[\"priority\"]\n",
    "        \n",
    "        # Add the analysis to our message history\n",
    "        state[\"messages\"].append(AIMessage(content=f\"Analysis: {analysis['summary']}\"))\n",
    "        \n",
    "        print(f\"{Fore.GREEN}✓ AI Analysis complete:{Style.RESET_ALL}\")\n",
    "        print(f\"  Department: {analysis['department']}\")\n",
    "        print(f\"  Priority: {analysis['priority']}\")\n",
    "        print(f\"  Summary: {analysis['summary']}\")\n",
    "    except:\n",
    "        # Fallback to simple analysis\n",
    "        print(f\"{Fore.YELLOW}⚠ Falling back to simple analysis{Style.RESET_ALL}\")\n",
    "        return analyze_ticket(state)\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "# Create an enhanced graph with LLM\n",
    "def create_llm_graph():\n",
    "    graph = StateGraph(TicketState)\n",
    "    graph.add_node(\"analyze\", analyze_with_llm)\n",
    "    graph.set_entry_point(\"analyze\")\n",
    "    graph.add_edge(\"analyze\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Test the LLM-powered graph\n",
    "print(\"\\n🤖 Testing LLM-Powered Graph:\\n\")\n",
    "llm_graph = create_llm_graph()\n",
    "\n",
    "test_ticket_complex = {\n",
    "    \"ticket_id\": \"TICKET-002\",\n",
    "    \"customer_message\": \"I was charged twice for my subscription last month. This is really frustrating and I need this fixed immediately!\",\n",
    "    \"resolved\": False,\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "result = llm_graph.invoke(test_ticket_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Streaming - Real-time Updates\n",
    "\n",
    "In a real support system, we want to see updates as they happen, not wait until everything is done. Streaming gives us real-time visibility into what our agent is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_customer(state: TicketState) -> TicketState:\n",
    "    \"\"\"Generate a response to the customer\"\"\"\n",
    "    print(f\"{Fore.CYAN}💬 Generating response...{Style.RESET_ALL}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are a helpful customer support agent. \n",
    "        The ticket has been routed to the {state['department']} department with {state['priority']} priority.\n",
    "        Provide a helpful initial response to acknowledge their issue.\"\"\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"message\": state[\"customer_message\"]})\n",
    "    state[\"messages\"].append(response)\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def create_streaming_graph():\n",
    "    \"\"\"Create a graph that supports streaming\"\"\"\n",
    "    graph = StateGraph(TicketState)\n",
    "    \n",
    "    # Add multiple nodes to see streaming in action\n",
    "    graph.add_node(\"analyze\", analyze_with_llm)\n",
    "    graph.add_node(\"respond\", respond_to_customer)\n",
    "    \n",
    "    # Connect them\n",
    "    graph.set_entry_point(\"analyze\")\n",
    "    graph.add_edge(\"analyze\", \"respond\")\n",
    "    graph.add_edge(\"respond\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Demonstrate streaming\n",
    "print(\"\\n📡 Testing Streaming:\\n\")\n",
    "streaming_graph = create_streaming_graph()\n",
    "\n",
    "# Stream the execution\n",
    "print(\"Streaming updates as they happen:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for event in streaming_graph.stream(test_ticket_complex):\n",
    "    # Each event shows what node was executed and its output\n",
    "    for node, output in event.items():\n",
    "        print(f\"\\n{Fore.BLUE}[{node}]{Style.RESET_ALL} completed\")\n",
    "        if node == \"respond\" and output.get(\"messages\"):\n",
    "            # Show the generated response\n",
    "            last_message = output[\"messages\"][-1]\n",
    "            print(f\"\\n{Fore.GREEN}Response to customer:{Style.RESET_ALL}\")\n",
    "            print(f\"{last_message.content}\")\n",
    "    time.sleep(0.5)  # Simulate real-time updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Persistence - Saving State\n",
    "\n",
    "What happens when our system crashes? Or when we need to handle thousands of tickets? Persistence allows us to save and restore our agent's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a persistent graph with checkpointing\n",
    "def create_persistent_graph():\n",
    "    \"\"\"Create a graph with persistence capabilities\"\"\"\n",
    "    graph = StateGraph(TicketState)\n",
    "    \n",
    "    graph.add_node(\"analyze\", analyze_with_llm)\n",
    "    graph.add_node(\"respond\", respond_to_customer)\n",
    "    \n",
    "    graph.set_entry_point(\"analyze\")\n",
    "    graph.add_edge(\"analyze\", \"respond\")\n",
    "    graph.add_edge(\"respond\", END)\n",
    "    \n",
    "    # Add persistence with checkpointer\n",
    "    checkpointer = create_checkpointer()\n",
    "    \n",
    "    return graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "print(\"\\n💾 Testing Persistence:\\n\")\n",
    "persistent_graph = create_persistent_graph()\n",
    "\n",
    "# Process a ticket with a thread ID (this allows us to resume later)\n",
    "config = {\"configurable\": {\"thread_id\": \"customer-123\"}}\n",
    "\n",
    "# First run\n",
    "print(\"First execution:\")\n",
    "result1 = persistent_graph.invoke(test_ticket_complex, config=config)\n",
    "\n",
    "# Now let's \"resume\" the conversation\n",
    "print(\"\\n\\n🔄 Resuming conversation with same thread:\")\n",
    "followup_ticket = {\n",
    "    \"ticket_id\": \"TICKET-002-FOLLOWUP\",\n",
    "    \"customer_message\": \"Thanks for the response. When can I expect my refund?\",\n",
    "    \"resolved\": False,\n",
    "    \"messages\": result1[\"messages\"]  # Continue with previous messages\n",
    "}\n",
    "\n",
    "# The graph remembers the previous state!\n",
    "result2 = persistent_graph.invoke(followup_ticket, config=config)\n",
    "\n",
    "# Show the conversation history\n",
    "print(\"\\n📜 Full conversation history:\")\n",
    "for i, msg in enumerate(result2[\"messages\"]):\n",
    "    print(f\"\\n[{i+1}] {type(msg).__name__}: {msg.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Tools - Giving Agents Abilities\n",
    "\n",
    "Real support agents need to do more than just talk - they need to check orders, process refunds, and update systems. Tools give our agents these abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools that our agent can use\n",
    "@tool\n",
    "def check_order_status(order_id: str) -> str:\n",
    "    \"\"\"Check the status of a customer order\"\"\"\n",
    "    # Simulate database lookup\n",
    "    print(f\"{Fore.YELLOW}🔍 Checking order {order_id}...{Style.RESET_ALL}\")\n",
    "    return f\"Order {order_id} is currently being processed. Expected delivery: 3-5 business days.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def process_refund(order_id: str, amount: float) -> str:\n",
    "    \"\"\"Process a refund for a customer order\"\"\"\n",
    "    print(f\"{Fore.YELLOW}💰 Processing refund of ${amount} for order {order_id}...{Style.RESET_ALL}\")\n",
    "    return f\"Refund of ${amount} has been initiated. It will appear in your account within 5-7 business days.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def escalate_to_human(reason: str) -> str:\n",
    "    \"\"\"Escalate the ticket to a human agent\"\"\"\n",
    "    print(f\"{Fore.RED}🚨 Escalating to human: {reason}{Style.RESET_ALL}\")\n",
    "    return \"Your ticket has been escalated to a human agent who will contact you within 24 hours.\"\n",
    "\n",
    "\n",
    "# Create a more sophisticated state that includes tool calls\n",
    "class AdvancedTicketState(TypedDict):\n",
    "    ticket_id: str\n",
    "    customer_message: str\n",
    "    department: Optional[str]\n",
    "    priority: Optional[str]\n",
    "    resolved: bool\n",
    "    messages: List[BaseMessage]\n",
    "    next_action: Optional[str]  # What to do next\n",
    "\n",
    "\n",
    "def agent_with_tools(state: AdvancedTicketState) -> AdvancedTicketState:\n",
    "    \"\"\"An agent that can use tools to help customers\"\"\"\n",
    "    print(f\"{Fore.CYAN}🤖 Agent processing with tools...{Style.RESET_ALL}\")\n",
    "    \n",
    "    # Create an agent that can use tools\n",
    "    tools = [check_order_status, process_refund, escalate_to_human]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    # Prepare the conversation\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"You are a helpful customer support agent with access to tools.\n",
    "        Use the appropriate tools to help the customer.\n",
    "        Always be polite and professional.\"\"\"),\n",
    "        HumanMessage(content=state[\"customer_message\"])\n",
    "    ]\n",
    "    \n",
    "    # Get the agent's response\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    state[\"messages\"].append(response)\n",
    "    \n",
    "    # Check if the agent wants to use tools\n",
    "    if response.tool_calls:\n",
    "        state[\"next_action\"] = \"use_tools\"\n",
    "    else:\n",
    "        state[\"next_action\"] = \"respond\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def create_tool_graph():\n",
    "    \"\"\"Create a graph with tool capabilities\"\"\"\n",
    "    graph = StateGraph(AdvancedTicketState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph.add_node(\"agent\", agent_with_tools)\n",
    "    graph.add_node(\"tools\", ToolNode([check_order_status, process_refund, escalate_to_human]))\n",
    "    \n",
    "    # Add conditional routing\n",
    "    def route_next(state: AdvancedTicketState) -> str:\n",
    "        if state[\"next_action\"] == \"use_tools\":\n",
    "            return \"tools\"\n",
    "        return END\n",
    "    \n",
    "    graph.set_entry_point(\"agent\")\n",
    "    graph.add_conditional_edges(\"agent\", route_next)\n",
    "    graph.add_edge(\"tools\", \"agent\")  # After tools, go back to agent\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Test the tool-enabled agent\n",
    "print(\"\\n🛠️ Testing Agent with Tools:\\n\")\n",
    "tool_graph = create_tool_graph()\n",
    "\n",
    "# Test with a refund request\n",
    "refund_request = {\n",
    "    \"ticket_id\": \"TICKET-003\",\n",
    "    \"customer_message\": \"I need a refund for order #12345. I was charged $99.99 twice!\",\n",
    "    \"resolved\": False,\n",
    "    \"messages\": [],\n",
    "    \"next_action\": None\n",
    "}\n",
    "\n",
    "# Stream the execution to see tools in action\n",
    "for event in tool_graph.stream(refund_request):\n",
    "    for node, output in event.items():\n",
    "        print(f\"\\n{Fore.BLUE}[{node}]{Style.RESET_ALL} executed\")\n",
    "        if output.get(\"messages\"):\n",
    "            last_msg = output[\"messages\"][-1]\n",
    "            if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "                print(\"Tool calls requested:\", [tc[\"name\"] for tc in last_msg.tool_calls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Human-in-the-Loop\n",
    "\n",
    "Some decisions are too important for AI alone. Human-in-the-loop patterns allow us to pause execution and wait for human approval before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requires_approval(state: AdvancedTicketState) -> AdvancedTicketState:\n",
    "    \"\"\"Check if an action requires human approval\"\"\"\n",
    "    print(f\"{Fore.YELLOW}🤔 Checking if approval needed...{Style.RESET_ALL}\")\n",
    "    \n",
    "    # Check the last message for high-value actions\n",
    "    last_message = state[\"messages\"][-1] if state[\"messages\"] else None\n",
    "    \n",
    "    if last_message and hasattr(last_message, \"tool_calls\"):\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            # Refunds over $100 need approval\n",
    "            if tool_call[\"name\"] == \"process_refund\":\n",
    "                amount = tool_call[\"args\"].get(\"amount\", 0)\n",
    "                if amount > 100:\n",
    "                    print(f\"{Fore.RED}⚠️  High-value refund (${amount}) requires approval!{Style.RESET_ALL}\")\n",
    "                    # Interrupt execution for human approval\n",
    "                    raise NodeInterrupt(f\"Approval needed for ${amount} refund\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def create_human_in_loop_graph():\n",
    "    \"\"\"Create a graph with human approval workflow\"\"\"\n",
    "    graph = StateGraph(AdvancedTicketState)\n",
    "    \n",
    "    graph.add_node(\"agent\", agent_with_tools)\n",
    "    graph.add_node(\"check_approval\", requires_approval)\n",
    "    graph.add_node(\"tools\", ToolNode([check_order_status, process_refund, escalate_to_human]))\n",
    "    \n",
    "    def route_after_agent(state: AdvancedTicketState) -> str:\n",
    "        if state[\"next_action\"] == \"use_tools\":\n",
    "            return \"check_approval\"\n",
    "        return END\n",
    "    \n",
    "    graph.set_entry_point(\"agent\")\n",
    "    graph.add_conditional_edges(\"agent\", route_after_agent)\n",
    "    graph.add_edge(\"check_approval\", \"tools\")\n",
    "    graph.add_edge(\"tools\", \"agent\")\n",
    "    \n",
    "    # Use memory saver for interrupts\n",
    "    return graph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "\n",
    "print(\"\\n👤 Testing Human-in-the-Loop:\\n\")\n",
    "human_graph = create_human_in_loop_graph()\n",
    "\n",
    "# Test with a high-value refund\n",
    "high_value_refund = {\n",
    "    \"ticket_id\": \"TICKET-004\",\n",
    "    \"customer_message\": \"I was incorrectly charged $250 for a premium subscription I never ordered. Please refund immediately!\",\n",
    "    \"resolved\": False,\n",
    "    \"messages\": [],\n",
    "    \"next_action\": None\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"high-value-001\"}}\n",
    "\n",
    "# First execution - will interrupt for approval\n",
    "print(\"Initial execution:\")\n",
    "try:\n",
    "    for event in human_graph.stream(high_value_refund, config=config):\n",
    "        for node, output in event.items():\n",
    "            print(f\"\\n{Fore.BLUE}[{node}]{Style.RESET_ALL} executed\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n{Fore.RED}Execution interrupted: {e}{Style.RESET_ALL}\")\n",
    "    print(\"\\nWaiting for human approval...\")\n",
    "    \n",
    "    # Simulate human approval\n",
    "    input(\"\\nPress Enter to approve the refund...\")\n",
    "    \n",
    "    # Resume execution after approval\n",
    "    print(\"\\n✅ Approved! Resuming execution...\")\n",
    "    for event in human_graph.stream(None, config=config):\n",
    "        for node, output in event.items():\n",
    "            print(f\"\\n{Fore.BLUE}[{node}]{Style.RESET_ALL} executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Memory and Context\n",
    "\n",
    "Good customer support remembers past interactions. Let's add long-term memory to our agent so it can provide personalized service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced state with customer history\n",
    "class CustomerMemoryState(TypedDict):\n",
    "    ticket_id: str\n",
    "    customer_id: str\n",
    "    customer_message: str\n",
    "    customer_history: List[Dict[str, Any]]  # Past interactions\n",
    "    messages: List[BaseMessage]\n",
    "    resolved: bool\n",
    "\n",
    "\n",
    "# Simulate a customer database\n",
    "CUSTOMER_DATABASE = {\n",
    "    \"CUST-001\": {\n",
    "        \"name\": \"Alice Johnson\",\n",
    "        \"tier\": \"Premium\",\n",
    "        \"history\": [\n",
    "            {\"date\": \"2024-01-15\", \"issue\": \"Login problems\", \"resolved\": True},\n",
    "            {\"date\": \"2024-02-20\", \"issue\": \"Billing question\", \"resolved\": True}\n",
    "        ]\n",
    "    },\n",
    "    \"CUST-002\": {\n",
    "        \"name\": \"Bob Smith\",\n",
    "        \"tier\": \"Basic\",\n",
    "        \"history\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def load_customer_context(state: CustomerMemoryState) -> CustomerMemoryState:\n",
    "    \"\"\"Load customer history and context\"\"\"\n",
    "    print(f\"{Fore.CYAN}📚 Loading customer context...{Style.RESET_ALL}\")\n",
    "    \n",
    "    customer_id = state[\"customer_id\"]\n",
    "    customer_data = CUSTOMER_DATABASE.get(customer_id, {})\n",
    "    \n",
    "    if customer_data:\n",
    "        state[\"customer_history\"] = customer_data[\"history\"]\n",
    "        print(f\"✓ Loaded history for {customer_data['name']} ({customer_data['tier']} tier)\")\n",
    "        print(f\"  Previous tickets: {len(customer_data['history'])}\")\n",
    "    else:\n",
    "        state[\"customer_history\"] = []\n",
    "        print(\"  New customer - no history found\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def personalized_agent(state: CustomerMemoryState) -> CustomerMemoryState:\n",
    "    \"\"\"Agent that uses customer history for personalized responses\"\"\"\n",
    "    print(f\"{Fore.CYAN}🤖 Generating personalized response...{Style.RESET_ALL}\")\n",
    "    \n",
    "    customer_data = CUSTOMER_DATABASE.get(state[\"customer_id\"], {})\n",
    "    \n",
    "    # Build context from history\n",
    "    history_context = \"\"\n",
    "    if state[\"customer_history\"]:\n",
    "        history_context = \"Previous interactions:\\n\"\n",
    "        for interaction in state[\"customer_history\"]:\n",
    "            history_context += f\"- {interaction['date']}: {interaction['issue']}\\n\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are a customer support agent with access to customer history.\n",
    "        Customer: {customer_data.get('name', 'Unknown')}\n",
    "        Tier: {customer_data.get('tier', 'Basic')}\n",
    "        {history_context}\n",
    "        \n",
    "        Provide personalized support based on their history and tier.\n",
    "        Premium customers get priority treatment.\"\"\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"message\": state[\"customer_message\"]})\n",
    "    state[\"messages\"].append(response)\n",
    "    \n",
    "    # Update customer history\n",
    "    new_interaction = {\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"issue\": state[\"customer_message\"][:50] + \"...\",\n",
    "        \"resolved\": state[\"resolved\"]\n",
    "    }\n",
    "    CUSTOMER_DATABASE[state[\"customer_id\"]][\"history\"].append(new_interaction)\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def create_memory_graph():\n",
    "    \"\"\"Create a graph with customer memory\"\"\"\n",
    "    graph = StateGraph(CustomerMemoryState)\n",
    "    \n",
    "    graph.add_node(\"load_context\", load_customer_context)\n",
    "    graph.add_node(\"personalized_response\", personalized_agent)\n",
    "    \n",
    "    graph.set_entry_point(\"load_context\")\n",
    "    graph.add_edge(\"load_context\", \"personalized_response\")\n",
    "    graph.add_edge(\"personalized_response\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "print(\"\\n🧠 Testing Memory and Context:\\n\")\n",
    "memory_graph = create_memory_graph()\n",
    "\n",
    "# Test with a returning premium customer\n",
    "premium_customer_ticket = {\n",
    "    \"ticket_id\": \"TICKET-005\",\n",
    "    \"customer_id\": \"CUST-001\",\n",
    "    \"customer_message\": \"I'm having login issues again. This is the second time this year!\",\n",
    "    \"customer_history\": [],\n",
    "    \"messages\": [],\n",
    "    \"resolved\": False\n",
    "}\n",
    "\n",
    "result = memory_graph.invoke(premium_customer_ticket)\n",
    "print(f\"\\n{Fore.GREEN}Response:{Style.RESET_ALL}\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8: Time Travel - Debugging and Analysis\n",
    "\n",
    "When things go wrong, we need to understand what happened. Time travel lets us replay executions and see exactly what our agent did at each step.\n",
    "\n",
    "**Note**: Since we're using `AdvancedTicketState` here, we need adapted versions of our functions that handle this state type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_debuggable_graph():\n",
    "    \"\"\"Create a graph with full debugging capabilities\"\"\"\n",
    "    graph = StateGraph(AdvancedTicketState)\n",
    "    \n",
    "    # Add multiple nodes to track\n",
    "    graph.add_node(\"analyze\", analyze_with_llm)\n",
    "    graph.add_node(\"route\", lambda s: {**s, \"department\": s.get(\"department\", \"general\")})\n",
    "    graph.add_node(\"respond\", respond_to_customer)\n",
    "    \n",
    "    graph.set_entry_point(\"analyze\")\n",
    "    graph.add_edge(\"analyze\", \"route\")\n",
    "    graph.add_edge(\"route\", \"respond\")\n",
    "    graph.add_edge(\"respond\", END)\n",
    "    \n",
    "    # Use persistent checkpointing\n",
    "    checkpointer = create_checkpointer()\n",
    "    return graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "print(\"\\n⏰ Testing Time Travel Debugging:\\n\")\n",
    "debug_graph = create_debuggable_graph()\n",
    "\n",
    "# Execute with detailed tracking\n",
    "config = {\"configurable\": {\"thread_id\": \"debug-session-001\"}}\n",
    "\n",
    "test_ticket = {\n",
    "    \"ticket_id\": \"TICKET-006\",\n",
    "    \"customer_message\": \"My application keeps crashing when I try to export data.\",\n",
    "    \"resolved\": False,\n",
    "    \"messages\": [],\n",
    "    \"next_action\": None\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "print(\"Executing graph with checkpointing...\")\n",
    "result = debug_graph.invoke(test_ticket, config=config)\n",
    "\n",
    "# Now let's travel through time and see what happened\n",
    "print(\"\\n🔍 Replaying execution history:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get all states from the execution\n",
    "states = []\n",
    "for state in debug_graph.get_state_history(config):\n",
    "    states.append(state)\n",
    "\n",
    "# Show states in chronological order (reverse the list)\n",
    "states.reverse()\n",
    "\n",
    "for i, state in enumerate(states):\n",
    "    print(f\"\\n📍 Step {i}: Node '{state.metadata.get('langgraph_node', 'unknown')}'\")\n",
    "    if state.values.get(\"department\"):\n",
    "        print(f\"   Department: {state.values['department']}\")\n",
    "    if state.values.get(\"priority\"):\n",
    "        print(f\"   Priority: {state.values['priority']}\")\n",
    "    print(f\"   Messages: {len(state.values.get('messages', []))}\")\n",
    "\n",
    "# You can also replay from any point\n",
    "print(\"\\n🔄 Replaying from a specific checkpoint...\")\n",
    "if len(states) > 1:\n",
    "    # Get state from before the last node\n",
    "    replay_state = states[-2]\n",
    "    print(f\"Replaying from state before '{replay_state.metadata.get('langgraph_node')}'\")\n",
    "    \n",
    "    # Modify and replay\n",
    "    modified_input = {\n",
    "        **replay_state.values,\n",
    "        \"customer_message\": \"Actually, it's a CRITICAL production issue!\"\n",
    "    }\n",
    "    \n",
    "    replay_result = debug_graph.invoke(modified_input, config={\"configurable\": {\"thread_id\": \"replay-001\"}})\n",
    "    print(f\"New priority after replay: {replay_result.get('priority')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Subgraphs - Modular Architecture\n",
    "\n",
    "As our system grows, we need to organize it better. Subgraphs let us create modular, reusable components that can be composed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specialized subgraphs for different departments\n",
    "\n",
    "def create_billing_subgraph():\n",
    "    \"\"\"Subgraph specifically for billing issues\"\"\"\n",
    "    \n",
    "    class BillingState(TypedDict):\n",
    "        issue_type: str\n",
    "        amount: Optional[float]\n",
    "        action_taken: str\n",
    "        messages: List[BaseMessage]\n",
    "    \n",
    "    def categorize_billing_issue(state: BillingState) -> BillingState:\n",
    "        print(f\"{Fore.YELLOW}💳 Categorizing billing issue...{Style.RESET_ALL}\")\n",
    "        # Simple categorization logic\n",
    "        state[\"issue_type\"] = \"refund_request\"\n",
    "        state[\"amount\"] = 99.99\n",
    "        return state\n",
    "    \n",
    "    def process_billing_action(state: BillingState) -> BillingState:\n",
    "        print(f\"{Fore.YELLOW}💰 Processing billing action...{Style.RESET_ALL}\")\n",
    "        if state[\"issue_type\"] == \"refund_request\":\n",
    "            state[\"action_taken\"] = f\"Refund of ${state['amount']} initiated\"\n",
    "        return state\n",
    "    \n",
    "    graph = StateGraph(BillingState)\n",
    "    graph.add_node(\"categorize\", categorize_billing_issue)\n",
    "    graph.add_node(\"process\", process_billing_action)\n",
    "    \n",
    "    graph.set_entry_point(\"categorize\")\n",
    "    graph.add_edge(\"categorize\", \"process\")\n",
    "    graph.add_edge(\"process\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "def create_technical_subgraph():\n",
    "    \"\"\"Subgraph for technical support issues\"\"\"\n",
    "    \n",
    "    class TechnicalState(TypedDict):\n",
    "        error_type: str\n",
    "        severity: str\n",
    "        solution: str\n",
    "        messages: List[BaseMessage]\n",
    "    \n",
    "    def diagnose_issue(state: TechnicalState) -> TechnicalState:\n",
    "        print(f\"{Fore.CYAN}🔧 Diagnosing technical issue...{Style.RESET_ALL}\")\n",
    "        state[\"error_type\"] = \"application_crash\"\n",
    "        state[\"severity\"] = \"high\"\n",
    "        return state\n",
    "    \n",
    "    def provide_solution(state: TechnicalState) -> TechnicalState:\n",
    "        print(f\"{Fore.CYAN}💡 Providing solution...{Style.RESET_ALL}\")\n",
    "        state[\"solution\"] = \"Please try clearing your cache and updating to the latest version.\"\n",
    "        return state\n",
    "    \n",
    "    graph = StateGraph(TechnicalState)\n",
    "    graph.add_node(\"diagnose\", diagnose_issue)\n",
    "    graph.add_node(\"solve\", provide_solution)\n",
    "    \n",
    "    graph.set_entry_point(\"diagnose\")\n",
    "    graph.add_edge(\"diagnose\", \"solve\")\n",
    "    graph.add_edge(\"solve\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Main graph that routes to subgraphs\n",
    "class MainRoutingState(TypedDict):\n",
    "    ticket_id: str\n",
    "    customer_message: str\n",
    "    department: str\n",
    "    subgraph_result: Optional[Dict[str, Any]]\n",
    "    messages: List[BaseMessage]\n",
    "\n",
    "\n",
    "def create_main_graph_with_subgraphs():\n",
    "    \"\"\"Main graph that delegates to specialized subgraphs\"\"\"\n",
    "    \n",
    "    # Create subgraphs\n",
    "    billing_graph = create_billing_subgraph()\n",
    "    technical_graph = create_technical_subgraph()\n",
    "    \n",
    "    def route_to_department(state: MainRoutingState) -> MainRoutingState:\n",
    "        print(f\"{Fore.BLUE}🚦 Routing to department...{Style.RESET_ALL}\")\n",
    "        \n",
    "        # Simple routing logic\n",
    "        if \"billing\" in state[\"customer_message\"].lower() or \"refund\" in state[\"customer_message\"].lower():\n",
    "            state[\"department\"] = \"billing\"\n",
    "        elif \"error\" in state[\"customer_message\"].lower() or \"crash\" in state[\"customer_message\"].lower():\n",
    "            state[\"department\"] = \"technical\"\n",
    "        else:\n",
    "            state[\"department\"] = \"general\"\n",
    "        \n",
    "        print(f\"  → Routed to {state['department']}\")\n",
    "        return state\n",
    "    \n",
    "    def handle_billing(state: MainRoutingState) -> MainRoutingState:\n",
    "        print(f\"{Fore.YELLOW}🏦 Handling in billing department...{Style.RESET_ALL}\")\n",
    "        # Prepare input for billing subgraph\n",
    "        billing_input = {\n",
    "            \"issue_type\": None,\n",
    "            \"amount\": None,\n",
    "            \"action_taken\": None,\n",
    "            \"messages\": state[\"messages\"]\n",
    "        }\n",
    "        \n",
    "        # Execute billing subgraph\n",
    "        result = billing_graph.invoke(billing_input)\n",
    "        state[\"subgraph_result\"] = result\n",
    "        return state\n",
    "    \n",
    "    def handle_technical(state: MainRoutingState) -> MainRoutingState:\n",
    "        print(f\"{Fore.CYAN}⚙️ Handling in technical department...{Style.RESET_ALL}\")\n",
    "        # Prepare input for technical subgraph\n",
    "        tech_input = {\n",
    "            \"error_type\": None,\n",
    "            \"severity\": None,\n",
    "            \"solution\": None,\n",
    "            \"messages\": state[\"messages\"]\n",
    "        }\n",
    "        \n",
    "        # Execute technical subgraph\n",
    "        result = technical_graph.invoke(tech_input)\n",
    "        state[\"subgraph_result\"] = result\n",
    "        return state\n",
    "    \n",
    "    def handle_general(state: MainRoutingState) -> MainRoutingState:\n",
    "        print(f\"{Fore.GREEN}📋 Handling in general support...{Style.RESET_ALL}\")\n",
    "        state[\"subgraph_result\"] = {\"response\": \"Thank you for contacting support. We'll help you shortly.\"}\n",
    "        return state\n",
    "    \n",
    "    # Build main graph\n",
    "    graph = StateGraph(MainRoutingState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph.add_node(\"route\", route_to_department)\n",
    "    graph.add_node(\"billing\", handle_billing)\n",
    "    graph.add_node(\"technical\", handle_technical)\n",
    "    graph.add_node(\"general\", handle_general)\n",
    "    \n",
    "    # Add conditional routing\n",
    "    def select_department(state: MainRoutingState) -> str:\n",
    "        return state[\"department\"]\n",
    "    \n",
    "    graph.set_entry_point(\"route\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"route\",\n",
    "        select_department,\n",
    "        {\n",
    "            \"billing\": \"billing\",\n",
    "            \"technical\": \"technical\",\n",
    "            \"general\": \"general\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # All departments lead to END\n",
    "    graph.add_edge(\"billing\", END)\n",
    "    graph.add_edge(\"technical\", END)\n",
    "    graph.add_edge(\"general\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "print(\"\\n🏗️ Testing Subgraphs:\\n\")\n",
    "main_graph = create_main_graph_with_subgraphs()\n",
    "\n",
    "# Test different types of tickets\n",
    "test_tickets = [\n",
    "    {\n",
    "        \"ticket_id\": \"TICKET-007\",\n",
    "        \"customer_message\": \"I need a refund for my subscription\",\n",
    "        \"messages\": []\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TICKET-008\",\n",
    "        \"customer_message\": \"The app crashes when I try to export\",\n",
    "        \"messages\": []\n",
    "    }\n",
    "]\n",
    "\n",
    "for ticket in test_tickets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {ticket['customer_message']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = main_graph.invoke(ticket)\n",
    "    print(f\"\\n✅ Result: {result.get('subgraph_result', {})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10: Multi-Agent Collaboration\n",
    "\n",
    "Complex problems often need multiple specialists working together. Let's create a multi-agent system where different AI agents collaborate to solve customer issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialized agents\n",
    "class MultiAgentState(TypedDict):\n",
    "    ticket_id: str\n",
    "    customer_message: str\n",
    "    analysis: Dict[str, Any]\n",
    "    technical_assessment: Optional[str]\n",
    "    business_impact: Optional[str]\n",
    "    proposed_solution: Optional[str]\n",
    "    final_response: Optional[str]\n",
    "    messages: List[BaseMessage]\n",
    "\n",
    "\n",
    "def analyst_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Analyst agent that understands the problem\"\"\"\n",
    "    print(f\"{Fore.BLUE}👔 Analyst Agent: Analyzing problem...{Style.RESET_ALL}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a business analyst. Analyze the customer's issue and identify:\n",
    "        1. The core problem\n",
    "        2. Business impact\n",
    "        3. Urgency level\n",
    "        Respond in JSON format.\"\"\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"message\": state[\"customer_message\"]})\n",
    "    \n",
    "    try:\n",
    "        state[\"analysis\"] = json.loads(response.content)\n",
    "        print(f\"  ✓ Analysis complete: {state['analysis'].get('core_problem', 'Unknown')}\")\n",
    "    except:\n",
    "        state[\"analysis\"] = {\"core_problem\": \"Unable to parse\", \"impact\": \"unknown\", \"urgency\": \"medium\"}\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def technical_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Technical agent that assesses technical aspects\"\"\"\n",
    "    print(f\"{Fore.CYAN}🔧 Technical Agent: Evaluating technical aspects...{Style.RESET_ALL}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are a technical expert. Based on this analysis:\n",
    "        {json.dumps(state['analysis'], indent=2)}\n",
    "        \n",
    "        Provide a technical assessment and identify potential root causes.\"\"\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"message\": state[\"customer_message\"]})\n",
    "    state[\"technical_assessment\"] = response.content\n",
    "    \n",
    "    print(f\"  ✓ Technical assessment completed\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def solution_architect(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Solution architect that designs the solution\"\"\"\n",
    "    print(f\"{Fore.GREEN}🏗️ Solution Architect: Designing solution...{Style.RESET_ALL}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are a solution architect. Based on:\n",
    "        Analysis: {json.dumps(state['analysis'], indent=2)}\n",
    "        Technical Assessment: {state['technical_assessment']}\n",
    "        \n",
    "        Design a comprehensive solution that addresses the root cause.\"\"\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"message\": state[\"customer_message\"]})\n",
    "    state[\"proposed_solution\"] = response.content\n",
    "    \n",
    "    print(f\"  ✓ Solution designed\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def communication_specialist(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Communication specialist that crafts the customer response\"\"\"\n",
    "    print(f\"{Fore.MAGENTA}💬 Communication Specialist: Crafting response...{Style.RESET_ALL}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are a communication specialist. Based on all the expert input:\n",
    "        Analysis: {json.dumps(state['analysis'], indent=2)}\n",
    "        Technical Assessment: {state['technical_assessment']}\n",
    "        Solution: {state['proposed_solution']}\n",
    "        \n",
    "        Craft a clear, empathetic response for the customer that:\n",
    "        1. Acknowledges their issue\n",
    "        2. Explains what we understand\n",
    "        3. Provides the solution\n",
    "        4. Sets clear expectations\"\"\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"message\": state[\"customer_message\"]})\n",
    "    state[\"final_response\"] = response.content\n",
    "    state[\"messages\"].append(response)\n",
    "    \n",
    "    print(f\"  ✓ Customer response prepared\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def supervisor_agent(state: MultiAgentState) -> str:\n",
    "    \"\"\"Supervisor that coordinates the agents\"\"\"\n",
    "    print(f\"{Fore.YELLOW}👨‍💼 Supervisor: Determining next agent...{Style.RESET_ALL}\")\n",
    "    \n",
    "    # Determine which agent should work next\n",
    "    if not state.get(\"analysis\"):\n",
    "        return \"analyst\"\n",
    "    elif not state.get(\"technical_assessment\"):\n",
    "        return \"technical\"\n",
    "    elif not state.get(\"proposed_solution\"):\n",
    "        return \"architect\"\n",
    "    elif not state.get(\"final_response\"):\n",
    "        return \"communicator\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "\n",
    "def create_multi_agent_graph():\n",
    "    \"\"\"Create a multi-agent collaboration graph\"\"\"\n",
    "    graph = StateGraph(MultiAgentState)\n",
    "    \n",
    "    # Add all agent nodes\n",
    "    graph.add_node(\"analyst\", analyst_agent)\n",
    "    graph.add_node(\"technical\", technical_agent)\n",
    "    graph.add_node(\"architect\", solution_architect)\n",
    "    graph.add_node(\"communicator\", communication_specialist)\n",
    "    \n",
    "    # Add conditional edges based on supervisor decisions\n",
    "    graph.add_conditional_edges(\n",
    "        \"analyst\",\n",
    "        supervisor_agent,\n",
    "        {\"technical\": \"technical\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"technical\",\n",
    "        supervisor_agent,\n",
    "        {\"architect\": \"architect\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"architect\",\n",
    "        supervisor_agent,\n",
    "        {\"communicator\": \"communicator\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    graph.add_edge(\"communicator\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    graph.set_entry_point(\"analyst\")\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "print(\"\\n👥 Testing Multi-Agent Collaboration:\\n\")\n",
    "multi_agent_graph = create_multi_agent_graph()\n",
    "\n",
    "# Test with a complex issue\n",
    "complex_issue = {\n",
    "    \"ticket_id\": \"TICKET-009\",\n",
    "    \"customer_message\": \"\"\"Our team can't access the dashboard since yesterday's update. \n",
    "    We're losing valuable time and this is affecting our quarterly reporting. \n",
    "    Multiple users across different departments are experiencing this.\"\"\",\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "# Stream the multi-agent collaboration\n",
    "print(\"🎭 Agents collaborating on the issue:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for event in multi_agent_graph.stream(complex_issue):\n",
    "    time.sleep(1)  # Simulate thinking time\n",
    "\n",
    "# Show final response\n",
    "print(f\"\\n{Fore.GREEN}📨 Final Response to Customer:{Style.RESET_ALL}\")\n",
    "print(\"-\" * 60)\n",
    "final_state = multi_agent_graph.invoke(complex_issue)\n",
    "print(final_state[\"final_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11: Durable Execution\n",
    "\n",
    "In production, our agents need to handle failures gracefully. Durable execution ensures that if something goes wrong, we can recover without losing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DurableState(TypedDict):\n",
    "    ticket_id: str\n",
    "    steps_completed: List[str]\n",
    "    retry_count: int\n",
    "    final_result: Optional[str]\n",
    "\n",
    "\n",
    "def unreliable_api_call(state: DurableState) -> DurableState:\n",
    "    \"\"\"Simulates an unreliable external API call\"\"\"\n",
    "    print(f\"{Fore.YELLOW}🌐 Calling external API (attempt {state['retry_count'] + 1})...{Style.RESET_ALL}\")\n",
    "    \n",
    "    # Simulate 50% failure rate\n",
    "    if random.random() < 0.5 and state[\"retry_count\"] < 2:\n",
    "        state[\"retry_count\"] += 1\n",
    "        print(f\"{Fore.RED}❌ API call failed! Will retry...{Style.RESET_ALL}\")\n",
    "        raise Exception(\"API temporarily unavailable\")\n",
    "    \n",
    "    print(f\"{Fore.GREEN}✅ API call successful!{Style.RESET_ALL}\")\n",
    "    state[\"steps_completed\"].append(\"api_call\")\n",
    "    state[\"retry_count\"] = 0\n",
    "    return state\n",
    "\n",
    "\n",
    "def process_data(state: DurableState) -> DurableState:\n",
    "    \"\"\"Process data after API call\"\"\"\n",
    "    print(f\"{Fore.CYAN}📊 Processing data...{Style.RESET_ALL}\")\n",
    "    state[\"steps_completed\"].append(\"data_processing\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_report(state: DurableState) -> DurableState:\n",
    "    \"\"\"Generate final report\"\"\"\n",
    "    print(f\"{Fore.GREEN}📄 Generating report...{Style.RESET_ALL}\")\n",
    "    state[\"steps_completed\"].append(\"report_generation\")\n",
    "    state[\"final_result\"] = \"Report generated successfully!\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def create_durable_graph():\n",
    "    \"\"\"Create a graph with durable execution capabilities\"\"\"\n",
    "    graph = StateGraph(DurableState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph.add_node(\"api_call\", unreliable_api_call)\n",
    "    graph.add_node(\"process\", process_data)\n",
    "    graph.add_node(\"report\", generate_report)\n",
    "    \n",
    "    # Define retry logic\n",
    "    def should_retry(state: DurableState) -> str:\n",
    "        if \"api_call\" not in state[\"steps_completed\"] and state[\"retry_count\"] < 3:\n",
    "            return \"api_call\"\n",
    "        elif \"api_call\" in state[\"steps_completed\"]:\n",
    "            return \"process\"\n",
    "        else:\n",
    "            return \"report\"  # Give up and generate error report\n",
    "    \n",
    "    # Set up flow with retry logic\n",
    "    graph.set_entry_point(\"api_call\")\n",
    "    graph.add_conditional_edges(\"api_call\", should_retry)\n",
    "    graph.add_edge(\"process\", \"report\")\n",
    "    graph.add_edge(\"report\", END)\n",
    "    \n",
    "    # Use persistent checkpointer for durability\n",
    "    checkpointer = create_checkpointer()\n",
    "    return graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "print(\"\\n🛡️ Testing Durable Execution:\\n\")\n",
    "durable_graph = create_durable_graph()\n",
    "\n",
    "# Test with automatic retries\n",
    "durable_state = {\n",
    "    \"ticket_id\": \"TICKET-010\",\n",
    "    \"steps_completed\": [],\n",
    "    \"retry_count\": 0,\n",
    "    \"final_result\": None\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"durable-001\"}}\n",
    "\n",
    "# Run with retry handling\n",
    "print(\"Starting durable execution with potential failures...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "max_attempts = 5\n",
    "attempt = 0\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    try:\n",
    "        attempt += 1\n",
    "        print(f\"\\nExecution attempt {attempt}:\")\n",
    "        \n",
    "        # Continue from last checkpoint\n",
    "        result = durable_graph.invoke(None if attempt > 1 else durable_state, config=config)\n",
    "        \n",
    "        print(f\"\\n{Fore.GREEN}✅ Execution completed successfully!{Style.RESET_ALL}\")\n",
    "        print(f\"Steps completed: {result['steps_completed']}\")\n",
    "        print(f\"Result: {result['final_result']}\")\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{Fore.YELLOW}⚠️  Execution failed: {e}{Style.RESET_ALL}\")\n",
    "        if attempt < max_attempts:\n",
    "            print(\"Retrying from last checkpoint...\")\n",
    "            time.sleep(1)  # Wait before retry\n",
    "        else:\n",
    "            print(f\"{Fore.RED}❌ Max retries reached. Execution failed.{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 12: Model Context Protocol (MCP)\n",
    "\n",
    "MCP allows our agents to connect with external systems and tools in a standardized way. This is crucial for enterprise integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate MCP server connections\n",
    "class MCPServer:\n",
    "    \"\"\"Simulated MCP server for external integrations\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, capabilities: List[str]):\n",
    "        self.name = name\n",
    "        self.capabilities = capabilities\n",
    "        print(f\"{Fore.BLUE}🔌 MCP Server '{name}' initialized with capabilities: {capabilities}{Style.RESET_ALL}\")\n",
    "    \n",
    "    def execute(self, command: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a command on the MCP server\"\"\"\n",
    "        print(f\"{Fore.CYAN}  → Executing '{command}' on {self.name}{Style.RESET_ALL}\")\n",
    "        \n",
    "        # Simulate different server responses\n",
    "        if self.name == \"crm_server\":\n",
    "            return {\n",
    "                \"customer_data\": {\n",
    "                    \"name\": \"John Doe\",\n",
    "                    \"account_type\": \"Premium\",\n",
    "                    \"lifetime_value\": 5000\n",
    "                }\n",
    "            }\n",
    "        elif self.name == \"inventory_server\":\n",
    "            return {\n",
    "                \"stock_level\": 42,\n",
    "                \"reorder_status\": \"pending\"\n",
    "            }\n",
    "        return {\"status\": \"success\"}\n",
    "\n",
    "\n",
    "class MCPState(TypedDict):\n",
    "    query: str\n",
    "    mcp_results: Dict[str, Any]\n",
    "    integrated_response: Optional[str]\n",
    "\n",
    "\n",
    "def create_mcp_graph():\n",
    "    \"\"\"Create a graph that uses MCP for external integrations\"\"\"\n",
    "    \n",
    "    # Initialize MCP servers\n",
    "    crm_server = MCPServer(\"crm_server\", [\"get_customer\", \"update_customer\"])\n",
    "    inventory_server = MCPServer(\"inventory_server\", [\"check_stock\", \"reserve_item\"])\n",
    "    \n",
    "    def gather_context(state: MCPState) -> MCPState:\n",
    "        \"\"\"Gather context from multiple MCP servers\"\"\"\n",
    "        print(f\"\\n{Fore.YELLOW}📡 Gathering context from MCP servers...{Style.RESET_ALL}\")\n",
    "        \n",
    "        state[\"mcp_results\"] = {}\n",
    "        \n",
    "        # Query CRM server\n",
    "        crm_result = crm_server.execute(\"get_customer\", {\"query\": state[\"query\"]})\n",
    "        state[\"mcp_results\"][\"crm\"] = crm_result\n",
    "        \n",
    "        # Query inventory server\n",
    "        inv_result = inventory_server.execute(\"check_stock\", {\"query\": state[\"query\"]})\n",
    "        state[\"mcp_results\"][\"inventory\"] = inv_result\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def integrate_and_respond(state: MCPState) -> MCPState:\n",
    "        \"\"\"Integrate MCP results and generate response\"\"\"\n",
    "        print(f\"\\n{Fore.GREEN}🔄 Integrating MCP results...{Style.RESET_ALL}\")\n",
    "        \n",
    "        customer_data = state[\"mcp_results\"].get(\"crm\", {}).get(\"customer_data\", {})\n",
    "        inventory_data = state[\"mcp_results\"].get(\"inventory\", {})\n",
    "        \n",
    "        response = f\"\"\"Based on integrated data:\n",
    "        - Customer: {customer_data.get('name', 'Unknown')} ({customer_data.get('account_type', 'Standard')})\n",
    "        - Lifetime Value: ${customer_data.get('lifetime_value', 0)}\n",
    "        - Stock Available: {inventory_data.get('stock_level', 0)} units\n",
    "        - Reorder Status: {inventory_data.get('reorder_status', 'unknown')}\n",
    "        \n",
    "        Recommendation: {'Priority service' if customer_data.get('account_type') == 'Premium' else 'Standard service'}\n",
    "        \"\"\"\n",
    "        \n",
    "        state[\"integrated_response\"] = response\n",
    "        return state\n",
    "    \n",
    "    graph = StateGraph(MCPState)\n",
    "    \n",
    "    graph.add_node(\"gather\", gather_context)\n",
    "    graph.add_node(\"integrate\", integrate_and_respond)\n",
    "    \n",
    "    graph.set_entry_point(\"gather\")\n",
    "    graph.add_edge(\"gather\", \"integrate\")\n",
    "    graph.add_edge(\"integrate\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "print(\"\\n🌐 Testing Model Context Protocol (MCP):\\n\")\n",
    "mcp_graph = create_mcp_graph()\n",
    "\n",
    "# Test MCP integration\n",
    "mcp_query = {\n",
    "    \"query\": \"Customer inquiry about product availability\",\n",
    "    \"mcp_results\": {},\n",
    "    \"integrated_response\": None\n",
    "}\n",
    "\n",
    "result = mcp_graph.invoke(mcp_query)\n",
    "print(f\"\\n{Fore.BLUE}📋 Integrated Response:{Style.RESET_ALL}\")\n",
    "print(result[\"integrated_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 13: Tracing and Observability\n",
    "\n",
    "In production, we need to monitor our agents' behavior, performance, and decision-making. Tracing gives us complete visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "class TraceCollector:\n",
    "    \"\"\"Collects traces for monitoring and debugging\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.traces = []\n",
    "    \n",
    "    def add_trace(self, event_type: str, node_name: str, data: Dict[str, Any]):\n",
    "        trace = {\n",
    "            \"trace_id\": str(uuid.uuid4())[:8],\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_type\": event_type,\n",
    "            \"node_name\": node_name,\n",
    "            \"data\": data\n",
    "        }\n",
    "        self.traces.append(trace)\n",
    "        return trace\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get execution summary\"\"\"\n",
    "        return {\n",
    "            \"total_events\": len(self.traces),\n",
    "            \"nodes_executed\": list(set(t[\"node_name\"] for t in self.traces)),\n",
    "            \"errors\": [t for t in self.traces if t[\"event_type\"] == \"error\"],\n",
    "            \"duration_ms\": self._calculate_duration()\n",
    "        }\n",
    "    \n",
    "    def _calculate_duration(self):\n",
    "        if not self.traces:\n",
    "            return 0\n",
    "        start = datetime.fromisoformat(self.traces[0][\"timestamp\"])\n",
    "        end = datetime.fromisoformat(self.traces[-1][\"timestamp\"])\n",
    "        return int((end - start).total_seconds() * 1000)\n",
    "\n",
    "\n",
    "# Global trace collector\n",
    "trace_collector = TraceCollector()\n",
    "\n",
    "\n",
    "class TracedState(TypedDict):\n",
    "    input: str\n",
    "    processing_steps: List[str]\n",
    "    result: Optional[str]\n",
    "    trace_id: str\n",
    "\n",
    "\n",
    "def traced_node(func):\n",
    "    \"\"\"Decorator to add tracing to any node\"\"\"\n",
    "    def wrapper(state: TracedState) -> TracedState:\n",
    "        node_name = func.__name__\n",
    "        \n",
    "        # Start trace\n",
    "        start_trace = trace_collector.add_trace(\n",
    "            \"node_start\",\n",
    "            node_name,\n",
    "            {\"input\": state.get(\"input\", \"\"), \"trace_id\": state[\"trace_id\"]}\n",
    "        )\n",
    "        \n",
    "        print(f\"{Fore.BLUE}🔍 [{start_trace['trace_id']}] Starting {node_name}{Style.RESET_ALL}\")\n",
    "        \n",
    "        try:\n",
    "            # Execute the actual function\n",
    "            result = func(state)\n",
    "            \n",
    "            # End trace\n",
    "            trace_collector.add_trace(\n",
    "                \"node_complete\",\n",
    "                node_name,\n",
    "                {\"output\": result.get(\"result\", \"\"), \"trace_id\": state[\"trace_id\"]}\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Error trace\n",
    "            trace_collector.add_trace(\n",
    "                \"error\",\n",
    "                node_name,\n",
    "                {\"error\": str(e), \"trace_id\": state[\"trace_id\"]}\n",
    "            )\n",
    "            raise\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@traced_node\n",
    "def validate_input(state: TracedState) -> TracedState:\n",
    "    \"\"\"Validate and preprocess input\"\"\"\n",
    "    print(f\"  Validating: {state['input'][:50]}...\")\n",
    "    state[\"processing_steps\"].append(\"validation\")\n",
    "    \n",
    "    # Simulate validation logic\n",
    "    if len(state[\"input\"]) < 10:\n",
    "        raise ValueError(\"Input too short\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "@traced_node\n",
    "def analyze_sentiment(state: TracedState) -> TracedState:\n",
    "    \"\"\"Analyze sentiment of the input\"\"\"\n",
    "    print(f\"  Analyzing sentiment...\")\n",
    "    state[\"processing_steps\"].append(\"sentiment_analysis\")\n",
    "    \n",
    "    # Simulate sentiment analysis\n",
    "    if \"angry\" in state[\"input\"].lower() or \"frustrated\" in state[\"input\"].lower():\n",
    "        sentiment = \"negative\"\n",
    "    elif \"happy\" in state[\"input\"].lower() or \"great\" in state[\"input\"].lower():\n",
    "        sentiment = \"positive\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "    \n",
    "    state[\"result\"] = f\"Sentiment: {sentiment}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "@traced_node\n",
    "def generate_response(state: TracedState) -> TracedState:\n",
    "    \"\"\"Generate final response based on analysis\"\"\"\n",
    "    print(f\"  Generating response...\")\n",
    "    state[\"processing_steps\"].append(\"response_generation\")\n",
    "    \n",
    "    state[\"result\"] = f\"Processed: {state['input'][:30]}... | {state['result']}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def create_traced_graph():\n",
    "    \"\"\"Create a graph with comprehensive tracing\"\"\"\n",
    "    graph = StateGraph(TracedState)\n",
    "    \n",
    "    graph.add_node(\"validate\", validate_input)\n",
    "    graph.add_node(\"analyze\", analyze_sentiment)\n",
    "    graph.add_node(\"respond\", generate_response)\n",
    "    \n",
    "    graph.set_entry_point(\"validate\")\n",
    "    graph.add_edge(\"validate\", \"analyze\")\n",
    "    graph.add_edge(\"analyze\", \"respond\")\n",
    "    graph.add_edge(\"respond\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "print(\"\\n📊 Testing Tracing and Observability:\\n\")\n",
    "\n",
    "# Clear previous traces\n",
    "trace_collector.traces = []\n",
    "\n",
    "traced_graph = create_traced_graph()\n",
    "\n",
    "# Test with multiple inputs to see tracing\n",
    "test_inputs = [\n",
    "    \"I am very happy with your service! Everything works great!\",\n",
    "    \"This is frustrating and I'm getting angry about these issues.\",\n",
    "    \"Short\",  # This will cause an error\n",
    "]\n",
    "\n",
    "for i, test_input in enumerate(test_inputs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i+1}: {test_input[:50]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    trace_id = str(uuid.uuid4())[:8]\n",
    "    test_state = {\n",
    "        \"input\": test_input,\n",
    "        \"processing_steps\": [],\n",
    "        \"result\": None,\n",
    "        \"trace_id\": trace_id\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = traced_graph.invoke(test_state)\n",
    "        print(f\"\\n{Fore.GREEN}✅ Success: {result['result']}{Style.RESET_ALL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{Fore.RED}❌ Error: {e}{Style.RESET_ALL}\")\n",
    "\n",
    "# Display trace summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 EXECUTION TRACE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = trace_collector.get_summary()\n",
    "print(f\"\\nTotal events: {summary['total_events']}\")\n",
    "print(f\"Nodes executed: {', '.join(summary['nodes_executed'])}\")\n",
    "print(f\"Total duration: {summary['duration_ms']}ms\")\n",
    "print(f\"Errors encountered: {len(summary['errors'])}\")\n",
    "\n",
    "if summary['errors']:\n",
    "    print(\"\\n🚨 Error Details:\")\n",
    "    for error in summary['errors']:\n",
    "        print(f\"  - [{error['trace_id']}] {error['node_name']}: {error['data']['error']}\")\n",
    "\n",
    "# Show detailed trace for debugging\n",
    "print(\"\\n📜 Detailed Trace Log (last 5 events):\")\n",
    "print(\"-\" * 60)\n",
    "for trace in trace_collector.traces[-5:]:\n",
    "    print(f\"[{trace['timestamp']}] {trace['event_type']:15} | {trace['node_name']:20} | Trace: {trace['trace_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Putting It All Together\n",
    "\n",
    "Let's create a comprehensive example that combines all the concepts we've learned into a production-ready customer support system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive example combining all features\n",
    "print(\"\\n🎯 COMPREHENSIVE CUSTOMER SUPPORT SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nThis system combines:\")\n",
    "print(\"✓ LLM-powered intelligence\")\n",
    "print(\"✓ Tool usage for actions\")\n",
    "print(\"✓ Persistence for conversation history\")\n",
    "print(\"✓ Human-in-the-loop for approvals\")\n",
    "print(\"✓ Multi-agent collaboration\")\n",
    "print(\"✓ Streaming for real-time updates\")\n",
    "print(\"✓ Durable execution for reliability\")\n",
    "print(\"✓ MCP for external integrations\")\n",
    "print(\"✓ Comprehensive tracing\")\n",
    "\n",
    "print(\"\\n🚀 System initialized and ready!\")\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Start simple and add complexity gradually\")\n",
    "print(\"2. Use persistence for production systems\")\n",
    "print(\"3. Add human oversight for critical decisions\")\n",
    "print(\"4. Implement proper error handling and retries\")\n",
    "print(\"5. Use tracing to debug and monitor your agents\")\n",
    "print(\"6. Modularize with subgraphs for maintainability\")\n",
    "print(\"7. Let multiple agents collaborate on complex tasks\")\n",
    "\n",
    "print(\"\\n📚 Next Steps:\")\n",
    "print(\"- Experiment with these patterns in your own projects\")\n",
    "print(\"- Explore LangGraph's advanced features\")\n",
    "print(\"- Build production-ready agent systems\")\n",
    "print(\"- Join the LangGraph community for support\")\n",
    "\n",
    "print(\"\\n✨ Happy building with LangGraph!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
