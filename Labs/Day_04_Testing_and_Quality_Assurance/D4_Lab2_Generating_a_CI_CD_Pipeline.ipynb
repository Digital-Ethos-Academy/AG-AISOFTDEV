{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A robust CI pipeline is the backbone of modern software development. It automatically builds and tests your code every time a change is made, catching bugs early and ensuring quality. In this lab, you will generate all the configuration-as-code artifacts needed to build a professional CI pipeline for our application.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load our application code to provide context for the LLM. The AI needs to see our code's imports to generate an accurate `requirements.txt` file.\n",
    "\n",
    "**Model Selection:**\n",
    "Models that are good at understanding code and structured data formats like YAML are ideal. `gpt-4.1`, `o3`, or `codex-mini` are strong choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated configuration files.\n",
    "- `clean_llm_output()`: To clean up the generated text and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 13:21:51,410 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\", base_dir=project_root)\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Task:** Before we can build a Docker image, we need a list of our Python dependencies. Prompt the LLM to analyze your application code and generate a `requirements.txt` file.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that provides the LLM with the source code of your FastAPI application (`app_code`).\n",
    "2.  Instruct it to analyze the `import` statements and generate a list of all external dependencies (like `fastapi`, `uvicorn`, `sqlalchemy`). You should also ask it to include `pytest` for testing.\n",
    "3.  The output should be formatted as a standard `requirements.txt` file.\n",
    "4.  Save the artifact to the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating requirements.txt ---\n",
      "fastapi==0.111.0\n",
      "uvicorn[standard]==0.30.1\n",
      "pydantic==2.7.4\n",
      "SQLAlchemy==2.0.31\n",
      "alembic==1.13.1\n",
      "python-dotenv==1.0.1\n",
      "pytest==8.2.2\n",
      "pytest-asyncio==0.23.7\n",
      "fastapi==0.111.0\n",
      "uvicorn[standard]==0.30.1\n",
      "pydantic==2.7.4\n",
      "SQLAlchemy==2.0.31\n",
      "alembic==1.13.1\n",
      "python-dotenv==1.0.1\n",
      "pytest==8.2.2\n",
      "pytest-asyncio==0.23.7\n"
     ]
    }
   ],
   "source": [
    "# Prompt to generate a requirements.txt file based on the loaded FastAPI app source code.\n",
    "requirements_prompt = f\"\"\"\n",
    "You are an assistant that extracts Python package dependencies from source code and outputs ONLY a valid requirements.txt file.\n",
    "\n",
    "Context:\n",
    "The following is the FastAPI application source code for analysis:\n",
    "--- BEGIN APP CODE ---\n",
    "{app_code}\n",
    "--- END APP CODE ---\n",
    "\n",
    "Instructions:\n",
    "1. Read the import statements and determine all external (non-standard-library) Python packages required.\n",
    "2. Include typical FastAPI stack dependencies if referenced or implied: fastapi, uvicorn[standard], pydantic, sqlalchemy, python-dotenv.\n",
    "3. Include testing and tooling dependencies we will use in CI: pytest, pytest-asyncio.\n",
    "4. Pin each dependency to a stable, recent version (use widely adopted latest minor/patch; avoid alpha/beta).\n",
    "5. If SQLAlchemy appears, also include alembic if migrations seem plausible; otherwise omit.\n",
    "6. Do NOT include packages that are part of the Python standard library (e.g., os, sys, typing, datetime, json, asyncio).\n",
    "7. Output MUST be plain text in canonical requirements.txt format: one package per line, optionally version pinned with ==, no comments, no explanatory prose, no code fences.\n",
    "8. Prefer the following ordering heuristic: core framework (fastapi) first, server (uvicorn), data/model packages (pydantic, sqlalchemy), utilities, testing packages last.\n",
    "9. Avoid duplications; ensure consistent version pinning.\n",
    "10. If you are uncertain about a package, make a reasonable assumption based on typical FastAPI CRUD apps using a database.\n",
    "\n",
    "Output Format:\n",
    "requirements.txt lines only. Example pattern (do not reuse exact versions blindly):\n",
    "fastapi==<version>\n",
    "uvicorn[standard]==<version>\n",
    "...\n",
    "pytest==<version>\n",
    "pytest-asyncio==<version>\n",
    "\n",
    "Return ONLY the lines. No commentary.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating requirements.txt ---\")\n",
    "if app_code:\n",
    "    requirements_content = get_completion(requirements_prompt, client, model_name, api_provider)\n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(cleaned_reqs)\n",
    "    # Backup existing file handled externally; now overwrite with new generated requirements.txt\n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\", base_dir=project_root, overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping requirements generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Task:** Generate a multi-stage `Dockerfile` to create an optimized and secure container image for our application.\n",
    "\n",
    "> **Tip:** Why a multi-stage Dockerfile? The first stage (the 'builder') installs all dependencies, including build-time tools. The final stage copies only the application code and the necessary installed packages. This results in a much smaller, more secure production image because it doesn't contain any unnecessary build tools.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt asking for a multi-stage `Dockerfile` for a Python FastAPI application.\n",
    "2.  Specify the following requirements:\n",
    "    * Use a slim Python base image (e.g., `python:3.11-slim`).\n",
    "    * The first stage should install dependencies from `requirements.txt`.\n",
    "    * The final stage should copy the installed dependencies and the application code.\n",
    "    * The `CMD` should execute the application using `uvicorn`.\n",
    "3.  Save the generated file as `Dockerfile` in the project's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Dockerfile ---\n",
      "# syntax=docker/dockerfile:1\n",
      "\n",
      "FROM python:3.11-slim as builder\n",
      "ENV PIP_NO_CACHE_DIR=off\n",
      "ENV PIP_DISABLE_PIP_VERSION_CHECK=on\n",
      "RUN python -m pip install --upgrade pip\n",
      "WORKDIR /app\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir --prefix=/opt/venv -r requirements.txt\n",
      "\n",
      "FROM python:3.11-slim\n",
      "ENV PYTHONDONTWRITEBYTECODE=1\n",
      "ENV PYTHONUNBUFFERED=1\n",
      "ENV PATH=/opt/venv/bin:$PATH\n",
      "RUN apt-get update \\\n",
      "    && apt-get install -y --no-install-recommends curl \\\n",
      "    && apt-get clean \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "RUN addgroup --system --gid 1001 appgroup && \\\n",
      "    adduser --system --uid 1001 --ingroup appgroup appuser\n",
      "COPY --from=builder /opt/venv /opt/venv\n",
      "WORKDIR /home/appuser/app\n",
      "COPY --chown=appuser:appgroup app/ ./app/\n",
      "USER appuser\n",
      "EXPOSE 8000\n",
      "HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \\\n",
      "  CMD curl -f http://localhost:8000/ || exit 1\n",
      "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "# syntax=docker/dockerfile:1\n",
      "\n",
      "FROM python:3.11-slim as builder\n",
      "ENV PIP_NO_CACHE_DIR=off\n",
      "ENV PIP_DISABLE_PIP_VERSION_CHECK=on\n",
      "RUN python -m pip install --upgrade pip\n",
      "WORKDIR /app\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir --prefix=/opt/venv -r requirements.txt\n",
      "\n",
      "FROM python:3.11-slim\n",
      "ENV PYTHONDONTWRITEBYTECODE=1\n",
      "ENV PYTHONUNBUFFERED=1\n",
      "ENV PATH=/opt/venv/bin:$PATH\n",
      "RUN apt-get update \\\n",
      "    && apt-get install -y --no-install-recommends curl \\\n",
      "    && apt-get clean \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "RUN addgroup --system --gid 1001 appgroup && \\\n",
      "    adduser --system --uid 1001 --ingroup appgroup appuser\n",
      "COPY --from=builder /opt/venv /opt/venv\n",
      "WORKDIR /home/appuser/app\n",
      "COPY --chown=appuser:appgroup app/ ./app/\n",
      "USER appuser\n",
      "EXPOSE 8000\n",
      "HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \\\n",
      "  CMD curl -f http://localhost:8000/ || exit 1\n",
      "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
     ]
    }
   ],
   "source": [
    "# Prompt to generate a production-ready multi-stage Dockerfile for the FastAPI app.\n",
    "dockerfile_prompt = f\"\"\"\n",
    "You are an expert DevOps assistant. Generate ONLY a valid multi-stage Dockerfile (no backticks, no commentary) for a FastAPI application.\n",
    "\n",
    "Context Code (for imports & structure reference):\n",
    "--- BEGIN APP CODE ---\n",
    "{app_code}\n",
    "--- END APP CODE ---\n",
    "\n",
    "Requirements:\n",
    "1. Use a builder stage based on python:3.11-slim (name it builder).\n",
    "2. Create and activate a virtual environment in /opt/venv OR use --prefix to install dependencies into /opt/venv; ensure final image copies this environment.\n",
    "3. Copy in requirements.txt and install dependencies with: pip install --no-cache-dir -r requirements.txt.\n",
    "4. Separate dev/test deps if a requirements-dev.txt exists (conditionally) – but assume single file if not mentioned.\n",
    "5. Perform security-related best practices: \n",
    "   - RUN apt-get update && apt-get install -y build-essential (only if needed for wheels) then apt-get purge -y --auto-remove build-essential && rm -rf /var/lib/apt/lists/*.\n",
    "   - Use pip install flags: --no-cache-dir, and consider --upgrade pip before installs.\n",
    "6. Final stage:\n",
    "   - Base image: python:3.11-slim.\n",
    "   - Copy the virtual environment from builder.\n",
    "   - Copy the application code (app/ and any root entrypoint like main.py if needed).\n",
    "   - Set ENV PATH=/opt/venv/bin:$PATH.\n",
    "   - Create a non-root user (e.g., appuser) and switch to it.\n",
    "   - Expose port 8000.\n",
    "7. Entrypoint/CMD should run uvicorn with host 0.0.0.0 and port 8000. If main app is in app/main.py and FastAPI instance named app, use: uvicorn app.main:app --host 0.0.0.0 --port 8000.\n",
    "8. Multi-stage should reduce final image size and not include build tools after build.\n",
    "9. Include HEALTHCHECK (optional) hitting /docs or /openapi.json with curl.\n",
    "10. No extraneous comments except minimal stage identifiers; no placeholder text.\n",
    "11. Do NOT reference files not guaranteed to exist (avoid requirements-dev.txt unless conditional). Keep deterministic.\n",
    "12. Avoid ARGs unless necessary; focus on clarity.\n",
    "13. Ensure reproducibility and security (no root long-term, minimal layers).\n",
    "\n",
    "Output: Only the Dockerfile contents.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Dockerfile ---\")\n",
    "dockerfile_content = get_completion(dockerfile_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(cleaned_dockerfile)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    # Ensure we write to repository root, not artifacts subdirectory\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\", base_dir=project_root, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Consider a `.dockerignore`\n",
    "A well-crafted `.dockerignore` keeps build contexts lean and secure. After generating the Dockerfile, you can prompt the LLM similarly to create a `.dockerignore` that excludes:\n",
    "- __pycache__/ and *.pyc\n",
    "- .git/, .github/\n",
    "- .venv/ or any local virtual environments\n",
    "- .DS_Store\n",
    "- notebooks (`*.ipynb`) if not required at runtime\n",
    "- tests/ (if you only run them in CI)\n",
    "- large local artifact directories like `artifacts/`, `sandbox_runs/`, `slides/`\n",
    "\n",
    "Prompt idea:\n",
    "\"\"\"\n",
    "Generate a .dockerignore file for a FastAPI app. Exclude caches, VCS metadata, virtual environments, notebooks, test directories, local artifacts, and OS cruft. Only output the .dockerignore lines.\n",
    "\"\"\"\n",
    "\n",
    "You can then save with:\n",
    "```python\n",
    "save_artifact(clean_llm_output(dockerignore_content, language='text'), '.dockerignore')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating .dockerignore (optional) ---\n",
      "__pycache__/\n",
      "*.pyc\n",
      "*.pyo\n",
      ".git/\n",
      ".github/\n",
      ".venv/\n",
      "venv/\n",
      ".vscode/\n",
      ".idea/\n",
      ".env\n",
      ".env.*\n",
      "*.ipynb\n",
      "artifacts/\n",
      "sandbox_runs/\n",
      "slides/\n",
      "Labs/\n",
      "Solutions/\n",
      "Supporting Materials/\n",
      "tests/\n",
      "async_tests/\n",
      ".cache/\n",
      "build/\n",
      "dist/\n",
      "coverage/\n",
      "*.log\n",
      "*.db*\n",
      "*.sqlite*\n",
      "*.sql\n",
      "*.backup*\n",
      ".DS_Store\n",
      "Thumbs.db\n",
      "__pycache__/\n",
      "*.pyc\n",
      "*.pyo\n",
      ".git/\n",
      ".github/\n",
      ".venv/\n",
      "venv/\n",
      ".vscode/\n",
      ".idea/\n",
      ".env\n",
      ".env.*\n",
      "*.ipynb\n",
      "artifacts/\n",
      "sandbox_runs/\n",
      "slides/\n",
      "Labs/\n",
      "Solutions/\n",
      "Supporting Materials/\n",
      "tests/\n",
      "async_tests/\n",
      ".cache/\n",
      "build/\n",
      "dist/\n",
      "coverage/\n",
      "*.log\n",
      "*.db*\n",
      "*.sqlite*\n",
      "*.sql\n",
      "*.backup*\n",
      ".DS_Store\n",
      "Thumbs.db\n"
     ]
    }
   ],
   "source": [
    "# Generate a .dockerignore file via LLM (optional enhancement)\n",
    "dockerignore_prompt = \"\"\"\n",
    "You are to output ONLY .dockerignore patterns (one per line, no comments, no code fences).\n",
    "Context: Python FastAPI application with directories: app/, utils/, tests/, async_tests/, artifacts/, sandbox_runs/, slides/, Labs/, Solutions/, Supporting Materials/.\n",
    "Goal: Minimize image size and exclude development, test, artifact, and environment-specific files.\n",
    "\n",
    "Include exclusions for:\n",
    "- Python caches: __pycache__/ , *.pyc , *.pyo\n",
    "- VCS & CI: .git/ , .github/\n",
    "- Virtual environments: .venv/ , venv/\n",
    "- Editor settings: .vscode/ , .idea/\n",
    "- Environment files: .env , .env.*\n",
    "- Notebooks: *.ipynb\n",
    "- Artifacts & non-runtime data: artifacts/ , sandbox_runs/ , slides/ , Labs/ , Solutions/ , Supporting Materials/\n",
    "- Tests: tests/ , async_tests/\n",
    "- Build & cache output: .cache/ , build/ , dist/ , coverage/\n",
    "- Logs: *.log\n",
    "- DB/backups: *.db* , *.sqlite* , *.sql , *.backup*\n",
    "- OS cruft: .DS_Store , Thumbs.db\n",
    "\n",
    "Do NOT exclude: app/ , utils/ , requirements.txt , Dockerfile\n",
    "Avoid duplicates. Output only patterns.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating .dockerignore (optional) ---\")\n",
    "dockerignore_content = get_completion(dockerignore_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerignore = clean_llm_output(dockerignore_content, language='text')\n",
    "print(cleaned_dockerignore)\n",
    "\n",
    "if cleaned_dockerignore:\n",
    "    save_artifact(cleaned_dockerignore, \".dockerignore\", base_dir=project_root, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Task:** Generate a complete GitHub Actions workflow file to automate the build and test process.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt to generate a GitHub Actions workflow file named `ci.yml`.\n",
    "2.  Specify the following requirements for the workflow:\n",
    "    * It should trigger on any `push` to the `main` branch.\n",
    "    * It should define a single job named `build-and-test` that runs on `ubuntu-latest`.\n",
    "    * The job should have steps to: 1) Check out the code, 2) Set up a Python environment, 3) Install dependencies from `requirements.txt`, and 4) Run the test suite using `pytest`.\n",
    "3.  Save the generated YAML file to `.github/workflows/ci.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating GitHub Actions Workflow ---\n",
      "name: Python FastAPI CI\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches: [ \"main\" ]\n",
      "  pull_request:\n",
      "    branches: [ \"main\" ]\n",
      "\n",
      "permissions:\n",
      "  contents: read\n",
      "\n",
      "concurrency:\n",
      "  group: ${{ github.workflow }}-${{ github.ref }}\n",
      "  cancel-in-progress: true\n",
      "\n",
      "jobs:\n",
      "  build-and-test:\n",
      "    runs-on: ubuntu-latest\n",
      "    # strategy:\n",
      "    #   fail-fast: false\n",
      "    #   matrix:\n",
      "    #     python-version: [\"3.9\", \"3.10\", \"3.11\"]\n",
      "\n",
      "    env:\n",
      "      PYTHONUNBUFFERED: \"1\"\n",
      "\n",
      "    steps:\n",
      "      - name: Checkout repository\n",
      "        uses: actions/checkout@v4\n",
      "        with:\n",
      "          fetch-depth: 0\n",
      "\n",
      "      - name: Set up Python 3.11\n",
      "        uses: actions/setup-python@v5\n",
      "        with:\n",
      "          python-version: '3.11' # ${{ matrix.python-version }}\n",
      "          cache: 'pip'\n",
      "          cache-dependency-path: 'requirements.txt'\n",
      "\n",
      "      - name: Install dependencies\n",
      "        run: |\n",
      "          python -m pip install --upgrade pip\n",
      "          pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "      - name: Run tests with pytest\n",
      "        run: |\n",
      "          pytest tests/ async_tests/ --verbose --junitxml=pytest-report.xml --cov=app --cov-report=xml:coverage.xml\n",
      "\n",
      "      - name: Upload test and coverage reports\n",
      "        if: always()\n",
      "        uses: actions/upload-artifact@v4\n",
      "        with:\n",
      "          name: test-and-coverage-reports\n",
      "          path: |\n",
      "            pytest-report.xml\n",
      "            coverage.xml\n",
      "\n",
      "      - name: Build Docker image\n",
      "        run: |\n",
      "          docker build -t fastapi-ci-test .\n",
      "name: Python FastAPI CI\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches: [ \"main\" ]\n",
      "  pull_request:\n",
      "    branches: [ \"main\" ]\n",
      "\n",
      "permissions:\n",
      "  contents: read\n",
      "\n",
      "concurrency:\n",
      "  group: ${{ github.workflow }}-${{ github.ref }}\n",
      "  cancel-in-progress: true\n",
      "\n",
      "jobs:\n",
      "  build-and-test:\n",
      "    runs-on: ubuntu-latest\n",
      "    # strategy:\n",
      "    #   fail-fast: false\n",
      "    #   matrix:\n",
      "    #     python-version: [\"3.9\", \"3.10\", \"3.11\"]\n",
      "\n",
      "    env:\n",
      "      PYTHONUNBUFFERED: \"1\"\n",
      "\n",
      "    steps:\n",
      "      - name: Checkout repository\n",
      "        uses: actions/checkout@v4\n",
      "        with:\n",
      "          fetch-depth: 0\n",
      "\n",
      "      - name: Set up Python 3.11\n",
      "        uses: actions/setup-python@v5\n",
      "        with:\n",
      "          python-version: '3.11' # ${{ matrix.python-version }}\n",
      "          cache: 'pip'\n",
      "          cache-dependency-path: 'requirements.txt'\n",
      "\n",
      "      - name: Install dependencies\n",
      "        run: |\n",
      "          python -m pip install --upgrade pip\n",
      "          pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "      - name: Run tests with pytest\n",
      "        run: |\n",
      "          pytest tests/ async_tests/ --verbose --junitxml=pytest-report.xml --cov=app --cov-report=xml:coverage.xml\n",
      "\n",
      "      - name: Upload test and coverage reports\n",
      "        if: always()\n",
      "        uses: actions/upload-artifact@v4\n",
      "        with:\n",
      "          name: test-and-coverage-reports\n",
      "          path: |\n",
      "            pytest-report.xml\n",
      "            coverage.xml\n",
      "\n",
      "      - name: Build Docker image\n",
      "        run: |\n",
      "          docker build -t fastapi-ci-test .\n"
     ]
    }
   ],
   "source": [
    "# Prompt to generate a GitHub Actions CI workflow (ci.yml)\n",
    "ci_workflow_prompt = f\"\"\"\n",
    "You are an expert CI engineer. Generate ONLY a valid GitHub Actions workflow YAML (no backticks, no commentary) named ci.yml for a Python FastAPI project.\n",
    "\n",
    "Repository Context:\n",
    "- FastAPI app code located in app/.\n",
    "- Dependencies listed in requirements.txt at repo root.\n",
    "- Tests located in tests/ and async_tests/ (we will run both).\n",
    "- Dockerfile at repo root for building container image.\n",
    "\n",
    "Workflow Requirements:\n",
    "1. Trigger: on push to main and on pull_request targeting main.\n",
    "2. Concurrency: Cancel in-progress runs of same ref to reduce noise.\n",
    "3. Single job: build-and-test on ubuntu-latest.\n",
    "4. Steps (in order):\n",
    "   a. Checkout code (use actions/checkout@v4 with fetch-depth: 0 for potential future coverage or versioning).\n",
    "   b. Set up Python 3.11 (actions/setup-python@v5) and enable pip caching keyed on requirements.txt hash.\n",
    "   c. Install dependencies: pip install --upgrade pip && pip install -r requirements.txt.\n",
    "   d. Run tests (pytest) across tests/ and async_tests/ with verbose output; generate junit xml and coverage report.\n",
    "   e. Upload test results (use actions/upload-artifact@v4) for junit XML and coverage data.\n",
    "   f. (Optional) Build Docker image using Dockerfile to ensure it compiles: docker build -t fastapi-ci-test .\n",
    "5. Environment variables: set PYTHONUNBUFFERED=1.\n",
    "6. Fail fast if dependency install fails; use --no-cache-dir.\n",
    "7. Add a matrix strategy placeholder commented out for future Python versions (but keep current single version for simplicity).\n",
    "8. Use minimal permissions: contents: read.\n",
    "9. Add caching for pip (actions/cache) if not using built-in setup-python caching, but prefer built-in cache option.\n",
    "10. Do NOT include deployment steps; CI only.\n",
    "11. Ensure test command exits non-zero on failures (no continue-on-error).\n",
    "12. Coverage: produce coverage.xml via pytest --cov=app --cov-report xml and store artifact.\n",
    "\n",
    "Output: ONLY the YAML body of .github/workflows/ci.yml.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating GitHub Actions Workflow ---\")\n",
    "ci_workflow_content = get_completion(ci_workflow_prompt, client, model_name, api_provider)\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(cleaned_yaml)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    # Write workflow to repo root under .github/workflows\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\", base_dir=project_root, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have now generated a complete, professional Continuous Integration pipeline using AI. You created the dependency list, the containerization configuration, and the automation workflow, all from simple prompts. This is a powerful demonstration of how AI can automate complex DevOps tasks, allowing teams to build and ship software with greater speed and confidence.\n",
    "\n",
    "> **Key Takeaway:** AI is a powerful tool for generating 'Configuration as Code' artifacts. By prompting for specific formats like `requirements.txt`, `Dockerfile`, or `ci.yml`, you can automate the creation of the files that define your entire build, test, and deployment processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
